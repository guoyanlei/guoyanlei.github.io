{"meta":{"title":"GuoYL's Notes","subtitle":"后端技术 | 大数据管理与分析","description":"大志非才不就，大才非学不成","author":"Guo Yanlei","url":"http://guoyanlei.top"},"pages":[{"title":"About","date":"2018-02-07T07:00:44.000Z","updated":"2020-12-04T11:17:39.039Z","comments":true,"path":"about/index.html","permalink":"http://guoyanlei.top/about/index.html","excerpt":"","text":"郭彦磊 男，1990年 Phone：13161806054 Email：&#103;&#x79;&#108;&#103;&#x75;&#x6f;&#x79;&#x61;&#110;&#108;&#x65;&#x69;&#64;&#x67;&#109;&#97;&#105;&#x6c;&#46;&#99;&#111;&#x6d; Blog：guoyanlei.top 教育背景 硕士 2013年9月～2016年7月 内蒙古科技大学 本科 2009年9月～2013年7月 燕山大学大学 里仁学院 技术栈离线计算12345★★★ 核心：HDFS&#x2F;YARN&#x2F;HBase&#x2F;Phoenix&#x2F;Kudu&#x2F;Zookeeper★★★ 引擎：Hive&#x2F;Spark&#x2F;Impala★★☆ 分析：Druid&#x2F;Aerospike&#x2F;ElasticSearch★★☆ 工具：CDH&#x2F;Logstash&#x2F;Flume&#x2F;DataX&amp;Sqoop&#x2F;Hue+Sentry&#x2F;Azkaban★☆☆ 其他：Grafana&#x2F;InfluxDB 实时计算123★★★ 计算：Flink★★★ 队列：Kafka★☆☆ 其他：Spark-Streaming&#x2F;Storm 后端框架12345★★☆ 框架：REST api + SpringMVC&#x2F;SpringBoot★☆☆ 数据：MySQL&#x2F;MongoDB&#x2F;ElasticSearch★☆☆ 缓存：Guava Cache&#x2F;Redis★☆☆ 其他：Dubbo&#x2F;Quartz 语言栈Java/Scala/Python/Shell 工作经历 xxxxx有限公司 2016年3月-至今 职位：数据平台研发主管职责：负责公司大数据业务及数据平台相关研发 工作内容： 大数据业务 数仓业务 实时计算 数据治理 集群运维 平台研发 广告平台 用户增长平台 事件分析平台 数据服务化平台 推荐平台 智能推送平台 项目经验大数据上云项目 （2020-11～2020-至今）"},{"title":"Categories","date":"2018-02-07T07:00:44.000Z","updated":"2018-05-23T08:55:59.454Z","comments":false,"path":"categories/index.html","permalink":"http://guoyanlei.top/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-02-07T07:00:44.000Z","updated":"2018-05-23T08:55:59.455Z","comments":true,"path":"tags/index.html","permalink":"http://guoyanlei.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"QCon2019大数据平台架构相关总结","slug":"2019050801-QCon2019大数据平台架构相关总结","date":"2019-05-08T07:30:00.000Z","updated":"2019-05-09T10:57:26.113Z","comments":true,"path":"2019/05/08/2019050801-QCon2019大数据平台架构相关总结/","link":"","permalink":"http://guoyanlei.top/2019/05/08/2019050801-QCon2019%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/","excerpt":"1. 快手万亿级别Kafka集群应用实践与技术演进应用实践快手对Kafka的三个重要应用场景： 在线集群：在线服务消息中间件 Log集群：业务系统日志收集和传输的缓存介质，之后面向重要的实时消费和数据处理 离线集群：是所有各种日志的最终汇聚点，一方面落地到数仓；另一方面面向次重要的实时消费和数据处理","text":"1. 快手万亿级别Kafka集群应用实践与技术演进应用实践快手对Kafka的三个重要应用场景： 在线集群：在线服务消息中间件 Log集群：业务系统日志收集和传输的缓存介质，之后面向重要的实时消费和数据处理 离线集群：是所有各种日志的最终汇聚点，一方面落地到数仓；另一方面面向次重要的实时消费和数据处理 业务场景架构如下： 其中，通过Mirror Service将多个在线集群和Log集群中数据汇总到离线集群 技术演进对kafka的使用做了如下优化： 优化一：kafka集群平滑扩容优化，解决了扩容节点时导致kafka集群物理资源大量消耗，影响producer写入 问题：社区kafka对partition的迁移是从最初的offset开始的，触发读盘，物理资源大量消耗 =&gt; produce延迟增高且 抖动；扩容不平滑 优化思路：从最新offset开始迁移，并同步一定时间，保障所有consumer都已经跟上（据说这一改进已经向社区提交了issues 优化二：Mirror集群化建设，摒弃了Apache Kafka的Mirrormaker，基于uReplicator做了改进 问题：Apache Kafka的Mirrormaker是静态管理，运维成本高，易出错；当增加topic或集群时导致正在运行的数据Mirror整体断流 优化思路：基于uber的UReplicator做了改进，通过Mirror服务集群化管理，可减低运维，避免出错，支持快速调整，应对突增流量 优化三：资源隔离，将不同业务线的topic进行物理隔离，保证互不影响 问题1：不同业务线topic缺少物理隔离，会相互影响 优化思路：根据不同的业务做到Broker级别物理隔离 问题2：Kafka Rpc队列缺少隔离，一旦某个topic处理慢，会导致所有请求hang住 优化思路：多RPC队列，进行隔离 优化四：对PageCache的改造，避免Consumer的lag读和Follower进行replica时可能产生的PageCache污染 问题：Kafka高性能依赖page cache，但page cache不可控（由操作系统管理），可能会被污染 优化思路：让kafka自己维护数据cache，严格按照时间顺序cache，并控制follower的数据不进入cache 优化五：消费智能限速，解决了某个Consumer lag延迟后读盘导致的producer写入受阻的问题 问题：某个Consumer lag延迟后读盘导致的producer写入受阻 思路：当磁盘繁忙，针对lag的consumer进行限速控制 总结从应用实践中了解了kafka的三个重要应用场景，并对多kafka集群的Mirror Service有了新的认识，之后可以对uReplicator进行一些调研，看是否可应用到跨kafka集群的数据同步中。 从技术演进中也就收获了一些kafka使用中可能遇到的问题和优化思路，看似思路都很简单，但是真正实施起来难度还是相当大的。 2. 滴滴大数据研发平台最佳实践简介大数据研发平台设计的初衷是：设计一个平台满足所有数据开发人员的数据分析、数据加工、模型训练等工作，同时做到数据安全的使用和管理，以及统一式的开发和运维，开发人员只关心自己的业务，不需要过多的底层。 整个功能类似阿里的DataWorks。 整体架构如下： 主要工作工作一：开发与生产隔离由于大数据开发的特殊性，经常会基于线上已有表的数据进行开发测试，这样就难免对线上的任务和数据有影响。 解决这一问题的思路就是将开发和生产进行逻辑上的隔离，开发环境里允许对线上数据进行查询，但是结果落地时，写入的是测试的库，避免对线上库和表造成污染，待真正上线时，将发布包提交到生产环境，数据才会写入线上的库。 这一点可以借鉴，但是需要平台化的管理，这块我们还是比较欠缺。 工作二：统一的任务执行平台其实也是个逻辑的东西，将底层的执行引擎透明化，用一个统一的任务执行平台进行管理，在这个平台上可以提交离线任务、实时任务、机器学习、提数、特征分析等多种任务，任务在真正执行时是根据任务的具体类型来提交到不同的处理引擎上的。 主要工作还是上层的抽象和封装。 工作三：实时表元数据化要实现开发实时就写SQL一样简单，首先要实现的就是实时表元数据化。 其实Flink SQL已经实现了这种的写SQL来完成实时数据的处理，但是在开发时需要人为的创建表的Schema。 他们做的工作就是提前将用到的各种实时表元数据化，用户在开发时只需要指定某个具体的表就可以实现实时任务的开发，不需要关心元数据是否创建。 工作四：基于列进行权限管理这部分工作还是为数据安全来考虑的，向手机号、身份证等敏感信息，不应该向所有大数据开发人员可见，如何做到列基本的权限管理呢，这是这部分工作的初衷。 我们其实已经基于Hue和Sentry有了对表级别的权限做了管控，但是在字段级别还没有好的解决思路。 他们的实现方案： 划分列的安全等级，1,2,3,4 为列进行等级打标 借助自研的安全管家服务，为用户实现授权 底层基于Apache Ranger实现，安全管家会生成安全策略和授权传递给ranger 总结这种平台化的东西也只有那些大厂才有能力和资源来做，也就收获了一些实现思路，具体怎么应用到我们的工作中，还需要很多路要走。 3. 苏宁OLAP引擎发展之路没get到太多干货，讲的最多的就是对查询引擎SparkSQL的进行了一些优化。 主要包括： Spark-HDFS Spark-Druid ES-Hadoop PG-Spark-JDBC 优化思路就是将查询SQL谓词下推，要充分利用各存储自身的查询性能，尽量避免把数据全部拉到集群中进行计算。 其OLAP整体架构： 数据中台架构： 4. ClickHouse在头条的技术演进主要从两个部分进行介绍，一是头条为什么选择了ClickHouse及如何使用的，二是主要做了哪些优化 由于自己对ClickHouse没做过多了解，第二部分优化没get到什么。 简介ClickHouse 2016年开源的，由俄罗斯IT公司Yandex开发，是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)，而且查询性能优越。 主要特点： 面向列+向量执行 自己管理存储（非Hadoop） 线性可扩展，高可靠（通过shard+replication实现） 面向SQL查询 快 性能优越的原因： Data Skipping 分区及分区剪枝 数据局部有序，类似LSM树的查询引擎 资源垂直整合 并发MPP+SMP架构 执行层是SIMD实现（单指令多数据流） 底层C++实现 使用的场景： 单表分析，这个表可以很宽 分布式Join性能并不出色 不足： 没有事务 批量数据接收 update or delete支持较弱 查询重写优化较弱 应用头条选择ClickHouse的原因： 产品需求 交互式分析能力（in seconds） 查询模式多变 以大宽表为主 数据量大 开源MPP OLAP引擎 - （性能、特点、优质） 在头条的应用： 总结目前ClickHouse商业应用还较少，只有部分大厂有使用，而且他们在使用过程中如果遇到一些问题，自己有能力和资源进行二研优化；更主要的是ClickHouse不理睬Hadoop生态，走自己的路。我们暂且了解观望。 5. 美团点评常态、异地、多机房、单集群Hadoop架构实践原生Hadoop在跨机房，跨地域进行搭建时，网络的延迟会大大影响hadoop的管理和任务处理性能。 但是有时候随着业务发展，集群节点越来越多，同地域的机房没办法承载，但是又不想搭建多个hadoop集群，此时就需要搭建这种跨机房、跨地域的hadoop集群了。 美团通过二研实现了单集群异地多机房Hadoop服务，并且保证了管理和任务处理的性能，实现了架构前向兼容，机房对业务透明。 整体思路，通过优化Hadoop，使得其有对地区的感知，尽量避免跨机房流量 实现第一步：多机房Hadoop资源管理 多机房存储资源管理 NameNode机房感知中增加了对地区的感知 NameNode副本分布属性增加了地区的支持 HDFS读写响应：保证吞吐，避免跨机房流量（仅向默认机房写⼊，就近读取） 具备初级多机房存储资源管理理能⼒ 多机房计算调度 基于Label Scheduler的多机房计算资源调度，即在提交作业时附加上机房的标签，禁⽌止跨机房作业调度 基于YARN Federation的跨地域计算资源调度，优先请求本地机房 第二步：多机房Hadoop资源管理 跨机房数据Cache处理，就是基于数据⾎血缘产⽣和读写规律进行构建副本cache的规则（在本地机房冗余一些数据，减少重复跨机房的数据读取） 带宽管控，充分利用好带宽 第三步：HDFS机房容错 HDFS分区容忍设计粒度为节点级别，而不在是块级别 实现机房、机架粒度容错，保证网络故障时，DataNode没有故障，数据没有丢失 总结收获了一些解决这样问题的思路，具体是否值得实现还得看业务。技术换运营，站在平台运营视⻆角进行架构设计，才能保证架构平稳落地。 6. 阿里巴巴新一代交互式分析引擎-Hologres主要介绍了Hologres是啥，其设计理念是啥，性能有多么优越。。。 Hologres是啥？ 新一代海量数据交互式分析引擎 一套引擎支持Point Query(hbase场景)，Ad-hoc Query(Druid场景)，OLAP Query(Impala场景) 快。。 存储计算分离 支持实时数据与批量数据导入 支持External Storage，与阿里云大数据产品无缝对接 亮点&amp;理念？ 统一的引擎架构，保证数据的一致性 解决数据在Hbase存一份、Druid存一份、xxx存一份造成的浪费和数据不一致 存储和计算分离 据说新的NVME SSD盘可以达到150000IOPS，磁盘IO不再是性能瓶颈，问题转变为如何把CPU高效利用起来 存储计算分离是未来大势所趋，存储和计算非对齐采购，成本更低，部署运维更方便 更加聪明的Optimizer 使用新技术 近几年硬件性能提升的很快，N年前的技术方案不一定能够很好的利用现在的硬件性能发挥到极致 使用全异步架构，把CPU利用到极致 向量化计算 具体架构就略了，目前还在开发迭代中，后期在看。 整体get到的点可能就是：在未来的某一天磁盘IO不再是性能瓶颈，它可能比CPU处理更快，到时候现在已有的操作系统可能就需要重新设计。 注： 对相关内容感兴趣的可以下载各分享PPT","categories":[{"name":"大数据平台架构","slug":"大数据平台架构","permalink":"http://guoyanlei.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://guoyanlei.top/tags/kafka/"},{"name":"架构","slug":"架构","permalink":"http://guoyanlei.top/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"基于Sentry和Hue对数仓表做权限管理","slug":"2019032001-基于Sentry和Hue对数仓表做权限管理","date":"2019-03-20T07:30:00.000Z","updated":"2019-03-20T03:58:07.213Z","comments":true,"path":"2019/03/20/2019032001-基于Sentry和Hue对数仓表做权限管理/","link":"","permalink":"http://guoyanlei.top/2019/03/20/2019032001-%E5%9F%BA%E4%BA%8ESentry%E5%92%8CHue%E5%AF%B9%E6%95%B0%E4%BB%93%E8%A1%A8%E5%81%9A%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/","excerpt":"Hue简介Hue是什么Hue是一个可快速开发和调试Hadoop生态系统各种应用的一个基于浏览器的图形化用户接口。 Hue是出自CDH公司，在基于CDH的大数据集群中安装和使用非常方便。","text":"Hue简介Hue是什么Hue是一个可快速开发和调试Hadoop生态系统各种应用的一个基于浏览器的图形化用户接口。 Hue是出自CDH公司，在基于CDH的大数据集群中安装和使用非常方便。 Hue能干什么12345678910111213141.访问HDFS和文件浏览 2.通过web调试和开发hive以及数据结果展示 3.查询solr和结果展示，报表生成 4.通过web调试和开发impala交互式SQL Query 5.spark调试和开发 6.Pig开发和调试 7.oozie任务的开发，监控，和工作流协调调度 8.Hbase数据查询和修改，数据展示 9.Hive的元数据（metastore）查询 10.MapReduce任务进度查看，日志追踪 11.创建和提交MapReduce，Streaming，Java job任务 12.Sqoop2的开发和调试 13.Zookeeper的浏览和编辑 14.数据库（MySQL，PostGres，SQlite，Oracle）的查询和展示 其架构图： Hue有哪些不足 虽然提供了角色权限管理，但是仅是针对Hue各功能的权限控制 没有办法从库级别、表级别对用户做权限管理 Sentry简介Sentry是什么Apache Sentry是一个适用于Hadoop，基于角色粒度的授权模块，旨在成为Hadoop组件的可插拔授权引擎。它允许定义授权规则以验证用户或应用程序对Hadoop资源的访问请求。目前可以与Apache Hive，Apache Solr，Impala和HDFS（仅限于Hive表数据）一起开箱即用。 Sentry同样是出自CDH公司，在基于CDH的大数据集群中安装和使用非常方便。 官网doc 其架构图： 主要分为三个模块： Sentry Server：管理授权元数据。它支持安全检索和操作元数据的界面; Data Engine：数据处理引擎（如Hive或Impala）。数据引擎执行前会加载Sentry插件，拦截所有访问资源的请求并将其路由到Sentry插件进行验证; Sentry plugin：运行在数据引擎中，验证用户是否有权限。 Sentry能干什么1234567- 对hive库、表、表字段做权限控制- 对impala库、表、表字段做权限控制，与hive的区别是可以缓存权限（在Catalog server和Impalad都有），授权验证在本地发生并且速度会更快- 对hdfs目录、文件做权限控制，更关注的是hive数仓的数据（任何属于Hive或Impala中表的数据）- 权限可分为三级： - SELECT privilege -&gt; Read access on the file. - INSERT privilege -&gt; Write access on the file. - ALL privilege -&gt; Read and Write access on the file. 基于Sentry和Hue对数仓表做权限管理安装步骤基于Cloudera-manager可以直接安装Hue和Sentry，安装Hue前需要先安装Oozie。 安装Hue和Sentry都需要先创建mysql库：hue和sentry，并在安装时制定库和库地址。 Hue安装遇到的问题 123456789# 测试mysql链接时缺少jar包，找不到jdbc driver解决方法：需要将这个包放到这个路径下&#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java.jar# 安装后启动进程失败原因：hue所在的服务器环境没有预先安装httpd，mod_ssl服务解决方法：yum -y install httpdyum -y install mod_ssl 权限配置修改Hive的Sentry配置 在hive配置中启用sentry 禁用“HiveServer2 启用模拟：hive.server2.enable.impersonation” 配置“sentry-site.xml 的 Hive 服务高级配置代码段（安全阀）” 1234&lt;property&gt; &lt;name&gt;sentry.hive.testing.mode&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 修改Impala的Sentry配置 在Impala配置中启用sentry 重启Impala即可 123注意：若启用了sentry，那么在Impala-jdbc调用时许指定用户。如原来的url：jdbc:impala:&#x2F;&#x2F;node1.lb.bigdata.dmp.com:21050&#x2F;prod;AuthMech&#x3D;0指定用户后url：jdbc:impala:&#x2F;&#x2F;node1.lb.bigdata.dmp.com:21050&#x2F;prod;AuthMech&#x3D;3;UID&#x3D;impala;PWD&#x3D;;UseSasl&#x3D;0 修改Hue的Sentry配置 在Hue配置中启用sentry 重启Hue即可 hive角色、用户组和Hue的区别 hive role：控制表、库的select、insert权限（通过sentry可以管理） hive group：通常被指定拥有哪些 hive role hue group：通过操作系统上的用户组和Hive的用户组进行关联。因此，配置完Hue用户组后还需配置OS上的用户组（需要配置所有hiveserver3，hue，sentry所在的机器) hue username：需要和hue group保持相同 权限配置 使用admin用户登录Hue，创建group：hive（赋予所有权限），username：hive 使用hive用户登录，即可在security菜单中管理hive roles了 下面是总结创建一个用户的过程 12345678910111213141）在hue中创建group（页面操作，admin用户登录）2）在hue中创建用户（页面操作，admin用户登录），并指定group：3）所有hiveserver3，hue，sentry所在的机器上创建group（命令行操作）- useradd group_name4）在hue中为hive创建role（可在impala或hive的Edit页面操作，hive用户登录）- create role role_name5）在hue中为某个group授予角色（可在impala或hive的Edit页面操作，hive用户登录）- grant role role_name to group group_name;6）在hue中为创建的role指定库、表等权限（在sentry页面操作，hive用户登录）7）使用第二步hue创建的用户登录，然后刷新，即可看到指定权限的库、表 遇到的问题hiveSQL执行add jar失败12345hive配置了sentry后执行下面语句失败add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;phoenix-4.13.2-cdh5.11.2-hive.jar;原因：The ADD JAR command does not work with HiveServer2 and the Beeline client when Beeline runs on a different host. 解决办法： cloudera官网 123456789101112131415首先，只能借助hive的辅助jar来实现在hive配置中修改（配置项：Hive 辅助 JAR 目录）：hive.aux.jars.path&#x3D;&#x2F;data&#x2F;dmp&#x2F;udfhive-env.sh中添加（配置项：hive-env.sh 的 Gateway 客户端环境高级配置代码段（安全阀））：HIVE_AUX_JARS_PATH&#x3D;&#x2F;data&#x2F;dmp&#x2F;udfhive-site.xml中添加（配置项hive-site.xml 的 Hive 服务高级配置代码段（安全阀））：hive.reloadable.aux.jars.path&#x3D;&#x2F;data&#x2F;dmp&#x2F;udf然后将用的jar放的hiveserver2所在机器的&#x2F;data&#x2F;dmp&#x2F;udf目录下之后若有新jar需要加入，直接将jar放到目录下，然后在HiveSQL中执行reload命令即可。最后在执行SQL时就不需要add jar操作 hiveSQL执行insert overwrite操作失败123456789ERROR : Failed with exception Directory hdfs:&#x2F;&#x2F;nameservice&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ods_hive_recommend_item_topic_nd could not be cleaned up.19-03-2019 14:11:17 CST ods_hive_recommend_item_topic_nd Directory hdfs:&#x2F;&#x2F;nameservice&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ods_hive_recommend_item_topic_nd could not be cleaned up.... ...Caused by: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: Permission denied by sticky bit: user&#x3D;hive, path&#x3D;&quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ods_hive_recommend_item_topic_nd&#x2F;000000_0&quot;:impala:hive:-rwxrwxrwt, parent&#x3D;&quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ods_hive_recommend_item_topic_nd&quot;:impala:hive:drwxrwxrwt 发现如下信息: 123456789101112Permission denied by sticky bit: user&#x3D;hive,由于sticky bit导致的错误:# sticky bit不同于suid, guid，对于others的execute权限位，则可以设置sticky bit标志，用t来表示，如果该位置本来就有可执行权限位，即x，则t和x叠加后用大写的T来表示。sticky bit只对目录起作用，如果一个目录设置了sticky bit，则该目录下的文件只能被该文件的owner或者root删除，其他用户即使有删除权限也无法删除该文件。-rwxrwxrwt 3 hdfs hive 636 2018-11-06 10:19 &#x2F;user&#x2F;hive&#x2F;xxxx&#x2F;000000_0-rwxrwxrwt 3 hdfs hive 635 2018-11-06 10:19 &#x2F;user&#x2F;hive&#x2F;xxxx&#x2F;000001_0 解决办法： 123将sticky bit配置给移除:hadoop fs -chmod -R -t &#x2F;user&#x2F;hive&#x2F;xxxx&#x2F;","categories":[{"name":"hue","slug":"hue","permalink":"http://guoyanlei.top/categories/hue/"}],"tags":[{"name":"hue","slug":"hue","permalink":"http://guoyanlei.top/tags/hue/"},{"name":"sentry","slug":"sentry","permalink":"http://guoyanlei.top/tags/sentry/"}]},{"title":"HDFS和Yarn同时重启对Flink on Yarn任务的影响","slug":"2019021401-HDFS和Yarn同时重启对Flink on Yarn任务的影响","date":"2019-02-14T07:30:00.000Z","updated":"2019-02-14T10:09:04.927Z","comments":true,"path":"2019/02/14/2019021401-HDFS和Yarn同时重启对Flink on Yarn任务的影响/","link":"","permalink":"http://guoyanlei.top/2019/02/14/2019021401-HDFS%E5%92%8CYarn%E5%90%8C%E6%97%B6%E9%87%8D%E5%90%AF%E5%AF%B9Flink%20on%20Yarn%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BD%B1%E5%93%8D/","excerpt":"HDFS和Yarn同时重启对Flink on Yarn任务的影响现象部分consumer的topic partition出现从Earlist开始消费的问题","text":"HDFS和Yarn同时重启对Flink on Yarn任务的影响现象部分consumer的topic partition出现从Earlist开始消费的问题 官网上1234567891. If offsets could not be found for a partition, the auto.offset.reset setting in the properties will be used.2. Flink Kafka Consumer Offset提交行为配置:Flink Kafka Consumer允许配置offset提交回Kafka brokers(Kafka 0.8是写回Zookeeper)的行为，注意Flink Kafka Consumer 并不依赖于这个提交的offset来进行容错性保证，这个提交的offset仅仅作为监控consumer处理进度的一种手段。配置offset提交行为的方式有多种，主要取决于Job的checkpoint机制是否启动。 1）checkpoint禁用:如果checkpoint禁用，Flink Kafka Consumer依赖于Kafka 客户端内部的自动周期性offset提交能力。因此，为了启用或者禁用offset提交，仅需在给定的Properties配置中设置enable.auto.commit(Kafka 0.8是auto.commit.enable)&#x2F;auto.commit.interval.ms为适当的值即可。 2）checkpoint启用:如果checkpoint启用，当checkpoint完成之后，Flink Kafka Consumer将会提交offset保存到checkpoint State中，这就保证了kafka broker中的committed offset与 checkpoint stata中的offset相一致。用户可以在Consumer中调用setCommitOffsetsOnCheckpoints(boolean) 方法来选择启用或者禁用offset committing(默认情况下是启用的)。注意，在这种情况下，配置在Properties中的自动周期性offset提交将会被完全忽略。 分析flink消费kafka的topic，为保证容错性，对offset的管理是通常是基于checkpoint机制的。 关于checkpoint存储offset机制可以参考这篇文章，中文可参考 而checkpoint状态保存在HDFS上，当HDFS重启时，checkpoint状态存在保存失败的问题，当yarn重启后，yarn会自动将flink任务重启，重启时从checkpoint开始恢复，但是存在故障的checkpoint，导致上述问题（If offsets could not be found for a partition, the auto.offset.reset setting in the properties will be used）。 源码分析flink-connector-kafka目前已有kafka 0.8、0.9、0.10、0.11四个版本的实现，本文分析的是FlinkKafkaConsumer011版本代码。 FlinkKafkaConsumer011类的父类继承关系如下，FlinkKafkaConsumerBase包含了大多数实现。 FlinkKafkaConsumer011 extends FlinkKafkaConsumer010 extends FlinkKafkaConsumer09 extends FlinkKafkaConsumerBase 1234public abstract class FlinkKafkaConsumerBase&lt;T&gt; extends RichParallelSourceFunction&lt;T&gt; implements CheckpointListener, ResultTypeQueryable&lt;T&gt;, CheckpointedFunction &#123; FlinkKafkaConsumerBase的内部实现分析： initializeState方法会在flinkkafkaconusmer初始化的时候最先调用 123456789101112131415161718192021222324252627282930313233343536373839404142方法通过运行时上下文FunctionSnapshotContext调用getOperatorStateStore和getSerializableListState拿到了checkpoint里面的state对象如果这个task是从失败等过程中恢复的，context.isRestored()会被判定为true程序会试图从flink checkpoint里获取原来分配到的kafka partition以及最后提交完成的offset。@Overridepublic final void initializeState(FunctionInitializationContext context) throws Exception &#123; OperatorStateStore stateStore = context.getOperatorStateStore(); ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; oldRoundRobinListState = stateStore.getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME); this.unionOffsetStates = stateStore.getUnionListState(new ListStateDescriptor&lt;&gt;( OFFSETS_STATE_NAME, TypeInformation.of(new TypeHint&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt;() &#123;&#125;))); if (context.isRestored() &amp;&amp; !restoredFromOldState) &#123; restoredState = new TreeMap&lt;&gt;(new KafkaTopicPartition.Comparator()); // migrate from 1.2 state, if there is any for (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : oldRoundRobinListState.get()) &#123; restoredFromOldState = true; unionOffsetStates.add(kafkaOffset); &#125; oldRoundRobinListState.clear(); if (restoredFromOldState &amp;&amp; discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) &#123; throw new IllegalArgumentException( &quot;Topic / partition discovery cannot be enabled if the job is restored from a savepoint from Flink 1.2.x.&quot;); &#125; // populate actual holder for restored state for (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : unionOffsetStates.get()) &#123; restoredState.put(kafkaOffset.f0, kafkaOffset.f1); &#125; LOG.info(&quot;Setting restore state in the FlinkKafkaConsumer: &#123;&#125;&quot;, restoredState); &#125; else &#123; LOG.info(&quot;No restore state for FlinkKafkaConsumer.&quot;); &#125;&#125; open方法会在initializeState技术后调用，主要逻辑分为几个步骤 判断offsetCommitMode。根据kafka的auto commit ，setCommitOffsetsOnCheckpoints()的值（默认为true）以及flink运行时有没有开启checkpoint三个参数的组合，offsetCommitMode共有三种模式： ON_CHECKPOINTS checkpoint结束后提交offset； KAFKA_PERIODIC kafkaconsumer自带的定期提交功能； DISABLED 不提交 创建分区发现者 判断是否从checkpoint状态恢复，若是，则从状态中读取各partition的offset；若否，则根据启动模式来设定offset SPECIFIC_OFFSETS 和 TIMESTAMP 两个模式直接设置好 其他的模式（EARLIEST, LATEST 和 GROUP_OFFSETS），会在后面真正读partition数据时设置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156@Overridepublic void open(Configuration configuration) throws Exception &#123; // determine the offset commit mode（判断offsetCommitMode） this.offsetCommitMode = OffsetCommitModes.fromConfiguration( getIsAutoCommitEnabled(), enableCommitOnCheckpoints, ((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled()); // create the partition discoverer（创建分区发现者） this.partitionDiscoverer = createPartitionDiscoverer( topicsDescriptor, getRuntimeContext().getIndexOfThisSubtask(), getRuntimeContext().getNumberOfParallelSubtasks()); this.partitionDiscoverer.open(); subscribedPartitionsToStartOffsets = new HashMap&lt;&gt;(); List&lt;KafkaTopicPartition&gt; allPartitions = partitionDiscoverer.discoverPartitions(); //判断是否从checkpoint状态恢复 if (restoredState != null) &#123; // 是 for (KafkaTopicPartition partition : allPartitions) &#123; // 从状态中恢复 if (!restoredState.containsKey(partition)) &#123; //状态中没有当前分区，则从ERALIST开始消费 restoredState.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET); &#125; &#125; for (Map.Entry&lt;KafkaTopicPartition, Long&gt; restoredStateEntry : restoredState.entrySet()) &#123; if (!restoredFromOldState) &#123; // seed the partition discoverer with the union state while filtering out // restored partitions that should not be subscribed by this subtask if (KafkaTopicPartitionAssigner.assign( restoredStateEntry.getKey(), getRuntimeContext().getNumberOfParallelSubtasks()) == getRuntimeContext().getIndexOfThisSubtask())&#123; subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue()); &#125; &#125; else &#123; // when restoring from older 1.1 / 1.2 state, the restored state would not be the union state; // in this case, just use the restored state as the subscribed partitions subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue()); &#125; &#125; LOG.info(&quot;Consumer subtask &#123;&#125; will start reading &#123;&#125; partitions with offsets in restored state: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets); &#125; else &#123; // 否 // use the partition discoverer to fetch the initial seed partitions, // and set their initial offsets depending on the startup mode. // for SPECIFIC_OFFSETS and TIMESTAMP modes, we set the specific offsets now; // for other modes (EARLIEST, LATEST, and GROUP_OFFSETS), the offset is lazily determined // when the partition is actually read. switch (startupMode) &#123; //启动模式 case SPECIFIC_OFFSETS: //指定offset开始消费 if (specificStartupOffsets == null) &#123; throw new IllegalStateException( &quot;Startup mode for the consumer set to &quot; + StartupMode.SPECIFIC_OFFSETS + &quot;, but no specific offsets were specified.&quot;); &#125; for (KafkaTopicPartition seedPartition : allPartitions) &#123; Long specificOffset = specificStartupOffsets.get(seedPartition); if (specificOffset != null) &#123; // since the specified offsets represent the next record to read, we subtract // it by one so that the initial state of the consumer will be correct subscribedPartitionsToStartOffsets.put(seedPartition, specificOffset - 1); &#125; else &#123; // default to group offset behaviour if the user-provided specific offsets // do not contain a value for this partition subscribedPartitionsToStartOffsets.put(seedPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET); &#125; &#125; break; case TIMESTAMP: //指定produce时间戳开始消费 if (startupOffsetsTimestamp == null) &#123; throw new IllegalStateException( &quot;Startup mode for the consumer set to &quot; + StartupMode.TIMESTAMP + &quot;, but no startup timestamp was specified.&quot;); &#125; for (Map.Entry&lt;KafkaTopicPartition, Long&gt; partitionToOffset : fetchOffsetsWithTimestamp(allPartitions, startupOffsetsTimestamp).entrySet()) &#123; subscribedPartitionsToStartOffsets.put( partitionToOffset.getKey(), (partitionToOffset.getValue() == null) // if an offset cannot be retrieved for a partition with the given timestamp, // we default to using the latest offset for the partition ? KafkaTopicPartitionStateSentinel.LATEST_OFFSET // since the specified offsets represent the next record to read, we subtract // it by one so that the initial state of the consumer will be correct : partitionToOffset.getValue() - 1); &#125; break; default: for (KafkaTopicPartition seedPartition : allPartitions) &#123; subscribedPartitionsToStartOffsets.put(seedPartition, startupMode.getStateSentinel()); &#125; &#125; if (!subscribedPartitionsToStartOffsets.isEmpty()) &#123; switch (startupMode) &#123; case EARLIEST: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the earliest offsets: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()); break; case LATEST: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the latest offsets: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()); break; case TIMESTAMP: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from timestamp &#123;&#125;: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), startupOffsetsTimestamp, subscribedPartitionsToStartOffsets.keySet()); break; case SPECIFIC_OFFSETS: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the specified startup offsets &#123;&#125;: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), specificStartupOffsets, subscribedPartitionsToStartOffsets.keySet()); List&lt;KafkaTopicPartition&gt; partitionsDefaultedToGroupOffsets = new ArrayList&lt;&gt;(subscribedPartitionsToStartOffsets.size()); for (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123; if (subscribedPartition.getValue() == KafkaTopicPartitionStateSentinel.GROUP_OFFSET) &#123; partitionsDefaultedToGroupOffsets.add(subscribedPartition.getKey()); &#125; &#125; if (partitionsDefaultedToGroupOffsets.size() &gt; 0) &#123; LOG.warn(&quot;Consumer subtask &#123;&#125; cannot find offsets for the following &#123;&#125; partitions in the specified startup offsets: &#123;&#125;&quot; + &quot;; their startup offsets will be defaulted to their committed group offsets in Kafka.&quot;, getRuntimeContext().getIndexOfThisSubtask(), partitionsDefaultedToGroupOffsets.size(), partitionsDefaultedToGroupOffsets); &#125; break; default: case GROUP_OFFSETS: LOG.info(&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the committed group offsets in Kafka: &#123;&#125;&quot;, getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets.keySet()); &#125; &#125; else &#123; LOG.info(&quot;Consumer subtask &#123;&#125; initially has no partitions to read from.&quot;, getRuntimeContext().getIndexOfThisSubtask()); &#125; &#125;&#125;","categories":[{"name":"flink","slug":"flink","permalink":"http://guoyanlei.top/categories/flink/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://guoyanlei.top/tags/kafka/"},{"name":"flink","slug":"flink","permalink":"http://guoyanlei.top/tags/flink/"}]},{"title":"kafka消息投递语义","slug":"2018111801-kafka消息投递语义","date":"2018-11-18T07:30:00.000Z","updated":"2018-11-19T08:29:08.218Z","comments":true,"path":"2018/11/18/2018111801-kafka消息投递语义/","link":"","permalink":"http://guoyanlei.top/2018/11/18/2018111801-kafka%E6%B6%88%E6%81%AF%E6%8A%95%E9%80%92%E8%AF%AD%E4%B9%89/","excerpt":"kafka支持3种消息投递语义 At most once：最多一次，消息可能会丢失，但不会重复（不确定消息是不是丢失，但是不再发送） At least once：最少一次，消息不会丢失，可能会重复（不确定消息是不是丢失，但是还会再发送） Exactly once：只且一次，消息不丢失不重复，只且消费一次（借助一些手段保证发送的消息是唯一） 但是整体的消息投递语义需要Producer端和Consumer端两者来保证。","text":"kafka支持3种消息投递语义 At most once：最多一次，消息可能会丢失，但不会重复（不确定消息是不是丢失，但是不再发送） At least once：最少一次，消息不会丢失，可能会重复（不确定消息是不是丢失，但是还会再发送） Exactly once：只且一次，消息不丢失不重复，只且消费一次（借助一些手段保证发送的消息是唯一） 但是整体的消息投递语义需要Producer端和Consumer端两者来保证。 Producer 消息生产者端当producer向broker发送一条消息，这时网络出错了，producer无法得知broker是否接受到了这条消息。网络出错可能是发生在消息传递的过程中，也可能发生在broker已经接受到了消息，并返回ack给producer的过程中。 这时，producer会有两种选择： 不管了（At most once），消息可能会丢失 再发一次（At least once），消息可能会重复 想要实现Exactly once，需要给每个Producer在初始化的时候都会被分配一个唯一的PID，Producer向指定的Topic的特定Partition发送的消息都携带一个sequence number（简称seqNum），从零开始的单调递增的。 Broker会将Topic-Partition对应的seqNum在内存中维护，每次接受到Producer的消息都会进行校验；只有seqNum比上次提交的seqNum刚好大一，才被认为是合法的。比它大的，说明消息有丢失；比它小的，说明消息重复发送了。 通过设置ack的值，可以区分多种消息确认机制。producer端的acks设置如下： acks=0 // 消息发了就发了，不等任何响应就认为消息发送成功 acks=1 // leader分片写消息成功就返回响应给producer acks=all（-1） // 当acks=all， min.insync.replicas=2，就要求INSRNC列表中必须要有2个副本都写成功，才返回响应给producer，如果INSRNC中已同步副本数量不足2，就会报异常，如果没有2个副本写成功，也会报异常，消息就会认为没有写成功。 Broker 消息接收端acks=1，表示当leader分片副本写消息成功就返回响应给producer，此时认为消息发送成功。 如果leader写成功但马上挂了，还没有将这个写成功的消息同步给其他的分片副本，那么这个分片此时的ISR列表为空 如果unclean.leader.election.enable=true，就会发生log truncation（日志截取），同样会发生消息丢失。 如果unclean.leader.election.enable=false，那么这个分片上的服务就不可用了，producer向这个分片发消息就会抛异常。 （unclean.leader.election.enable 是否允许不具备ISR资格的replicas选举为leader作为不得已的措施，甚至不惜牺牲部分数据。默认允许。建议允许。数据异常重要的情况例外。） 所以我们设置min.insync.replicas=2，unclean.leader.election.enable=false，producer端的acks=all，这样发送成功的消息就绝不会丢失。 Consumer 消息消费者端所有分片的副本都有自己的log文件（保存消息）和相同的offset值。当consumer没挂的时候，offset直接保存在内存中，如果挂了，就会发生负载均衡，需要consumer group中另外的consumer来接管并继续消费。 consumer消费消息的方式有以下2种; 读取消息后先保存offset，后处理消息（至少一次消费）：保存offset成功，但是消息处理失败，这时来接管的consumer就只能从上次保存的offset继续消费，这种情况下就有可能丢消息，但是保证了at most once语义。 读取消息后先处理消息，后保存offset（最多一次消费）：消息处理成功，但是在保存offset时失败，这时来接管的consumer只能从上一次保存的offset开始消费，这时消息就会被重复消费，也就是保证了at least once语义。","categories":[{"name":"kafka","slug":"kafka","permalink":"http://guoyanlei.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://guoyanlei.top/tags/kafka/"}]},{"title":"kudu-master节点迁移","slug":"2018111101-kudu-master节点迁移","date":"2018-11-11T07:30:00.000Z","updated":"2019-01-08T03:47:23.266Z","comments":true,"path":"2018/11/11/2018111101-kudu-master节点迁移/","link":"","permalink":"http://guoyanlei.top/2018/11/11/2018111101-kudu-master%E8%8A%82%E7%82%B9%E8%BF%81%E7%A7%BB/","excerpt":"kudu遇到问题：master节点配置较低，需要迁移到性能高的节点上，迁移比较麻烦，特此记录 迁移思路： 1）先添加kudu-master（在新节点上初始化数据目录，从已有master上同步过来元数据，刷新 Raft 配置，启动所有master） 2）删除要迁移的master（停掉所有进程，删除目标master，在新节点上重写 master 的 Raft 配置，再启动所有的）","text":"kudu遇到问题：master节点配置较低，需要迁移到性能高的节点上，迁移比较麻烦，特此记录 迁移思路： 1）先添加kudu-master（在新节点上初始化数据目录，从已有master上同步过来元数据，刷新 Raft 配置，启动所有master） 2）删除要迁移的master（停掉所有进程，删除目标master，在新节点上重写 master 的 Raft 配置，再启动所有的） 迁移前准备1. 识别存储目录，kudu的master同tablet一样配置有两个目录 fs_wal_dir：write-ahead-logs目录 /data/kudu/master/wal fs_data_dirs：数据目录 /data/kudu/master/data （线上是/data1/kudu/master/data …） 2. 识别master的PRC端口，默认端口值为 70513. 识别master的UUID123456789101112打开kudu-master web页面：http:&#x2F;&#x2F;node102.bigdata.dmp.local.com:8051&#x2F;masters各master的uuid85e0c097fcf747d286f59acf2ae3cfef LEADER node102.bigdata.dmp.local.comc4fc5ceda4454e00ad1257a6489cedcf FOLLOWER node101.bigdata.dmp.local.comc7873360d8404fe8bcb3b999b8bd3c2a FOLLOWER node103.bigdata.dmp.local.comrpc_addresses &#123; host: &quot;node102.bigdata.dmp.local.com&quot; port: 7051 &#125;rpc_addresses &#123; host: &quot;node101.bigdata.dmp.local.com&quot; port: 7051 &#125;rpc_addresses &#123; host: &quot;node103.bigdata.dmp.local.com&quot; port: 7051 &#125; 迁移1. 停掉所有kudu进程2. 在新节点上格式化数据目录1234567891011121314151617mkdir -p &#x2F;data&#x2F;kudu&#x2F;master&#x2F;sudo -u kudu kudu fs format --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data输出：I1031 15:38:54.215034 3151 env_posix.cc:1460] Not raising process file limit of 1000000; it is already as high as it can goI1031 15:38:54.215198 3151 file_cache.cc:463] Constructed file cache lbm with capacity 400000I1031 15:38:54.218797 3151 fs_manager.cc:377] Generated new instance metadata in path &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;instance:uuid: &quot;607c73cbf5484411a6be7fb0fc0b1554&quot;format_stamp: &quot;Formatted at 2018-10-31 07:38:54 on node104.bigdata.dmp.local.com&quot;I1031 15:38:54.220115 3151 fs_manager.cc:377] Generated new instance metadata in path &#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal&#x2F;instance:uuid: &quot;607c73cbf5484411a6be7fb0fc0b1554&quot;format_stamp: &quot;Formatted at 2018-10-31 07:38:54 on node104.bigdata.dmp.local.com&quot;sudo -u kudu kudu fs dump uuid --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 2&gt;&#x2F;dev&#x2F;null输出：607c73cbf5484411a6be7fb0fc0b1554 3. 在新节点上重写 master 的 Raft 配置12345678910111213141516171819202122232425262728293031sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 00000000000000000000000000000000 85e0c097fcf747d286f59acf2ae3cfef:node102.bigdata.dmp.local.com:7051 c4fc5ceda4454e00ad1257a6489cedcf:node101.bigdata.dmp.local.com:7051 c7873360d8404fe8bcb3b999b8bd3c2a:node103.bigdata.dmp.local.com:7051直接执行，会报错：Not found: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000: No such file or directory (error 2)从正常到master节点上scp过来：scp 00000000000000000000000000000000 root@node104.bigdata.dmp.local.com:&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;chown kudu:kudu 00000000000000000000000000000000再次执行输出：I1031 15:56:30.394467 7935 env_posix.cc:1460] Not raising process file limit of 1000000; it is already as high as it can goI1031 15:56:30.394745 7935 file_cache.cc:463] Constructed file cache lbm with capacity 400000I1031 15:56:30.398227 7935 fs_report.cc:345] Block manager report--------------------1 data directories: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;dataTotal live blocks: 0Total live bytes: 0Total live bytes (after alignment): 0Total number of LBM containers: 0 (0 full)Did not check for missing blocksDid not check for orphaned blocksTotal full LBM containers with extra space: 0 (0 repaired)Total full LBM container extra space in bytes: 0 (0 repaired)Total incomplete LBM containers: 0 (0 repaired)Total LBM partial records: 0 (0 repaired)I1031 15:56:30.398293 7935 fs_manager.cc:263] Time spent opening block manager: real 0.001s user 0.000s sys 0.001sI1031 15:56:30.398685 7935 fs_manager.cc:266] Opened local filesystem: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data,&#x2F;data&#x2F;kudu&#x2F;master&#x2F;waluuid: &quot;607c73cbf5484411a6be7fb0fc0b1554&quot;format_stamp: &quot;Formatted at 2018-10-31 07:38:54 on node104.bigdata.dmp.local.com&quot;I1031 15:56:30.401140 7935 tool_action_local_replica.cc:257] Backed up current config to &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000.pre_rewrite.1540972590398731 4. 启动现有的 master5. 使用以下命令将 master 数据复制到每个新 master，在每台新master上执行，只需要连接到一个master上：此步是关键 1234567891011121314151617181920212223242526272829在执行前，确保所有&#x2F;data&#x2F;kudu&#x2F;master&#x2F;目录下内容都是kudu:kudu用户chown -R kudu:kudu &#x2F;data&#x2F;kudu&#x2F;master&#x2F;sudo -u kudu kudu local_replica copy_from_remote --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 00000000000000000000000000000000 node102.bigdata.dmp.local.com:7051输出：I1031 16:08:49.705765 11161 env_posix.cc:1460] Not raising process file limit of 1000000; it is already as high as it can goI1031 16:08:49.706120 11161 file_cache.cc:463] Constructed file cache lbm with capacity 400000I1031 16:08:49.709882 11161 fs_report.cc:345] Block manager report--------------------1 data directories: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;dataTotal live blocks: 0Total live bytes: 0Total live bytes (after alignment): 0Total number of LBM containers: 0 (0 full)Did not check for missing blocksDid not check for orphaned blocksTotal full LBM containers with extra space: 0 (0 repaired)Total full LBM container extra space in bytes: 0 (0 repaired)Total incomplete LBM containers: 0 (0 repaired)Total LBM partial records: 0 (0 repaired)I1031 16:08:49.709954 11161 fs_manager.cc:263] Time spent opening block manager: real 0.002s user 0.000s sys 0.001sI1031 16:08:49.710440 11161 fs_manager.cc:266] Opened local filesystem: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data,&#x2F;data&#x2F;kudu&#x2F;master&#x2F;waluuid: &quot;607c73cbf5484411a6be7fb0fc0b1554&quot;format_stamp: &quot;Formatted at 2018-10-31 07:38:54 on node104.bigdata.dmp.local.com&quot;I1031 16:08:49.733937 11161 tablet_copy_client.cc:166] T 00000000000000000000000000000000 P 607c73cbf5484411a6be7fb0fc0b1554: Tablet Copy client: Beginning tablet copy session from remote peer at address node102.bigdata.dmp.local.com:7051I1031 16:08:49.755112 11161 tablet_copy_client.cc:422] T 00000000000000000000000000000000 P 607c73cbf5484411a6be7fb0fc0b1554: Tablet Copy client: Starting download of 908 data blocks...I1031 16:08:51.402842 11161 tablet_copy_client.cc:385] T 00000000000000000000000000000000 P 607c73cbf5484411a6be7fb0fc0b1554: Tablet Copy client: Starting download of 1 WAL segments...I1031 16:08:52.008639 11161 tablet_copy_client.cc:292] T 00000000000000000000000000000000 P 607c73cbf5484411a6be7fb0fc0b1554: Tablet Copy client: Tablet Copy complete. Replacing tablet superblock. 3. 在新的和老的上面 重写 master 的 Raft 配置此步是关键 12sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 00000000000000000000000000000000 85e0c097fcf747d286f59acf2ae3cfef:node102.bigdata.dmp.local.com:7051 c4fc5ceda4454e00ad1257a6489cedcf:node101.bigdata.dmp.local.com:7051 c7873360d8404fe8bcb3b999b8bd3c2a:node103.bigdata.dmp.local.com:7051 607c73cbf5484411a6be7fb0fc0b1554:node104.bigdata.dmp.local.com:7051 6. 在cm中添加kudu-master，启动所有master至此，已经添加新的master，之后就可以删除不想用的master 7. 停掉所有进程，在cm中删除kudu-master8. 在新的和老的上面 重写 master 的 Raft 配置（排除已删除的）12sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 00000000000000000000000000000000 85e0c097fcf747d286f59acf2ae3cfef:node102.bigdata.dmp.local.com:7051 c4fc5ceda4454e00ad1257a6489cedcf:node101.bigdata.dmp.local.com:7051 607c73cbf5484411a6be7fb0fc0b1554:node104.bigdata.dmp.local.com:7051 9. 分别启动master，Tablet Server线上迁移记录12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394-- 迁移前准备36c7f0a6a98a45ce8472fa747d5ac1dd FOLLOWER rpc_addresses &#123; host: &quot;node1.ikh.bigdata.dmp.com&quot; port: 7051 &#125; 7b0e8b0afc934038ac4afccb05372bb7 LEADER rpc_addresses &#123; host: &quot;node3.ikh.bigdata.dmp.com&quot; port: 7051 &#125; 86961f7799e94afa97c2f2be6773141d FOLLOWER rpc_addresses &#123; host: &quot;node2.ikh.bigdata.dmp.com&quot; port: 7051 &#125; -- 迁移1. 停掉所有kudu进程2. 在新节点上格式化数据目录mkdir -p &#x2F;data&#x2F;kudu&#x2F;master&#x2F;chown -R kudu:kudu master&#x2F;mkdir -p &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;mkdir -p &#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;chown -R kudu:kudu &#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F;sudo -u kudu kudu fs format --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F;从输出中获取uuid：c6a414aeddaa469b9953df0c71fbe245 node51.ikh.bigdata.dmp.comda6b236ddd48464eb064c2b8e859ce1e node52.ikh.bigdata.dmp.com86ec16fbeaca455b840c800631ec14c1 node53.ikh.bigdata.dmp.com3. 从正常的master节点上把Raft配置scp过来：scp &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000 root@node51.ikh.bigdata.dmp.com:&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;scp &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000 root@node52.ikh.bigdata.dmp.com:&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;scp &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000 root@node53.ikh.bigdata.dmp.com:&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;chown kudu:kudu &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;000000000000000000000000000000004. 把已有的Raft配置写入新的master节点sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F; 00000000000000000000000000000000 36c7f0a6a98a45ce8472fa747d5ac1dd:node1.ikh.bigdata.dmp.com:7051 7b0e8b0afc934038ac4afccb05372bb7:node3.ikh.bigdata.dmp.com:7051 86961f7799e94afa97c2f2be6773141d:node2.ikh.bigdata.dmp.com:70515. cm中启动现有的master6. 将现有的master中的文件块数据copy到新master上，只需连接其中一个现有mastersudo -u kudu kudu local_replica copy_from_remote --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F; 00000000000000000000000000000000 node1.ikh.bigdata.dmp.com:70517. 停掉master8. 在新的和老的所有master上面重写 Raft 配置，其中新master的uuid是第2步中的sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F; 00000000000000000000000000000000 36c7f0a6a98a45ce8472fa747d5ac1dd:node1.ikh.bigdata.dmp.com:7051 7b0e8b0afc934038ac4afccb05372bb7:node3.ikh.bigdata.dmp.com:7051 86961f7799e94afa97c2f2be6773141d:node2.ikh.bigdata.dmp.com:7051 c6a414aeddaa469b9953df0c71fbe245:node51.ikh.bigdata.dmp.com:7051 da6b236ddd48464eb064c2b8e859ce1e:node52.ikh.bigdata.dmp.com:7051 86ec16fbeaca455b840c800631ec14c1:node53.ikh.bigdata.dmp.com:70519. 在cm中添加新的kudu-master角色，启动所有master10. 停掉所有进程，在cm中删除kudu-master11. 在新的和老的上面 重写 master 的 Raft 配置（排除已删除的）sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F; 00000000000000000000000000000000 c6a414aeddaa469b9953df0c71fbe245:node51.ikh.bigdata.dmp.com:7051 da6b236ddd48464eb064c2b8e859ce1e:node52.ikh.bigdata.dmp.com:7051 86ec16fbeaca455b840c800631ec14c1:node53.ikh.bigdata.dmp.com:705112. 启动master，启动tablet server--注意：迁移后之前在impala上建的kudu&quot;外部表&quot;就不能读了，因为它们默认指定的是老的kudu-master地址解决办法：- 在impala上删掉这些&quot;外部表&quot;（删除过程可能需要等几分钟）- 之后再重新建表drop table prod.ods_kudu_liyue_dsp_issue_log_1dcreate EXTERNAL table prod.ods_kudu_liyue_dsp_issue_log_1d stored as kuduTBLPROPERTIES(&#39;EXTERNAL&#39;&#x3D;&#39;TRUE&#39;,&#39;kudu.table_name&#39; &#x3D; &#39;ods_kudu_liyue_dsp_issue_log_1d&#39;);","categories":[{"name":"kudu","slug":"kudu","permalink":"http://guoyanlei.top/categories/kudu/"}],"tags":[{"name":"kudu","slug":"kudu","permalink":"http://guoyanlei.top/tags/kudu/"}]},{"title":"从0开始架构学习总结","slug":"2018100801-从0开始架构学习总结","date":"2018-10-08T07:30:00.000Z","updated":"2018-11-19T04:00:39.260Z","comments":true,"path":"2018/10/08/2018100801-从0开始架构学习总结/","link":"","permalink":"http://guoyanlei.top/2018/10/08/2018100801-%E4%BB%8E0%E5%BC%80%E5%A7%8B%E6%9E%B6%E6%9E%84%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","excerpt":"架构基础架构概念 软件架构指软件系统的顶层结构。 子系统：子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中一部分 框架：关注的是“规范”（Framework），如Spring MVC框架 架构：关注的是“结构”（Architecture） 模块：逻辑的角度来拆分系统后，得到的单元就是“模块”，主要目的是职责分离 组件：从物理的角度来拆分系统后，得到的单元就是“组件”，主要目的是单元复用 一句话总结：架构是顶层设计；框架是面向编程或配置的半成品；组件是从技术维度上的复用；模块是从业务维度上职责的划分；系统是相互协同可运行的实体。","text":"架构基础架构概念 软件架构指软件系统的顶层结构。 子系统：子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中一部分 框架：关注的是“规范”（Framework），如Spring MVC框架 架构：关注的是“结构”（Architecture） 模块：逻辑的角度来拆分系统后，得到的单元就是“模块”，主要目的是职责分离 组件：从物理的角度来拆分系统后，得到的单元就是“组件”，主要目的是单元复用 一句话总结：架构是顶层设计；框架是面向编程或配置的半成品；组件是从技术维度上的复用；模块是从业务维度上职责的划分；系统是相互协同可运行的实体。 架构设计的目的 架构设计的主要目的是为了解决软件系统复杂度带来的问题。 一句话总结：架构即(重要)决策，是在一个有约束的盒子里去求解或接近最合适的解。这个有约束的盒子是团队经验、成本、资源、进度、业务所处阶段等所编织、掺杂在一起的综合体(人，财，物，时间，事情等)。架构无优劣，但是存在恰当的架构用在合适的软件系统中，而这些就是决策的结果。 软件系统复杂度的来源1. 高性能 单台计算机内部为了高性能带来的复杂度（垂直维度） 多台计算机集群为了高性能带来的复杂度（水平维度） 垂直维度方案比较适合业务阶段早期和成本可接受的阶段，该方案是提升性能最简单直接的方式，但是受成本与硬件能力天花板的限制。 水平维度方案所带来的好处要在业务发展的后期才能体现出来。起初，该方案会花费更多的硬件成本，另外一方面对技术团队也提出了更高的要求；但是，没有垂直方案的天花板问题。一旦达到一定的业务阶段，水平维度是技术发展的必由之路。 2. 高可用 系统无中断地执行其功能的能力 计算高可用（任务分配器，分配器于业务服务器的交互，分配算法等） 存储高可用（CAP定理：存储高可用不可能同时满足“一致性、可用性、分区容错性”，最多满足其中两个） 实现高可用的本质：“冗余”（高性能增加机器目的在于“扩展”处理性能；高可用增加机器目的在于“冗余”处理单元） 高可用的解决方法不是解决，而是减少或者规避，而规避某个问题的时候，一般都会引发另一个问题，只是这个问题比之前的小，高可用的设计过程其实也是一个取舍的过程。 3. 可扩展 系统为了应对将来需求变化而提供的一种扩展能力，当有新的需求出现时，系统不需要或者仅需要少量修改就可以支持，无须整个系统重构或者重建。 设计具备良好可扩展性的系统：1）从业务维度。对业务深入理解，对可预计的业务变化进行预测。2）从技术维度。利用扩展性好的技术，实现对变化的封装。 4. 低成本 低成本本质上是与高性能和高可用冲突的，当无法设计出满足成本要求的方案，就只能协调并调整成本目标 一般通过创新达到低成本目标： 引入新技术 开创一个全新技术领域 5. 安全 功能安全（防小偷）：减少系统潜在的缺陷，阻止黑客破坏行为 架构安全（防强盗）：保护系统不受恶意访问和攻击，保护系统的重要数据不被窃取 6. 规模 规模带来复杂度的主要原因就是“量变引起质变”，当数量超过一定的阈值后，复杂度会发生质的变化 规模问题需要与高性能、高可用、高扩展、高伸缩性统一考虑。常采用“分而治之，各个击破”的方法策略。 架构设计三原则合适原则 合适优于业界领先 简单原则 简单优于复杂 演化原则 演化优于一步到位 架构设计流程第一步：识别复杂度 将主要的复杂度问题列出来，然后根据业务、技术、团队等综合情况进行排序，优先解决当前面临的最主要的复杂度问题。 具体做法： 构建复杂度的来源清单——高性能、可用性、扩展性、安全、低成本、规模等。 结合需求、技术、团队、资源等对上述复杂度逐一分析是否需要？是否关键？ 按照上述的分析结论，得到复杂度按照优先级的排序清单，越是排在前面的复杂度，就越关键，就越优先解决。 第二步：设计备选方案 备选方案不要过于详细。备选阶段解决的是技术选型问题，而不是技术细节。 备选方案的数量以 3~5个为最佳。 备选方案的技术差异要明显。 备选方案不要只局限于已经熟悉的技术。 第三步：评估和选择备选方案 列出我们需要关注的质量属性点，然后分别从这些质量属性的维度去评估每个方案，再综合挑选适合当时情况的最优方案。 质量属性：性能、可用性、硬件成本、项目投入、复杂度、安全性、可扩展性等 第四步：详细方案设计 详细方案设计是将方案涉及的关键技术细节给确定下来 高性能架构模式 高性能数据库集群读写分离 基本原理是将数据库读写操作分散到不同的节点上。 主从：“从机”是需要提供读数据的功能的 主备：一般被认为仅仅提供备份功能，不提供访问功能 主从基本实现： 数据库服务器搭建主从集群，一主一从、一主多从都可以 数据库主机负责读写操作，从机只负责读操作。 数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据。 业务服务器将写操作发给数据库主机，将读操作发给数据库从机。 需解决的问题： 复制延迟：数据同步延迟 写操作后的读操作指定发给数据库主服务器 读从机失败后再读一次主机 关键业务读写操作全部指向主机，非关键业务采用读写分离 分配机制：将读写区分开 程序代码封装（在代码中抽象一个数据访问层，Hibernate） 中间件封装（独立一套系统出来，实现读写操作分离和数据库服务器连接的管理，MySQL Router） 总结：并不是说一有性能问题就上读写分离，而是应该先优化，例如优化慢查询，调整不合理的业务逻辑，引入缓存等，只有确定系统没有优化空间后，才考虑读写分离或者集群。 分库分表业务分库 业务分库指的是按照业务模块将数据分散到不同的数据库服务器。 需注意的问题： JOIN操作问题 事务问题 成本问题 分表 同一业务的单表数据进行拆分 垂直分表：拆分字段到不同的表 水平分表：拆分记录到不同的表 水平分表的路由问题： 范围路由：选取有序的数据列（例如，整形、时间戳等）作为路由的条件，不同分段分散到不同的数据库表中。 Hash路由：选取某个列（或者某几个列组合也可以）的值进行Hash运算，然后根据 Hash 结果分散到不同的数据库表中。 配置路由：配置路由就是路由表，用一张独立的表来记录路由信息。 高性能NoSQL常见的NoSQL方案分为4类。 K-V 存储：解决关系数据库无法存储数据结构的问题，以Redis为代表 Redis 的 Value 是具体的数据结构，包括string、hash、list、set、sorted set、bitmap 和 hyperloglog，常被称为数据结构服务器 Redis 的事务只能保证隔离性和一致性（I 和 C），无法保证原子性和持久性（A 和 D） 文档数据库：解决关系数据库强schema约束的问题，以MongoDB为代表 新增字段简单，历史数据不回出错 很容易存储复杂数据 代价是不支持事物、无法实现关系数据库的 join 操作 列式数据库：解决关系数据库大数据场景下的I/O问题，以HBase为代表 读取表中某些列时，只需要把需要的列读取到内存中（节省I/O） 列式存储还具备更高的存储压缩比，能够节省更多的存储空间 需要频繁地更新多个列时，磁盘是随机写操作，效率不高 全文搜索引擎：解决关系数据库的全文搜索性能问题，以Elasticsearch为代表 技术原理被称为“倒排索引”，建立单词到文档的索引 全文搜索引擎的索引对象是单词和文档，而关系数据库的索引对象是键和行 高性能缓存 缓存就是为了弥补存储系统在复杂业务场景（经过复杂运算过的数据、读多写少等）下的不足，其基本原理是将可能重复使用的数据放到内存中，一次生成、多次使用，避免每次使用都去访问存储系统。 缓存架构的设计要点 缓存穿透 缓存没有发挥作用，业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据。 缓存雪崩 当缓存失效（过期）后引起系统性能急剧下降的情况。由于旧的缓存已经被清除，新的缓存还未生成，并且处理这些请求的线程都不知道另外有一个线程正在生成缓存，因此所有的请求都会去重新生成缓存，都会去访问存储系统，从而对存储系统造成巨大的性能压力。这些压力又会拖慢整个系统。 解放方法： 更新锁：对缓存更新操作进行加锁保护，保证只有一个线程能够进行缓存更新，未能获取更新锁的线程要么等待锁释放后重新读取缓存，要么就返回空值或默认值。 后台更新：后台线程来更新缓存，而不是由业务线程来更新缓存，缓存本身的有效期设置为永久，后台线程定时更新缓存。 缓存热点 如果大部分甚至所有的业务请求都命中同一份缓存数据，则这份数据所在的缓存服务器的压力也很大。 解决方法：复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力。（不同的缓存副本不要设置统一的过期时间，否则就会出现所有缓存副本同时生成同时失效的情况，从而引发缓存雪崩） 高性能服务器 尽量提升单服务器的性能，将单服务器的性能发挥到极致。 如果单服务器无法支撑性能，设计服务器集群方案。 单服务器的高性能 单服务器高性能的关键之一就是设计服务器采取的 “并发模型” 服务器如何处理连接：I/O 模型：阻塞、非阻塞、同步、异步 服务器如何处理请求：进程模型：单进程、多进程、多线程 单服务器高性能模式（PPC 和 TPC 模式）： PPC（Process Per Connection）：指每次有新的连接就新建一个 “进程” 去专门处理这个连接的请求 prefork：系统在启动的时候就预先创建好进程，然后才开始接受用户的请求，省去fork进程操作 TPC（Thread Per Connection）：指每次有新的连接就新建一个 “线程” 去专门处理这个连接的请求 prethread：预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作 单服务器高性能模式（Reactor 和 Proactor 模式）： 资源复用，即不再单独为每个连接创建进程，而是创建一个进程池，将连接分配给进程，一个进程可以处理多个连接的业务. IO操作分两个阶段【去饭店点餐排队】： 等待数据准备好(读到内核缓存) 【准备菜品】 将数据从内核缓存读到用户空间(进程空间) 【端走吃到肚里】 一般来说1花费的时间远远大于2。 同步阻塞IO：1上阻塞2上也阻塞【付完钱在收银台等着，菜好之后取走离开】 同步非阻塞IO：1上非阻塞2上阻塞（Reactor模型）【付完钱领号，等待叫号，自己来取】 异步非阻塞IO：1上非阻塞2上非阻塞是（Proactor模型）【付完钱领号，回到座位干别的，菜好后服务员把菜端来】 服务器集群的高性能 高性能集群的复杂性：需要一个任务分配器（负载均衡），以及任务分配算法 负载均衡负载均衡的分类： DNS负载均衡：实现地理级别的均衡（简单、成本低） 硬件负载均衡：用于负载均衡的基础网络设备（价格昂贵、扩展能力差） 软件负载均衡：常见的Nginx（网络7层负载均衡）和 LVS（4层负载均衡） DNS 负载均衡用于实现地理级别的负载均衡；硬件负载均衡用于实现集群级别的负载均衡；软件负载均衡用于实现机器级别的负载均衡。 任务分配算法 轮询：按照顺序轮流分配到服务器上 加权轮询：根据服务器权重进行任务分配 负载最低优先：将任务分配给当前负载最低的服务器（站在服务器的角度） 性能最优类：优先将任务分配给处理速度最快的服务器（站在客户端的角度） Hash类：根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上 高可用架构模式 分布式系统架构CAP定理 在一个分布式系统（指互相连接并共享数据的节点的集合）中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲。 一致性（C）：对某个指定的客户端来说，读操作保证能够返回最新的写操作结果 可用性（A）：非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应） 分区容错型（P）：当出现网络分区后，系统能够继续“履行职责”。 设计分布式系统架构时必须选择 P（分区容忍），因为网络本身无法做到 100%可靠，有可能出故障，所以分区是一个必然的现象。 CP：发生分区现象后，有问题的节点不能同步数据，为了保证一致性（C），请求数据时只能返回Error，就不能满足可用性（A） AP：发生分区现象后，有问题的节点不能同步数据，为了保证可用性（A），请求数据时只能返回旧数据，就不能满足一致性（C） 注意： CAP 关注的粒度是数据，而不是整个系统。 CAP 是忽略网络延迟的（数据能够瞬间复制到所有节点） 既要考虑分区发生时选择 CP 还是 AP，也要考虑分区没有发生时如何保证 CA。 放弃并不等于什么都不做，需要为分区恢复后做准备。 BASE理论：即使无法做到强一致性（CAP），但应用可以采用适合的方式达到最终一致性。 基本可用（Basically Available）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用 软状态（Soft State）：允许系统存在中间状态，而该中间状态不会影响系统整体可用性 最终一致性（Eventual Consistency）：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态 ACID：数据库管理系统为了保证 “事务” 的正确性而提出来的一个理论。 Atomicity（原子性）：要么全部完成，要么全部不完成 Consistency（一致性）：开始和结束后数据库完整型没有破坏 Isolation（隔离性）：多个并发事务同时对数据进行读写和修改的能力 Durability（持久性）：事务处理结束后，对数据的修改就是永久的 高可用存储架构双机高可用架构（隐含的假设：主机能够存储所有数据） 主备复制（备机仅仅只为备份，并没有提供读写操作，硬件成本上有浪费） 主从复制（主机负责读写操作，从机只负责读操作，不负责写操作） 主备/主从切换（在原有方案的基础上增加“切换”功能，即系统自动决定主机角色，并完成角色切换） 主主复制（两台机器都是主机，互相将数据复制给对方，任意连接一台进行读写操作） 主备切换架构： 互连式：主备机直接建立状态传递的渠道 中介式：主备机之间不直接连接，而都去连接中介，并且通过中介来传递状态 模拟式：主备机之间并不传递任何状态数据，而是备机模拟成一个客户端，向主机发起模拟的读写操作，根据读写操作的响应情况来判断主机的状态 集群式高可用存储架构数据集群 数据集中集群（数据都只能往主机中写，而读操作可以参考主备、主从架构进行灵活多变，如zookeeper集群-ZAB算法） 数据分散集群（每台服务器都会负责存储一部分数据，每台服务器又会备份一部分数据，如Hadoop集群） 数据分区 集中式：所有的分区都将数据备份到备份中心 互备式：每个分区备份另外一个分区的数据 独立式：每个分区自己有独立的备份中心 计算高可用 主备：主机执行所有计算任务，当主机故障时，任务分配器不会自动将计算任务发送给备机 主从：任务分配器需要将任务进行分类，确定哪些任务可以发送给主机执行，哪些任务可以发送给备机执行 集群 对称集群：负载均衡集群（集群中每个服务器的角色都是一样的，都可以执行所有任务） 非对称集群：集群中的服务器分为多个不同的角色，不同的角色执行不同的任务 业务高可用存储高可用和计算高可用都是为了解决部分服务器故障的场景下，如何保证系统能够继续提供服务。在一些极端场景下，有可能所有服务器都出现故障，此时就需要设计异地多活架构。 异地：指地理位置上不同的地方 多活：指不同地理位置上的系统都能够提供业务服务 同城异区（同一个城市不同区的多个机房） 跨城异地（不同城市的多个机房） 跨国异地（不同国家的多个机房） 四大设计技巧: 保证 “核心” 业务的异地多活 保证核心数据 “最终” 一致性 采用多种手段同步数据 只保证绝大部分用户的异地多活 采用多种手段，保证绝大部分用户的核心业务异地多活 四步： 业务分级（挑选出核心的业务，只为核心业务设计异地多活） 数据分类（识别所有的数据及数据特征） 数据同步 异常处理 接口级的故障接口级故障表现为：系统并没有宕机，网络也没有中断，但业务却出现问题了（业务响应缓慢、大量访问超时、大量访问出现异常） 优先保证核心业务和优先保证绝大部分用户 解决方案： 降级：（应对系统自身的故障）将某些业务或者接口的功能降低，可以是只提供部分功能，也可以是完全停掉所有功能 系统后门降级 独立降级系统 熔断：（应对依赖的外部系统故障的情况）壮士断腕，停掉外部依赖的调用 限流：（直接拒绝用户）从系统功能优先级的角度考虑如何应对故障，只允许系统能够承受的访问量进来，超出系统访问能力的请求将被丢弃 基于请求限流 基于资源限流 排队：（让用户等待一段时间）使用 Kafka 这类消息队列来缓存用户请求 可扩展架构模式 可扩展的基本思想 总结为一个字：拆！ 流程 &gt; 服务 &gt; 功能 面向流程拆分：将整个业务流程拆分为几个阶段，每个阶段作为一部分（得到分层架构） 面向服务拆分：将系统提供的服务拆分，每个服务作为一部分（SOA、微服务） 面向功能拆分：将系统提供的功能拆分，每个功能作为一部分（微内核架构） 分层架构 本质在于隔离关注点，即每个层中的组件只会处理本层的逻辑。 C/S 架构、B/S 架构（划分的对象是整个业务系统，划分的维度是用户交互） MVC 架构、MVP 架构（划分的对象是单个业务子系统，划分的维度是职责） 逻辑分层架构（划分的对象可以是整个或单个业务子系统，划分的维度也是职责，逻辑分层架构中的层是自顶向下依赖的） SOA架构（面向服务架构） SOA 出现的背景是企业内部的系统重复建设且效率低下，SOA 解决了传统 IT 系统重复建设和扩展效率低的问题。 SOA是集成的思想，是解决服务孤岛打通链条，是无奈之举。 ESB集中化的管理带来了性能不佳，厚重等问题。也无法快速扩展。不适合互联网的业务特点 微服务架构 SOA 和微服务本质上是两种不同的架构设计理念，只是在“服务”这个点上有交集而已。 微服务是 SOA 的实现方式（服务粒度比SOA细） 微服务是去掉 ESB 后的 SOA（更为轻量级，例如HTTP RESTful） 微服务是一种和 SOA 相似但本质上不同的架构理念（服务交付快，适合互联网） 微服务拆分方法（维度）： 基于业务逻辑拆分（系统中的业务模块按照职责范围识别出来，每个单独的业务模块拆分为一个独立的服务） 基于可扩展拆分（将系统中的业务模块按照稳定性排序，将已经成熟和改动不大的服务拆分为稳定服务，将经常变化和迭代的服务拆分为变动服务） 基于可靠性拆分（将系统中的业务模块按照优先级排序，将可靠性要求高的核心服务和可靠性要求低的非核心服务拆分开来，然后重点保证核心服务的高可用） 基于性能拆分（将性能要求高或者性能压力大的模块拆分出来，避免性能压力大的服务影响其他服务） 微服务基础设施： 自动化测试（通过自动化测试系统来完成绝大部分测试回归的工作） 自动化部署（自动化部署的系统来完成大量的部署操作） 配置中心（有的运行期配置需要动态修改并且所有节点即时生效，人工操作是无法做到的） 接口框架（HTTP/REST 或者 RPC 方式） API网关（内部的微服务之间是互联互通的，相互之间的访问都是点对点的，需要一个统一的 API 网关，负责外部系统的访问操作） 服务发现 自理式：每个微服务自己完成服务发现 代理式：由负载均衡系统来完成微服务之间的服务发现 服务路由（进行某次调用请求时，我们还需要从所有符合条件的可用微服务节点中挑选出一个具体的节点发起请求） 服务容错（从整体上来看，系统中某个微服务出故障的概率会大大增加） 服务监控（需要服务监控系统来完成微服务节点的监控） 服务跟踪（跟踪某一个请求在微服务中的完整路径） 服务安全（接入安全、数据安全、传输安全） 微内核架构（插件化架构）两类组件： 核心系统（core system）：负责和具体业务功能无关的通用功能（模块加载、模块间通信等） 插件模块（plug-in modules）：负责实现具体的业务逻辑 互联网架构模式 “存储层”技术 SQL存储：以对业务透明的形式提供资源分配、数据备份、迁移、容灾、读写分离、分库分表等一系列服务 NoSQL存储：存储复杂数据、高性能 小文件存储（图片等）：HBase、Hadoop 大文件存储（业务大数据如视频等、海量日志数据）：HBase、HDFS “开发层”技术 开发框架：SSH、SpringMVC等（只是负责完成业务功能的开发） Web服务器：Tomcat、Nginx等（真正能够运行起来给用户提供服务） 容器：Docker等（一个虚拟化或者容器技术） “服务层”技术 配置中心：集中管理各个系统的配置 服务中心：解决跨系统依赖的“配置”和“调度”问题 消息队列：为了实现这种跨系统异步通知的中间件系统 “网络层”技术 负载均衡（将请求均衡地分配到多个系统上） DNS：实现地理级别的均衡 Nginx（7层）、LVS（4层）、F5（4层）：用于同一地点内机器级别的负载均衡 CDN：将内容缓存在离用户最近的地方，用户访问的是缓存的内容（“以空间换时间”） 多机房（主要目标是灾备） 同城多机房 跨城多机房 跨国多机房 多中心（以多机房为前提，要求每个中心都同时对外提供服务，且业务能够自动在多中心之间切换，故障后不需人工干预自动恢复） “用户层”和“业务层”技术 用户管理（单点登录，授权登陆） 消息推送（分为短信、邮件、站内信、App推送） 存储云、图片云（买云服务最快最经济） “平台”技术 运维平台：配置、部署、监控、应急 标准化（制定运维标准，规范配置管理、部署流程、监控指标、应急能力等） 平台化（在运维标准化的基础上，将运维的相关操作都集成到运维平台中） 自动化（将重复操作固化下来，由系统自动完成） 可视化（为了提升数据查看效率） 测试平台：单元测试、集成测试、接口测试、性能测试 用例管理（为了能够重复执行这些测试用例，测试平台需要将用例管理起来） 资源管理（具体的运行环境：硬件、软件、业务系统） 任务管理（将测试用例分配到具体的资源上执行，跟踪任务的执行情况） 数据管理（测试完成后，记录各种相关的数据） 数据平台 数据管理（数据采集、数据存储、数据访问和数据安全） 数据分析（数据统计、数据挖掘、机器学习、深度学习） 数据应用（包括在线业务如推荐、广告等，也包括离线业务如报表等） 管理平台 身份认证 权限管理","categories":[{"name":"学架构","slug":"学架构","permalink":"http://guoyanlei.top/categories/%E5%AD%A6%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"架构","slug":"架构","permalink":"http://guoyanlei.top/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"kafka手动修改副本数ReplicationFactor","slug":"2018092401-kafka手动修改副本数ReplicationFactor","date":"2018-09-24T07:30:00.000Z","updated":"2018-09-25T09:33:12.299Z","comments":true,"path":"2018/09/24/2018092401-kafka手动修改副本数ReplicationFactor/","link":"","permalink":"http://guoyanlei.top/2018/09/24/2018092401-kafka%E6%89%8B%E5%8A%A8%E4%BF%AE%E6%94%B9%E5%89%AF%E6%9C%AC%E6%95%B0ReplicationFactor/","excerpt":"kafka手动修改副本数ReplicationFactor问题描述上周对kafka集群添加了一个broker（104），然后又删掉了，但是这个被删掉的broker一直存在于一个topic的ReplicationFactor中。 在kafka-manager中的表现是： 使用如下命令查看这个topic 1.&#x2F;kafka-topics.sh --zookeeper node101.zk.dmp.dmp.com:2181,node102.zk.dmp.dmp.com:2181,node103.zk.dmp.dmp.com:2181 --describe --topic pv-event","text":"kafka手动修改副本数ReplicationFactor问题描述上周对kafka集群添加了一个broker（104），然后又删掉了，但是这个被删掉的broker一直存在于一个topic的ReplicationFactor中。 在kafka-manager中的表现是： 使用如下命令查看这个topic 1.&#x2F;kafka-topics.sh --zookeeper node101.zk.dmp.dmp.com:2181,node102.zk.dmp.dmp.com:2181,node103.zk.dmp.dmp.com:2181 --describe --topic pv-event 尝试在kafka-manager中（Manual Partition Assignments）进行手动修改，发现并不能实现，如图所示： 因此只能选择其他方案，经搜索发现好多是增加副本数的方法，但是不确定减少副本数是不是生效。 增加副本参考方案 将验证参考文章的方法是可以的，特此总结如下。 解决方案：借助配置文件手动修改副本数首先，新建一个json配置文件replication.json 12345678910111213141516171819202122232425262728293031323334&#123; &quot;version&quot;: 1, &quot;partitions&quot;: [ &#123; &quot;topic&quot;: &quot;pv-event&quot;, &quot;partition&quot;: 0, &quot;replicas&quot;: [ 103,102,101 ] &#125;, &#123; &quot;topic&quot;: &quot;pv-event&quot;, &quot;partition&quot;: 1, &quot;replicas&quot;: [ 102,103,101 ] &#125;, &#123; &quot;topic&quot;: &quot;pv-event&quot;, &quot;partition&quot;: 2, &quot;replicas&quot;: [ 103,102,101 ] &#125;, ... ... &#123; &quot;topic&quot;: &quot;pv-event&quot;, &quot;partition&quot;: 17, &quot;replicas&quot;: [ 103,104,102,101 ] &#125; ]&#125; 其次，使用kafka-reassign-partitions.sh工具来执行 1234bin&#x2F;kafka-reassign-partitions.sh --zookeeper node101.zk.dmp.dmp.com:2181,node102.zk.dmp.dmp.com:2181,node103.zk.dmp.dmp.com:2181 --reassignment-json-file replication.json --execute# 运行过程中可以通过以下命令查看bin&#x2F;kafka-reassign-partitions.sh --zookeeper node101.zk.dmp.dmp.com:2181,node102.zk.dmp.dmp.com:2181,node103.zk.dmp.dmp.com:2181 --reassignment-json-file replication.json --verify 运行过程： 运行结果：","categories":[{"name":"kafka","slug":"kafka","permalink":"http://guoyanlei.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://guoyanlei.top/tags/kafka/"}]},{"title":"kafka在zk中的目录结构","slug":"2018092001-kafka在zk中的目录结构","date":"2018-09-20T07:30:00.000Z","updated":"2018-09-21T06:49:59.543Z","comments":true,"path":"2018/09/20/2018092001-kafka在zk中的目录结构/","link":"","permalink":"http://guoyanlei.top/2018/09/20/2018092001-kafka%E5%9C%A8zk%E4%B8%AD%E7%9A%84%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/","excerpt":"kafka在zk中的目录结构 /brokers当一个broker启动时,会向zookeeper注册自己持有的topic和partitions信息 /brokers/ids每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复)，此节点为临时znode(EPHEMERAL)","text":"kafka在zk中的目录结构 /brokers当一个broker启动时,会向zookeeper注册自己持有的topic和partitions信息 /brokers/ids每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复)，此节点为临时znode(EPHEMERAL) 12345678910111213141516171819202122232425&gt; ls &#x2F;brokers&#x2F;ids&#x2F;101[101, 102, 103]&gt; get &#x2F;brokers&#x2F;ids&#x2F;101&#123; &quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;, &quot;endpoints&quot;:[&quot;PLAINTEXT:&#x2F;&#x2F;node104.kafka.bigdata.dmp.com:9092&quot;], &quot;jmx_port&quot;:8999, &#x2F;&#x2F;jmx端口号 &quot;host&quot;:&quot;node104.kafka.bigdata.dmp.com&quot;, &#x2F;&#x2F;主机名或ip地址 &quot;timestamp&quot;:&quot;1537425599417&quot;, &#x2F;&#x2F;broker初始启动时的时间戳 &quot;port&quot;:9092, &#x2F;&#x2F;broker的服务端端口号，由server.properties中参数port确定 &quot;version&quot;:4 &#x2F;&#x2F;版本编号默认为1&#125;cZxid &#x3D; 0x7183581d4ctime &#x3D; Thu Sep 20 14:39:59 CST 2018mZxid &#x3D; 0x7183581d4mtime &#x3D; Thu Sep 20 14:39:59 CST 2018pZxid &#x3D; 0x7183581d4cversion &#x3D; 0dataVersion &#x3D; 0aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x36324802b64f5d0dataLength &#x3D; 230numChildren &#x3D; 0 /brokers/topics包含各topic的partition状态信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt; ls &#x2F;brokers&#x2F;topics [pv-event, pv-event-nginx-log, gdt-request, leopard-module-stats-app, wolves-event, ad-track-nginx-log, ...]&gt; ls &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log[partitions]&gt; get &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log&#123; &quot;version&quot;:1, &quot;partitions&quot;: &#123; &quot;8&quot;:[103,101,102], &#x2F;&#x2F;同步副本组brokerId列表(ISR) &quot;4&quot;:[102,101,103],&quot;11&quot;:[103,102,101],&quot;9&quot;:[101,103,102],&quot;5&quot;:[103,102,101],&quot;10&quot;:[102,101,103],&quot;6&quot;:[101,102,103],&quot;1&quot;:[102,103,101],&quot;0&quot;:[101,102,103],&quot;2&quot;:[103,101,102],&quot;7&quot;:[102,103,101],&quot;3&quot;:[101,103,102] &#125;&#125;cZxid &#x3D; 0x200000143ctime &#x3D; Thu Oct 12 16:07:33 CST 2017mZxid &#x3D; 0x71306a4eemtime &#x3D; Sun Aug 26 03:25:57 CST 2018pZxid &#x3D; 0x200000146cversion &#x3D; 1dataVersion &#x3D; 13843aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x0dataLength &#x3D; 246numChildren &#x3D; 1&gt; ls &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log&#x2F;partitions[0, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt; ls &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log&#x2F;partitions&#x2F;0[state]&gt; get &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log&#x2F;partitions&#x2F;0&#x2F;state&#123; &quot;controller_epoch&quot;:28, &#x2F;&#x2F;表示kafka集群中的中央控制器选举次数 &quot;leader&quot;:101, &#x2F;&#x2F;该partition选举leader的brokerId &quot;version&quot;:1, &#x2F;&#x2F;版本编号默认为1, &quot;leader_epoch&quot;:382, &#x2F;&#x2F;该partition leader选举次数 &quot;isr&quot;:[101,103,102] &#x2F;&#x2F;[同步副本组brokerId列表]&#125;cZxid &#x3D; 0x200000154ctime &#x3D; Thu Oct 12 16:07:33 CST 2017mZxid &#x3D; 0x7183c9936mtime &#x3D; Thu Sep 20 17:49:19 CST 2018pZxid &#x3D; 0x200000154cversion &#x3D; 0dataVersion &#x3D; 1426aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x0dataLength &#x3D; 87numChildren &#x3D; 0 /brokers/seqid该目录的作用是帮助kafka自动生成broker.id的。 自动生成broker.id的原理是先往/brokers/seqid节点中写入一个空字符串，然后获取返回的Stat信息中的version的值，然后将version的值和reserved.broker.max.id参数配置的值相加可得。之所以是先往节点中写入数据再获取Stat信息，这样可以确保返回的version值大于0，进而就可以确保生成的broker.id值大于reserved.broker.max.id参数配置的值，符合非自动生成的broker.id的值在[0, reserved.broker.max.id]区间的设定。 12345678910111213&gt; get &#x2F;brokers&#x2F;seqidnullcZxid &#x3D; 0x20000000fctime &#x3D; Thu Oct 12 15:35:21 CST 2017mZxid &#x3D; 0x20000000fmtime &#x3D; Thu Oct 12 15:35:21 CST 2017pZxid &#x3D; 0x20000000fcversion &#x3D; 0dataVersion &#x3D; 0 &#x2F;&#x2F;基于此version的值aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x0dataLength &#x3D; 0numChildren &#x3D; 0 /consumers每个consumer都有一个唯一的ID，此id用来标记消费者信息. 该目录下仅展示使用zk进行消费的consumers，如果之间指定kafka节点进行消费，不会在此展示 /consumers/{groupId}/ids12345678910111213141516171819202122232425262728293031&gt; ls &#x2F;consumers[console-consumer-84155, console-consumer-32194, wolves_report, console-consumer-9761, wolves_v2_gdt, console-consumer-63530, wolves, wolves_feedback, wolves_kuaishou, console-consumer-62629, ftrl1, console-consumer-56068, wolves_tuia]&gt; ls &#x2F;consumers&#x2F;wolves_report[ids, owners, offsets]&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;ids[wolves_report_node1.tc.wolves.dmp.com-1536837975646-39504764, wolves_report_node1.tc.wolves.dmp.com-1536838003051-182cc752,...]&gt; get &#x2F;consumers&#x2F;wolves_report&#x2F;ids&#x2F;wolves_report_node1.tc.wolves.dmp.com-1536837975646-39504764&#123; &quot;version&quot;:1, &#x2F;&#x2F;版本编号，默认为1 &quot;subscription&quot;: &#x2F;&#x2F;订阅topic列表 &#123; &quot;wolves-event&quot;:3 &#x2F;&#x2F;consumer中topic消费者线程数 &#125;, &quot;pattern&quot;:&quot;static&quot;, &quot;timestamp&quot;:&quot;1537128878487&quot; &#x2F;&#x2F;consumer启动时的时间戳&#125;cZxid &#x3D; 0x717782b21ctime &#x3D; Mon Sep 17 04:14:38 CST 2018mZxid &#x3D; 0x717782b21mtime &#x3D; Mon Sep 17 04:14:38 CST 2018pZxid &#x3D; 0x717782b21cversion &#x3D; 0dataVersion &#x3D; 0aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x36324802b64ea62dataLength &#x3D; 94numChildren &#x3D; 0 /consumers/{groupId}/owner1234567891011121314151617181920&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;owners [wolves-event] &#x2F;&#x2F; topic&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;owners&#x2F;wolves-event[0, 1, 2] &#x2F;&#x2F; partitionId&gt; get &#x2F;consumers&#x2F;wolves_report&#x2F;owners&#x2F;wolves-event&#x2F;0wolves_report_node1.tc.wolves.dmp.com-1536837527210-1310d8f9-0cZxid &#x3D; 0x717782ba9ctime &#x3D; Mon Sep 17 04:14:40 CST 2018mZxid &#x3D; 0x717782ba9mtime &#x3D; Mon Sep 17 04:14:40 CST 2018pZxid &#x3D; 0x717782ba9cversion &#x3D; 0dataVersion &#x3D; 0aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x26324802b69ea62dataLength &#x3D; 62numChildren &#x3D; 0 /consumers/{groupId}/offset用来跟踪每个consumer目前所消费的partition中最大的offset 1234567891011121314151617181920&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;offsets [wolves-event] &#x2F;&#x2F; topic&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;offsets&#x2F;wolves-event[0, 1, 2] &#x2F;&#x2F; partitionId&gt; get &#x2F;consumers&#x2F;wolves_report&#x2F;offsets&#x2F;wolves-event&#x2F;048800cZxid &#x3D; 0x200e97e36ctime &#x3D; Thu Nov 23 17:22:10 CST 2017mZxid &#x3D; 0x718665858mtime &#x3D; Fri Sep 21 12:02:39 CST 2018pZxid &#x3D; 0x200e97e36cversion &#x3D; 0dataVersion &#x3D; 11910567aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x0dataLength &#x3D; 5numChildren &#x3D; 0 /admin/admin/reassign_partitions用以partitions重分区，reassign结束后会删除该目录 12&gt; ls &#x2F;admin&#x2F;reassign_partitions[] /admin/preferred_replica_election用以partitions各副本leader选举，replica election结束后会删除该目录 12&gt; ls &#x2F;admin&#x2F;reassign_partitions[] /admin/delete_topics管理已删除的topics，broker启动时检查并确保存在 12&gt; ls &#x2F;admin&#x2F;delete_topics[] /controller存储center controller中央控制器所在kafka broker的信息 1234567891011121314151617&gt; get &#x2F;controller&#123; &quot;version&quot;:1, &#x2F;&#x2F;版本编号默认为1 &quot;brokerid&quot;:101, &#x2F;&#x2F;broker唯一编号 &quot;timestamp&quot;:&quot;1537425633921&quot; &#x2F;&#x2F;broker中央控制器变更时的时间戳&#125;cZxid &#x3D; 0x7183583bdctime &#x3D; Thu Sep 20 14:40:33 CST 2018mZxid &#x3D; 0x7183583bdmtime &#x3D; Thu Sep 20 14:40:33 CST 2018pZxid &#x3D; 0x7183583bdcversion &#x3D; 0dataVersion &#x3D; 0aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x36324802b64f5d0dataLength &#x3D; 56numChildren &#x3D; 0 /controller_epoch此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1; 12345678910111213&gt; get &#x2F;controller_epoch28cZxid &#x3D; 0x200000017ctime &#x3D; Thu Oct 12 15:35:21 CST 2017mZxid &#x3D; 0x7183583bemtime &#x3D; Thu Sep 20 14:40:33 CST 2018pZxid &#x3D; 0x200000017cversion &#x3D; 0dataVersion &#x3D; 27aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x0dataLength &#x3D; 2numChildren &#x3D; 0 /config /config/changes broker启动时检查并确保存在，所有broker全程监控child change /config/clients broker启动时检查并确保存在 /config/topics broker启动时检查并确保存在 zookeeper操作命令在确保zookeeper服务启动状态下，通过 bin/zkCli.sh -server xxx:2181 连接 123456781. 显示根目录下、文件： ls &#x2F; 使用 ls 命令来查看当前 ZooKeeper 中所包含的内容2. 显示根目录下、文件： ls2 &#x2F; 查看当前节点数据并能看到更新次数等数据3. 创建文件，并设置初始内容： create &#x2F;zk &quot;test&quot; 创建一个新的 znode节点“ zk ”以及与它关联的字符串4. 获取文件内容： get &#x2F;zk 确认 znode 是否包含我们所创建的字符串5. 修改文件内容： set &#x2F;zk &quot;zkbak&quot; 对 zk 所关联的字符串进行设置6. 删除文件： delete &#x2F;zk 将刚才创建的 znode 删除7. 退出客户端： quit8. 帮助命令： help","categories":[{"name":"kafka","slug":"kafka","permalink":"http://guoyanlei.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://guoyanlei.top/tags/kafka/"}]},{"title":"CDH离线安装记录及常见错误处理","slug":"2018091201-CDH离线安装记录及常见错误处理","date":"2018-09-11T07:30:00.000Z","updated":"2020-11-01T13:47:33.809Z","comments":true,"path":"2018/09/11/2018091201-CDH离线安装记录及常见错误处理/","link":"","permalink":"http://guoyanlei.top/2018/09/11/2018091201-CDH%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/","excerpt":"准备安装包cloudera manager下载地址 CDH下载地址选择自己合适的版本 manifest.json下载地址","text":"准备安装包cloudera manager下载地址 CDH下载地址选择自己合适的版本 manifest.json下载地址 网络配置对于集群中搜索节点都需要做： 修改主机名称 关闭防火墙 \u001d配置免密码登陆对于集群中所有节点，任意两个节点实现免密码登陆。并且开启ssh服务。 \b安装jdk对于集群中节点安装jdk，可以自己安装，也可以通过cloudera manager安装。安装完成之后，配置JAVA_HOME。对于线上，建议自己安装。 安装和配置NTP服务cloudera manager通过ntp服务，保持集群内的时钟同步。时钟偏差会应用服务，特别是kudu，因为kudu的事务需要以来时钟一致。 安装NTP 1yum install ntp 集群内，建议在\b集群内做一个NTP服务器，作为优先级最高的NTP服务器，其它的作为备用。\b 手动同步可以如下命令： 1ntpdate -d NTP服务器的host或者IP 可以通过如下命令查看NTP的统计 1ntpstat 还可以通过ntpq查看ntp服务同步各个服务器的情况 1ntpq -p 若系统是CentOS7的同步时钟的方式建议使用chrony，将ntp切换为chrony参考之前的文章 准备MySQLcloudera-manager、hive、hue、oozie都需要MySQL来存储元数据。所以需要提前准备好一台MySQL。 对于不同不同的服务，采用不同的kudu，不同的用户，进行授权。 安装依赖通过yum安装cloudera-manager的依赖，命令如下： 12345678910111213yum -y install chkconfigyum -y install bind-utilsyum -y install psmiscyum -y install libxsltyum -y install zlibyum -y install sqliteyum -y install cyrus-sasl-plainyum -y install cyrus-sasl-gssapiyum -y install fuseyum -y install portmapyum -y install fuse-libsyum -y install redhat-lsb 安装cloudera manager server 在cloudera manager server所在节点创建cloudera-manager安装\b目录 1mkdir -p &#x2F;data&#x2F;dmp&#x2F;cloudera-manager 将下载好的cloudera-manager安装包cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz解压放在改目录： 1tar -zxvf cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz -C &#x2F;data&#x2F;dmp&#x2F;cloudera-manager 创建cloudera-scm用户 Cloudera管理器服务器和托管服务被配置为在默认情况下使用用户帐户Cloudera-scm，创建具有这个名称的用户是最简单的方法。创建用户，在安装完成后自动使用。 执行如下命令： 1useradd --system --home&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;run&#x2F;cloudera-scm-server&#x2F; --no-create-home --shell&#x3D;&#x2F;bin&#x2F;false --comment &quot;Cloudera SCM User&quot; cloudera-scm 下载MySQL驱动 123cd &#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;share&#x2F;cmf&#x2F;libwget http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;service&#x2F;local&#x2F;repositories&#x2F;hongkong-nexus&#x2F;content&#x2F;Mysql&#x2F;mysql-connector-java&#x2F;5.1.38&#x2F;mysql-connector-java-5.1.38.jar 配置cloudera manager server的数据库 进入cloudera-manager安装目录下的cm-5.12.2/share/cmf/schema/目录中，找到scm_prepare_database.sh，这个脚本是执行cloudera-amanger数据库初始化的脚本。 123cd &#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;share&#x2F;cmf&#x2F;schema&#x2F;.&#x2F;scm_prepare_database.sh mysql cm -h $&#123;MySQL节点的host或者ip&#125; -u$&#123;MySQL用户&#125; -p$&#123;MySQL密码&#125; --scm-host $&#123;cloudera manager server的ip或者host&#125; scm scm scm 看到如下信息，就配置成功 123[main] DbCommandExecutor INFO Successfully connected to database.All done, your SCM database is configured correctly! 格式 1234scm_prepare_database.sh mysql cm -h &lt;hostName&gt; -u&lt;username&gt; -p&lt;password&gt; --scm-host &lt;hostName&gt; scm scm scm对应于：数据库类型 数据库 服务器 用户名 密码 –scm-host Cloudera_Manager_Server 所在节点…… Manager节点上创建Parcel目录 (1). 创建目录/data/dmp/cloudera/parcel-repo，并且将该目录授权给cloudera-scm用户，执行命令： 123mkdir -p &#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcel-repochown cloudera-scm:cloudera-scm &#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcel-repo (2) 将爱在好的parcel\b复制到/data/dmp/cloudera/parcel-repo\b目录下，parcel列表 123456CDH-5.12.2-1.cdh5.12.2.p0.4-el7.parcelCDH-5.12.2-1.cdh5.12.2.p0.4-el7.parcel.shamanifest.jsonKUDU-1.4.0-1.cdh5.12.2.p0.8-el7.parcelKUDU-1.4.0-1.cdh5.12.2.p0.8-el7.parcel.sha 注意，需要将CDH-5.12.2-1.cdh5.12.2.p0.4-el7.parcel.sha1重命名为CDH-5.12.2-1.cdh5.12.2.p0.4-el7.parcel.sha，否则系统会重新\b下载。 安装cloudera-manager的agent对于\b通过cloudera-manager管理的主机，都需要安装agent，cloudera-manager的安装软件和监控都是通过agent完成的。所有加入的节点，都需要cloudera-manager的server节点保持通信，实现ssh免密码登陆。 头3步和cloudera manager server节点相同 在cloudera manager server所在节点创建cloudera-manager安装\b目录 1mkdir -p &#x2F;data&#x2F;dmp&#x2F;cloudera-manager 将下载好的cloudera-manager安装包cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz解压放在改目录： 1tar -zxvf cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz -C &#x2F;data&#x2F;dmp&#x2F;cloudera-manager 创建cloudera-scm用户 Cloudera管理器服务器和托管服务被配置为在默认情况下使用用户帐户Cloudera-scm，创建具有这个名称的用户是最简单的方法。创建用户，在安装完成后自动使用。 执行如下命令： 1useradd --system --home&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;run&#x2F;cloudera-scm-server&#x2F; --no-create-home --shell&#x3D;&#x2F;bin&#x2F;false --comment &quot;Cloudera SCM User&quot; cloudera-scm 配置agent 在/data/dmp/cloudera-manager/cm-5.12.2/etc/cloudera-scm-agent/config.ini中可以pacel、各个服务的安装目录、cloudera-manager\b server的地址。 创建parcels目录 在agent节点上，创建/data/dmp/cloudera/parcels，并且\b将改目录用户组授权给cloudera-scm 123mkdir -p &#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcelschown cloudera-scm:cloudera-scm &#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels 启动服务 在manager节点启动cloudera-scm-server服务。执行命令: 1&#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-server start 在agent节点启动agent 1&#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-agent start 登陆cloudera manager安装服务访问cloudera manager server节点的7180端口，这里的\b地址http://cloudera-manager.etouch.cn/cmf/login 通过用户名和密码登陆，默认的用户名和密码都是admin。 安装提示一步一步进行配置，在安装会有机器监测，对于监测的问题，cloudera manager会有提示，提示如何修改。 向集群添加安装新的节点agent123456789101112mkdir -p /data/dmp/cloudera-managertar -zxvf cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz -C /data/dmp/cloudera-manageruseradd --system --home=/data/dmp/cloudera-manager/cm-5.12.2/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scmcd /data/dmp/cloudera-manager/cm-5.12.2/etc/cloudera-scm-agentrm -f config.iniscp /data/dmp/cloudera-manager/cm-5.12.2/etc/cloudera-scm-agent/config.ini root@node104.bigdata.dmp.local.com:/data/dmp/cloudera-manager/cm-5.12.2/etc/cloudera-scm-agent/config.inimkdir -p /data/dmp/cloudera/parcelschown cloudera-scm:cloudera-scm /data/dmp/cloudera/parcels Cloudera Manager安装常见错误和脚本1. 防火墙和selinux关闭 出现连接拒绝，用telnet测试相关端口，server：7180、agent：7182 2. CM agent的日志在/data/dmp/log下面 新装agent起不来，可能需要手动创建相关文件夹。见：http://community.cloudera.com/t5/Cloudera-Manager-Installation/Failed-to-receive-heartbeat-from-agent-CM-server-guid/m-p/64228","categories":[{"name":"cloudera-manager","slug":"cloudera-manager","permalink":"http://guoyanlei.top/categories/cloudera-manager/"}],"tags":[{"name":"cloudera-manager","slug":"cloudera-manager","permalink":"http://guoyanlei.top/tags/cloudera-manager/"}]},{"title":"CDH-HDFS误删HA的namenode后无法启动","slug":"2018081001-CDH-HDFS误删HA的namenode后无法启动","date":"2018-08-10T07:30:00.000Z","updated":"2018-08-10T07:25:18.271Z","comments":true,"path":"2018/08/10/2018081001-CDH-HDFS误删HA的namenode后无法启动/","link":"","permalink":"http://guoyanlei.top/2018/08/10/2018081001-CDH-HDFS%E8%AF%AF%E5%88%A0HA%E7%9A%84namenode%E5%90%8E%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/","excerpt":"背景今天干了一件现在想还有点害怕的是，记录下。 新加了几天集群，想迁移一下Journal node，然后就顺手点了cloudera-manager上的角色迁移，在迁的过程中报错了，一系列操作后，不小心禁用了HA，禁用过程失败了，hdfs就无法启动了，报的错误如下图所示。随后又删除了3个journal node，添加了一个secondary namenode，hdfs的实例页面还是报同样的错误，重启hdfs也报这个错。突然不知道咋办有点慌了，唉。","text":"背景今天干了一件现在想还有点害怕的是，记录下。 新加了几天集群，想迁移一下Journal node，然后就顺手点了cloudera-manager上的角色迁移，在迁的过程中报错了，一系列操作后，不小心禁用了HA，禁用过程失败了，hdfs就无法启动了，报的错误如下图所示。随后又删除了3个journal node，添加了一个secondary namenode，hdfs的实例页面还是报同样的错误，重启hdfs也报这个错。突然不知道咋办有点慌了，唉。 冷静了一段时间后，从网上找到了解决方案，还是暗自庆幸的，但是下次不该这么鲁莽了。 解决方法1）删除了如下图所示的hdfs的配置文件。因为把namenode 的ha删除之后，hdfs已经不再具有ha功能了，原来的ha设置就没有用了，但是配置上如果还存在，就会报错。 2）删除journal node，添加secondary namenode3）先重启name node，再重启data node，到此，问题已经得到解决。4）最后启动secondaryNamenode","categories":[{"name":"cloudera-manager","slug":"cloudera-manager","permalink":"http://guoyanlei.top/categories/cloudera-manager/"}],"tags":[{"name":"cloudera-manager","slug":"cloudera-manager","permalink":"http://guoyanlei.top/tags/cloudera-manager/"},{"name":"hdfs","slug":"hdfs","permalink":"http://guoyanlei.top/tags/hdfs/"}]},{"title":"PyCharm本地开发pyspark并提交远程执行","slug":"2018080201-PyCharm本地开发pyspark并提交远程执行","date":"2018-08-01T07:30:00.000Z","updated":"2018-08-02T04:56:01.835Z","comments":true,"path":"2018/08/01/2018080201-PyCharm本地开发pyspark并提交远程执行/","link":"","permalink":"http://guoyanlei.top/2018/08/01/2018080201-PyCharm%E6%9C%AC%E5%9C%B0%E5%BC%80%E5%8F%91pyspark%E5%B9%B6%E6%8F%90%E4%BA%A4%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C/","excerpt":"最近在学习pyspark的开发，遇到些问题记录下。 我们在开发pyspark时经常需要进行测试，自己电脑上安装搭建一个spark环境代价有点高，目前有的同事在开发时，通常是开发完把代码贴出到本地测试集群进行测试，因此，能不能借助pycharm里的一个功能，连接本地测试集群的pyspark进行执行呢，经过一番搜索终于实现了这一个功能。","text":"最近在学习pyspark的开发，遇到些问题记录下。 我们在开发pyspark时经常需要进行测试，自己电脑上安装搭建一个spark环境代价有点高，目前有的同事在开发时，通常是开发完把代码贴出到本地测试集群进行测试，因此，能不能借助pycharm里的一个功能，连接本地测试集群的pyspark进行执行呢，经过一番搜索终于实现了这一个功能。 新建带有Virtualenv的工程Virtualenv是什么？ Python 的第三方包成千上万，在一个 Python 环境下开发时间越久、安装依赖越多，就越容易出现依赖包冲突的问题。为了解决这个问题，开发者们开发出了 virtualenv，可以搭建虚拟且独立的 Python 环境。这样就可以使每个项目环境与其他项目独立开来，保持环境的干净，解决包冲突问题。 下面我们先见一个project，在pycharm中默认的是Virtualenv管理环境，当然还有conda，功能和Virtualenv类似。 开发&amp;导入pyspark新建一个py文件 12345678910from pyspark import SparkContextfrom pyspark.sql import HiveContextAPP_NAME &#x3D; &#39;APP_TEST&#39;sc &#x3D; SparkContext(appName&#x3D;APP_NAME)sqlContext &#x3D; HiveContext(sc)sqlContext.sql(&quot;show databases&quot;).show() 引入未安装的包时，pycharm会提示安装，安装需借助pip安装，若为安装pip需先安装，安装完成后会在项目目录：venv/lib/python2.7/site-packages 中出现安装好的第三方库。 至此，我们就可以开发和使用pyspark中的一些文件了。 开发是方便，但是我们还想变开发变测试，由于本地并没有装大数据相关环境，因此，我们还需要做些配置，来远程提交我们的代码并执行测试。 提交远程执行配置远程project interpreter（程序解释器）打开pycharm的prefrences配置，从中找到我们项目的project interpreter。 图中展示的是我们本机中导入的程序解释器，我们还可以点击右上方配置按钮，添加远程的程序解释器。 在配置中添加远程服务器的主机ip和用户、密码 在最后一页，可以看到，远程服务器中python的位置。我们在执行程序时，pycharm会自动的将我们的最新代码提交的远程的目录下（/tmp/pycharm_project_237） 当我们切换到远程的project interpreter时，就会看到远程的一些库。我们在开发时，远程服务器缺少哪些库，就需要先到服务器上安装好那些库，之后才可以提交到远程执行，否则会报no module find。 远程执行上面的配置完成后，就可以在当前文件的Edit Configuration中进行执行配置，在project interpreter选项中可以选择是使用本地的程序解释器还是远程程序解释器执行了。 执行上面定义好的的程序。 可以看出，实际程序执行的是同步到远程服务器的代码。并借助ssh登录到远程服务器执行，并返回执行的结果。 1ssh:&#x2F;&#x2F;root@node102.bigdata.dmp.local.com:22&#x2F;usr&#x2F;bin&#x2F;python -u &#x2F;tmp&#x2F;pycharm_project_568&#x2F;suishen&#x2F;uid_to_mongo.py 这样就很方便了，不需要每次边写代码，边复制粘贴进行测试了。 CDH-spark在执行时遇到的问题由于服务器安装的是CDH版本的spark，因此，在执行pyspark程序时需要指定SPARK_HOME。 解决方案有很多种，可参考（https://blog.csdn.net/syani/article/details/72851425） 我们使用的是第二种，在代码中灵活的配置SPARK_HOME和其他的环境地址。 1234567import osimport sysos.environ[&#39;SPARK_HOME&#39;] &#x3D; &quot;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.12.2-1.cdh5.12.2.p0.4&#x2F;lib&#x2F;spark&quot;sys.path.append(&quot;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.12.2-1.cdh5.12.2.p0.4&#x2F;lib&#x2F;spark&#x2F;python&quot;)sys.path.append(&quot;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.12.2-1.cdh5.12.2.p0.4&#x2F;lib&#x2F;spark&#x2F;python&#x2F;lib&#x2F;py4j-0.9-src.zip&quot;)","categories":[{"name":"pyspark","slug":"pyspark","permalink":"http://guoyanlei.top/categories/pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"http://guoyanlei.top/tags/pyspark/"}]},{"title":"分布式数据仓库在公司的应用与演变","slug":"2018071801-分布式数据仓库在公司的应用与演变","date":"2018-07-18T07:30:00.000Z","updated":"2018-07-26T04:09:33.501Z","comments":true,"path":"2018/07/18/2018071801-分布式数据仓库在公司的应用与演变/","link":"","permalink":"http://guoyanlei.top/2018/07/18/2018071801-%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%9C%A8%E5%85%AC%E5%8F%B8%E7%9A%84%E5%BA%94%E7%94%A8%E4%B8%8E%E6%BC%94%E5%8F%98/","excerpt":"本文只关注与分布式数据仓库在公司的使用和演变，不涉及任何技术细节。希望通过本文的总结，能让你对数据仓库在公司的应用与演变，以及相关技术栈有初步的认识。","text":"本文只关注与分布式数据仓库在公司的使用和演变，不涉及任何技术细节。希望通过本文的总结，能让你对数据仓库在公司的应用与演变，以及相关技术栈有初步的认识。 数据仓库生态 如图为公司内部数据仓库生态，主要包括了三部分：数据采集，数据聚合和数据应用 数据采集 客户端用户行为日志采集，通过采集和分析用户行为日志，可以帮助运营团队合理化运营，商务团队智能化广告投放，数据挖掘团队精准化文章推荐，产品团队动态化把握版本迭代。 业务系统数据同步，主要同步有统计需求的业务数据和相关维度数据。业务数据，如微鲤看看金币流水，可定时将金币流水数据同步到数据仓库中，减轻每日频繁的统计对业务系统的影响，减少业务开发团队对数据统计需求的工作量。维度数据，如peacock投放的文章，广告等详情数据，同步这些数据是为了在基于条目统计时可更好的分析各维度下的文章或广告投放。 广告平台日志采集，为了更好的分析广告平台中广告投放ctr，广告请求响应效率，以及广告素材质量等。数据仓库主要采集了广告曝光、点击数据，请求第三方的广告素材数据，以及广告请求响应状态码、响应时间等数据。 数据聚合数据聚合中，通过维度建模、数据分层、统计任务的定时调度，将客户端、业务系统、广告平台采集的数据进行规范化的聚合、统计。最终，生成面向各主题需求的统计结果，共数据应用层使用。 这里不在阐述维度建模、数据分层相关技术，可参考之前的文章。 数据应用在数据应用层中，主要列举了公司内部正在依托于数据仓库而运行的各系统，下面简单介绍各系统的职能。 DMP：数据报表平台。统计展示最频繁的统计需求结果，同时为其他平台提供数据接口 Peacock &amp; 好学：内容、广告投放系统。需借助DMP展示各投放内容的实时曝光、点击等统计 Leopard：正在研发的即席查询平台。将逐渐替代DMP，可在该平台即席查询任意组合条件的统计数据 ADX &amp; DSP：广告交易平台，借助数据仓库统计广告曝光、点击，请求响应等数据 Wolves：市场渠道质量评估平台，基于真实的用户行为数据，动态评估第三方推广渠道所带来的用户质量 Rooster：智能推送系统。借助数据仓库完成自定义的统计需求（即面向各维度的用户群进行推送），统计推送下发、到达和曝光结果。 用户画像 &amp; 蜂巢推荐：借助数据仓库可为各app用户建立自己的用户画像，并基于用户画像为用户提供个性化的内容推荐。 临时需求：当在各数据平台中无法查询的统计需求，可临时提交给数据仓库工程师，由其构建特殊的查询SQL，来获取统计结果。 邮件订阅：对于一些常规的，只是阶段性的需求，可以邮件订阅的方式来获取统计数据。 数据仓库技术架构-v1.0 下面列举V1.0使用的数据仓库技术栈： Flume：分布式、可靠、高可用的海量日志收集的系统。支持各种各样的数据源（HTTP、log等Source），能将这些数据源的数据高效的收集，聚合、移动，最后存储到指定存储系统（Kafka or HDFS）中。图中是使用Flume去收集各日志采集服务器中用户行为日志数据。 Kafka：分布式的、基于发布/订阅的消息系统。可以将Flume收集到日志，按照不同的主题按序存储到不同的Topic队列中，然后多个Consumer可以同时去消费Topic中的数据。 Storm：分布式实时大数据处理系统。用以实时的消费Kafka中的数据，并做相关计算处理。可直接将数据存储到HDFS中，也可基于某些统计逻辑，将统计后到数据存储到HBase中。 HDFS：分布式文件系统，能提供高吞吐量的数据访问。 Hive：基于HDFS的一个数据仓库工具，可以将结构化的数据文件映射为数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。适合用来对一段时间内的数据进行分析查询。 HBase：分布式的、面向列的开源数据库，是一种Key/Value系统，其数据文件都存储在HDFS中，非常适合用来进行大数据的实时查询。 Phoenix：本质是用Java写的基于JDBC API操作HBase的开源SQL引擎。 Yarn：一个通用资源管理系统，可为上层应用提供统一的资源管理和调度。 Sqoop：用在Hive与传统关系型数据库间的数据传递，可以将一个关系型数据库中的数据导进到Hive中，也可以将Hive表中的数据导进到关系型数据库中。 Redis &amp; Mysql：与业务紧密相联的存储。可将Hive or HBase中统计的数据同步到这些业务库中。 数据仓库技术架构-v2.0 下面列举V2.0新增的数据仓库技术栈： Logstash：一款强大的数据处理工具，可以实现数据传输，格式处理，格式化输出，还有强大的插件功能，常用于日志处理。它结合Elasticsearch和kibana可实现一套完整的日志分析系统，其中，ES进行存储、建立搜索索引，kibana调用ES接口进行数据可视化。 Spark：拥有DAG执行引擎，支持在内存中对数据进行迭代计算，适合大数据分析统计，实时数据处理，图计算及机器学习。 Spark-Streaming：将持续不断输入的数据流转换成多个batch分片，使用一批spark应用实例进行处理。 Hive on Spark：目的是把Spark作为Hive的一个计算引擎，将Hive的查询作为Spark的任务提交到Spark集群上进行计算。与SparkSQL结构类似，只是SQL引擎不同，但是计算引擎都是spark Kudu：新一代面向实时分析的存储引擎，底层使用类似Parquet的存储结构，支持实时写入，实时更新，实时查询。扫描性能比Parquet略差。 Impala：全新的执行引擎（真正的MPP查询引擎），在执行SQL语句的时候，Impala不会把中间数据写入到磁盘，而是在内存中完成了所有的处理。使用Impala的时候，查询任务会马上执行而不是生产Mapreduce任务，这会节约大量的初始化时间。 DataX：阿里开源的，一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。 ES &amp; MongoDB：借助Hive的StorageHandler插件，可将hive的数据通过外部表的形式关联到ES or MongoDB中，可以实现双向读写。 即席查询系统即席查询是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。 即席查询技术架构基于数据仓库中事实表的设计，我们的Ad-Hoc简单的架构如下图： 架构比较简单，主要借助Impala + parquet去查询HDFS中的数据，并借助kudu接收实时写入的数据， 在T+1日后，会将kudu中实时写入的数据转存到HDFS中，并转换成parquet格式。 Parquet存储格式 按列存储 可按时间分区 局部排序 适配多种计算框架 只支持批量写入，无法追加，无法实时写入 kudu 新一代面向实时分析的存储引擎 底层使用类似Parquet的存储结构 支持实时写入，实时更新，实时查询 扫描性能比Parquet略差 即席查询中动态视图的使用下面介绍动态视图的使用，结合上面说的，我们有时候即需要查询hive中T-1的历史数据，又要查询今天实时写入的数据，怎么实现？ Impala就可以帮助我们，即可以查询Hive又可以查询Kudu，但是每次我们都需要判断日期是不是今天，来判断要不要查kudu中的数据，因此就想到了视图，并结合Union all来实现。ok，到这这里就下面的视图创建语句。 1234567891011create view prod.ods_view_pv_event_1d asselect xxx ,xxx from prod.ods_hive_pv_event_1d where ds &lt;&#x3D; &#39;20180704&#39; union allselect xxx ,xxx from prod.ods_kudu_pv_event_1d where nginx_date &#x3D; &#39;20180705&#39;; 虽然视图创建成功，也满足了对于20180705的需求，但是如果过了这一天，20180706应该怎么查呢，还需要再创建一个视图？ 其实，我们可以如下这么做，下面的语句只是将上面写死的日期改成了可根据now()动态生成的日期。这样我们就不需要每次使用的时候创建一个视图了。 视图并不是数据库中以存储的数据值集形式存在，而仅仅是存储在数据库中具有关联名称的查询语句！！ 123456789101112create view prod.ods_view_pv_event_1d asselect xxx ,xxx from prod.ods_hive_pv_event_1d where ds &lt;&#x3D; from_timestamp(date_sub(now(), 1),&#39;yyyyMMdd&#39;) union allselect xxx ,xxx from prod.ods_kudu_pv_event_1d where nginx_date &#x3D; from_timestamp(now(),&#39;yyyyMMdd&#39;); 结束语： 以上总结了分布式数据仓库在公司的应用与演变，以及相关技术栈，最后简单介绍了我们正在努力研发的即席查询系统，以及即席查询系统中动态视图的妙用。","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://guoyanlei.top/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/tags/hive/"},{"name":"impala","slug":"impala","permalink":"http://guoyanlei.top/tags/impala/"},{"name":"kudu","slug":"kudu","permalink":"http://guoyanlei.top/tags/kudu/"}]},{"title":"Hive-Mongo外部表使用记录","slug":"2018070501-Hive-Mongo外部表使用记录","date":"2018-07-05T07:30:00.000Z","updated":"2018-07-06T14:40:50.630Z","comments":true,"path":"2018/07/05/2018070501-Hive-Mongo外部表使用记录/","link":"","permalink":"http://guoyanlei.top/2018/07/05/2018070501-Hive-Mongo%E5%A4%96%E9%83%A8%E8%A1%A8%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/","excerpt":"统计需求需要在数据仓库中统计业务系统的数据，而这些数据存储在mongodb中，如何获取mongo中的数据并统计呢？下面记录下官网所提供的mongo-hadoop-hive方式来获取mongo中的数据。 官方：https://github.com/mongodb/mongo-hadoop/wiki/Hive-Usage","text":"统计需求需要在数据仓库中统计业务系统的数据，而这些数据存储在mongodb中，如何获取mongo中的数据并统计呢？下面记录下官网所提供的mongo-hadoop-hive方式来获取mongo中的数据。 官方：https://github.com/mongodb/mongo-hadoop/wiki/Hive-Usage hive-mongo外部表使用MongoStorageHandler实现hive-mongo外部表的映射，建表语句如下： 123456789101112131415161718192021use business;add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;mongo-hadoop-hive-1.5.1.jar;add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;mongo-hadoop-core-1.5.1.jar;add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;mongo-java-driver-3.2.2.jar;create external table ods_dim_mongodb_volunteer_stat_info( id bigint, date string, volunteer_nums int, grap_totals int, grap_success_totals int, rescuit_success int, avg_match double, avg_new double)STORED BY &#39;com.mongodb.hadoop.hive.MongoStorageHandler&#39;with SERDEPROPERTIES (&#39;mongo.columns.mapping&#39;&#x3D;&#39;&#123;&quot;id&quot;:&quot;_id&quot;,&quot;date&quot;:&quot;date&quot;,&quot;volunteer_nums&quot;:&quot;volunteerNums&quot;,&quot;grap_totals&quot;:&quot;grapTotals&quot;,&quot;grap_success_totals&quot;:&quot;grapSuccessTotals&quot;,&quot;rescuit_success&quot;:&quot;rescuitSuccess&quot;,&quot;avg_match&quot;:&quot;avgMatch&quot;,&quot;avg_new&quot;:&quot;avgNew&quot;&#125;&#39;)TBLPROPERTIES ( &#39;mongo.uri&#39;&#x3D;&#39;mongodb:&#x2F;&#x2F;&#123;username&#125;:&#123;password&#125;@&#123;hostname&#125;:27017&#x2F;&#123;db&#125;.&#123;collection&#125;&#39;); 添加splitVector权限创建外部表完毕后，在hive上进行查询时，报错如下： 123456789not authorized on certificate to execute command &#123; splitVector: &quot;certificate.certificate.access_log_test&quot;, keyPattern: &#123; _id: 1 &#125;, min: &#123;&#125;, max: &#123;&#125;, force: false, maxChunkSize: 8 &#125; 分析原因 线上mongo使用分片 hive MongoStorageHandler 在拆分非分片集合时需要splitVector命令的，该命令仅限于管理员用户。mongo.input.split.create_input_splits的默认设置是true，也就是会对数据进行拆分，根据集群数，cpu核数然后将数据进行拆分成多个InputSplits,以允许Hadoop并行处理，也就是说，Hadoop为每个映射器分配一个InputSplits。 解决方案没有splitVector权限，则需要为该用户加上该权限 首先添加一个新role，并给予splitVector权限，之后把这个role分配至指定的user上。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 添加角色db.createRole( &#123; role: &quot;hadoopSplitVector&quot;, privileges: [ &#123; resource: &#123; db: &quot;&#123;db&#125;&quot;, collection: &quot;&#123;collection&#125;&quot; &#125;, actions: [ &quot;splitVector&quot; ] &#125; ], roles:[] &#125;)# 若需要对一个新的colletion添加splitVector权限，则只需更新角色即可db.updateRole( &quot;hadoopSplitVector&quot;, &#123; privileges: [ &#123; resource: &#123; db: &quot;suishen_lizhi&quot;, collection: &quot;wltt_invite_stats&quot; &#125;, actions: [ &quot;splitVector&quot; ] &#125;, &#123; resource: &#123; db: &quot;suishen_lizhi&quot;, collection: &quot;volunteer_stat_info&quot; &#125;, actions: [ &quot;splitVector&quot; ] &#125;, ], roles:[] &#125;) 12345678910111213141516171819202122232425262728293031# 更新用户db.updateUser( &quot;dmp&quot;, &#123; roles: [ &#123; role:&quot;read&quot;, db:&quot;&#123;db&#125;&quot; &#125;, &#123; role:&quot;hadoopSplitVector&quot;, db:&quot;&#123;db&#125;&quot; &#125; ] &#125;)# 若没有可用户用户可以创建一个只读用户db.createUser( &#123; user: &quot;xxx&quot;, pwd: &quot;xxx&quot;, roles: [ &#123; role: &quot;read&quot;, db: &quot;&#123;db&#125;&quot; &#125; ] &#125;) 注：上面添加更新用户、角色都需要较高权限的用户进行操作。 至此，已经可以使用外部表来查询mongo中的数据了。 查Mongo时’=’的坑在使用外部表查询mongo时，在条件中使用 where app = ‘zhwnl’，发现查询的结果包含了其他的app，并不是我们限制的查询条件，这是因为MongoStorageHandler并没有将我们的’=’转成mongo中的’$.eq’。 但是，我们可以’like’来解决，功能和’=’等同。","categories":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/categories/hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/tags/hive/"},{"name":"mongodb","slug":"mongodb","permalink":"http://guoyanlei.top/tags/mongodb/"}]},{"title":"Impala动态视图在即席查询中的妙用","slug":"2018070301-Impala动态视图在即席查询中的妙用","date":"2018-07-03T07:30:00.000Z","updated":"2018-07-06T14:37:40.980Z","comments":true,"path":"2018/07/03/2018070301-Impala动态视图在即席查询中的妙用/","link":"","permalink":"http://guoyanlei.top/2018/07/03/2018070301-Impala%E5%8A%A8%E6%80%81%E8%A7%86%E5%9B%BE%E5%9C%A8%E5%8D%B3%E5%B8%AD%E6%9F%A5%E8%AF%A2%E4%B8%AD%E7%9A%84%E5%A6%99%E7%94%A8/","excerpt":"查询视图视图仅仅是存储在数据库中具有关联名称的Impala查询语言的语句。 它是以预定义的SQL查询形式的表的组合。视图可以包含表的所有行或选定的行。 可以从一个或多个表创建视图。 创建视图没什么难度，这里总结下是视图在我们Ad-Hoc查询是怎么用的。","text":"查询视图视图仅仅是存储在数据库中具有关联名称的Impala查询语言的语句。 它是以预定义的SQL查询形式的表的组合。视图可以包含表的所有行或选定的行。 可以从一个或多个表创建视图。 创建视图没什么难度，这里总结下是视图在我们Ad-Hoc查询是怎么用的。 Ad-Hoc即席查询即席查询是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。 基于数据仓库中事实表的设计，我们的Ad-Hoc简单的架构如下图： 架构比较简单，主要借助Impala + parquet去查询HDFS中的数据，并借助kudu接收实时写入的数据， 在T+1日后，会将kudu中实时写入的数据转存到HDFS中，并转换成parquet格式。 Parquet存储格式 按列存储 可按时间分许 局部排序 只支持批量写入，无法追加，无法实时写入 Kudu：新一代面向实时分析的存储引擎 底层使用类似Parquet的存储结构 支持实时写入，实时更新，实时查询 扫描性能比Parquet略差 动态视图的使用下面介绍动态视图的使用，结合上面说的，我们有时候即需要查询hive中T-1的历史数据，又要查询今天实时写入的数据，怎么实现呢？ Impala就可以帮助我们，即可以查询Hive又可以查询Kudu，但是每次我们都需要判断日期是不是今天，来判断要不要查kudu中的数据，因此就想到了视图，并结合Union all来实现。ok，到这这里就下面的视图创建语句。 1234567891011create view prod.ods_view_pv_event_1d asselect xxx ,xxx from prod.ods_hive_pv_event_1d where ds &lt;&#x3D; &#39;20180704&#39; union allselect xxx ,xxx from prod.ods_kudu_pv_event_1d where nginx_date &#x3D; &#39;20180705&#39;; 虽然视图创建成功，也满足了对于20180705的需求，但是如果过了这一天，20180706应该怎么查呢，还需要再创建一个视图？ 其实，我们可以如下这么做，下面的语句只是将上面写死的日期改成了可根据now()动态的日期。这样我们就不需要每次使用的时候创建一个视图了。 主要是视图，并不是数据库中以存储的数据值集形式存在，而仅仅是存储在数据库中具有关联名称的查询语句！！（这一点需要仔细参悟，自己当时就想了很久） 123456789101112create view prod.ods_view_pv_event_1d asselect xxx ,xxx from prod.ods_hive_pv_event_1d where ds &lt;&#x3D; from_timestamp(date_sub(now(), 1),&#39;yyyyMMdd&#39;) union allselect xxx ,xxx from prod.ods_kudu_pv_event_1d where nginx_date &#x3D; from_timestamp(now(),&#39;yyyyMMdd&#39;);","categories":[{"name":"impala","slug":"impala","permalink":"http://guoyanlei.top/categories/impala/"}],"tags":[{"name":"impala","slug":"impala","permalink":"http://guoyanlei.top/tags/impala/"}]},{"title":"使用DataX实现离线同步分库分表数据","slug":"2018070201-使用DataX实现离线同步分库分表数据","date":"2018-07-02T06:30:00.000Z","updated":"2018-09-12T02:56:09.945Z","comments":true,"path":"2018/07/02/2018070201-使用DataX实现离线同步分库分表数据/","link":"","permalink":"http://guoyanlei.top/2018/07/02/2018070201-%E4%BD%BF%E7%94%A8DataX%E5%AE%9E%E7%8E%B0%E7%A6%BB%E7%BA%BF%E5%90%8C%E6%AD%A5%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E6%95%B0%E6%8D%AE/","excerpt":"DataX简介DataX 是阿里巴巴内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。 DataX更像是一个数据枢纽，它可以读取多种数据源中数据，经过内部的转换又可以输出到多种数据源中。 其架构设计主要包含三部分： Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。 Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。 更详细的介绍，请移步 https://yq.aliyun.com/articles/59373 这里只总结DataX在同步MySQL分库分表的使用。","text":"DataX简介DataX 是阿里巴巴内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。 DataX更像是一个数据枢纽，它可以读取多种数据源中数据，经过内部的转换又可以输出到多种数据源中。 其架构设计主要包含三部分： Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。 Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。 更详细的介绍，请移步 https://yq.aliyun.com/articles/59373 这里只总结DataX在同步MySQL分库分表的使用。 同步分库中表的数据我们业务系统的用户基本信息表，被分库存储在不同的库中，下面介绍使用DataX如何将这些分库的数据同步到HDFA中。 下载 &amp; 使用从Datax Github 中下载源码编译，或直接下载已编译好的工具包。 下载后解压至本地某个目录，进入bin目录，即可运行同步作业： 123$ cd &#123;DATAX_HOME&#125;&#x2F;bin$ python datax.py &#123;YOUR_JOB.json&#125; 配置和使用很简单，只需配置YOUR_JOB.json，在这个文件中配置输入源. 其中 datax.py可以配置多个参数，如下： 123456-j &lt;jvm parameters&gt; # 设置jvm参数，如-Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError--jobid&#x3D;&lt;job unique id&gt; # 设置job唯一id-m &lt;job runtime mode&gt; # 设置运行模式: standalone(默认), local, distribute.-p &lt;parameter used in job config&gt; # 动态设置一些参数，当在配置文件设置了一些变量时，如$&#123;tableName&#125;，那么可以通过-p&quot;-DtableName&#x3D;your-table-name&quot;来动态设置。-r &lt;parameter used in view job config[reader] template&gt; # 使用一些reader模版，eg: mysqlreader,streamreader-w &lt;parameter used in view job config[writer] template&gt;, # 使用一些reader模版，eg: mysqlwriter,streamwriter 同步分库的配置下面给出同步分库中表数据的配置，content中包含两部分内容，reader和writer，其中reader中配置了mysql的分库地址。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# user_info.json&#123; &quot;job&quot;: &#123; &quot;setting&quot;: &#123; &quot;speed&quot;: &#123; &quot;channel&quot;:60 &#125; &#125;, &quot;content&quot;: [ &#123; &quot;reader&quot;: &#123; &quot;name&quot;: &quot;mysqlreader&quot;, &quot;parameter&quot;: &#123; &quot;username&quot;: &quot;xxx&quot;, &quot;password&quot;: &quot;xxx&quot;, &quot;column&quot;: [ &quot;uid&quot; ,&quot;nick_name&quot; ,&quot;accounts&quot; ,&quot;email&quot; , ... ... ], &quot;connection&quot;: [ &#123;&quot;table&quot;: [&quot;user_info&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;metastore_1000&quot;]&#125;, &#123;&quot;table&quot;: [&quot;user_info&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_2:3306&#x2F;metastore_1001&quot;]&#125;, &#123;&quot;table&quot;: [&quot;user_info&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_3:3306&#x2F;metastore_1002&quot;]&#125;, ... ... ] &#125; &#125;, &quot;writer&quot;: &#123; &quot;name&quot;: &quot;hdfswriter&quot;, &quot;parameter&quot;: &#123; &quot;defaultFS&quot;: &quot;hdfs:&#x2F;&#x2F;nameservice:8020&quot;, &quot;hadoopConfig&quot;:&#123; &quot;dfs.nameservices&quot;: &quot;nameservice&quot;, &quot;dfs.ha.namenodes.nameservice&quot;: &quot;namenode44,namenode46&quot;, &quot;dfs.namenode.rpc-address.nameservice.namenode44&quot;: &quot;nn1.hadoop.bigdata.dmp.com:8020&quot;, &quot;dfs.namenode.rpc-address.nameservice.namenode46&quot;: &quot;nn2.hadoop.bigdata.dmp.com:8020&quot;, &quot;dfs.client.failover.proxy.provider.nameservice&quot;: &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot; &#125;, &quot;fileType&quot;: &quot;orc&quot;, &quot;path&quot;: &quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_info&quot;, &quot;fileName&quot;: &quot;user_info&quot;, &quot;column&quot;: [ &#123; &quot;name&quot;: &quot;uid&quot;, &quot;type&quot;: &quot;BIGINT&quot; &#125;, &#123; &quot;name&quot;: &quot;nick_name&quot;, &quot;type&quot;: &quot;STRING&quot; &#125;, &#123; &quot;name&quot;: &quot;accounts&quot;, &quot;type&quot;: &quot;STRING&quot; &#125;, &#123; &quot;name&quot;: &quot;email&quot;, &quot;type&quot;: &quot;STRING&quot; &#125;, ... ... ], &quot;writeMode&quot;: &quot;append&quot;, &quot;fieldDelimiter&quot;: &quot;\\t&quot;, &quot;compress&quot;:&quot;NONE&quot; &#125; &#125; &#125; ] &#125;&#125; 同步分表中的数据和同步分库的结构类似，同步分表的conf如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# user_relation.json&#123; &quot;job&quot;: &#123; &quot;setting&quot;: &#123; &quot;speed&quot;: &#123; &quot;channel&quot;:60 &#125; &#125;, &quot;content&quot;: [ &#123; &quot;reader&quot;: &#123; &quot;name&quot;: &quot;mysqlreader&quot;, &quot;parameter&quot;: &#123; &quot;username&quot;: &quot;xxx&quot;, &quot;password&quot;: &quot;xxx&quot;, &quot;column&quot;: [ &quot;uid&quot; ,&quot;attention_uid&quot; ,&quot;status&quot; ,&quot;create_time&quot; ,&quot;update_time&quot; ], &quot;connection&quot;: [ &#123;&quot;table&quot;: [&quot;user_relation_0&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;, &#123;&quot;table&quot;: [&quot;user_relation_1&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;, &#123;&quot;table&quot;: [&quot;user_relation_10&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;, &#123;&quot;table&quot;: [&quot;user_relation_100&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;, &#123;&quot;table&quot;: [&quot;user_relation_101&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;, ... ... ] &#125; &#125;, &quot;writer&quot;: &#123; &quot;name&quot;: &quot;hdfswriter&quot;, &quot;parameter&quot;: &#123; &quot;defaultFS&quot;: &quot;hdfs:&#x2F;&#x2F;nameservice:8020&quot;, &quot;hadoopConfig&quot;:&#123; &quot;dfs.nameservices&quot;: &quot;nameservice&quot;, &quot;dfs.ha.namenodes.nameservice&quot;: &quot;namenode44,namenode46&quot;, &quot;dfs.namenode.rpc-address.nameservice.namenode44&quot;: &quot;nn1.hadoop.bigdata.dmp.com:8020&quot;, &quot;dfs.namenode.rpc-address.nameservice.namenode46&quot;: &quot;nn2.hadoop.bigdata.dmp.com:8020&quot;, &quot;dfs.client.failover.proxy.provider.nameservice&quot;: &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot; &#125;, &quot;fileType&quot;: &quot;orc&quot;, &quot;path&quot;: &quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_relation&quot;, &quot;fileName&quot;: &quot;user_info&quot;, &quot;column&quot;: [ &#123; &quot;name&quot;: &quot;uid&quot;, &quot;type&quot;: &quot;BIGINT&quot; &#125;, &#123; &quot;name&quot;: &quot;attention_uid&quot;, &quot;type&quot;: &quot;BIGINT&quot; &#125;, &#123; &quot;name&quot;: &quot;status&quot;, &quot;type&quot;: &quot;int&quot; &#125;, &#123; &quot;name&quot;: &quot;create_time&quot;, &quot;type&quot;: &quot;BIGINT&quot; &#125;, &#123; &quot;name&quot;: &quot;update_time&quot;, &quot;type&quot;: &quot;BIGINT&quot; &#125; ], &quot;writeMode&quot;: &quot;append&quot;, &quot;fieldDelimiter&quot;: &quot;\\t&quot;, &quot;compress&quot;:&quot;NONE&quot; &#125; &#125; &#125; ] &#125;&#125; 具体的hdfs写入，参考文档说明 在执行时，若hdfs中的目录为空，需要先创建，若不为空，需根据writeMode来判断，若模式是：append，写入前不做任何处理，DataX hdfswriter直接使用filename写入，并保证文件名不冲突；若模式是：nonConflict，如果目录下有fileName前缀的文件，直接报错。 可以建立一个执行脚本，如下： 1234567#!&#x2F;bin&#x2F;bashhdfs dfs -rm -r &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_relationhdfs dfs -rm -r &#x2F;user&#x2F;impala&#x2F;.Trash&#x2F;Current&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_relationhdfs dfs -mkdir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_relationpython datax.py user_relation.json -j &#39;-Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError&#39; 创建Hive表按指定格式导入到hdfs后，就可以建立hive表，并指定hdfs目录，就可以读取其中的数据了。 12345678910create table business.ods_dim_weili_user_relation( uid bigint ,attention_uid bigint ,status int ,create_time bigint ,update_time bigint)row format delimited fields terminated by &#39;\\t&#39;stored as orc;","categories":[{"name":"数据同步","slug":"数据同步","permalink":"http://guoyanlei.top/categories/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"}],"tags":[{"name":"DataX","slug":"DataX","permalink":"http://guoyanlei.top/tags/DataX/"}]},{"title":"浅析数据仓库维度建模","slug":"2018062001-浅析数据仓库维度建模","date":"2018-06-20T07:30:00.000Z","updated":"2018-06-20T10:09:38.682Z","comments":true,"path":"2018/06/20/2018062001-浅析数据仓库维度建模/","link":"","permalink":"http://guoyanlei.top/2018/06/20/2018062001-%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/","excerpt":"维度建模基础维度 &amp; 维度属性维度是维度建模的基础和灵魂，在维度建模中，将度量称为“事实”，将环境描述为“维度”，维度是用来分析事实所需要的多样环境。 如：将用户交易数据看做一个“事实”，可通过买家、卖家、商品和时间等多个维度来描述整个交易过程所发生的环境。 维度所包含的表示维度的列，称为维度属性。维度属性是查询约束条件（where）、分组（group by）和报表标签生成的基本来源。","text":"维度建模基础维度 &amp; 维度属性维度是维度建模的基础和灵魂，在维度建模中，将度量称为“事实”，将环境描述为“维度”，维度是用来分析事实所需要的多样环境。 如：将用户交易数据看做一个“事实”，可通过买家、卖家、商品和时间等多个维度来描述整个交易过程所发生的环境。 维度所包含的表示维度的列，称为维度属性。维度属性是查询约束条件（where）、分组（group by）和报表标签生成的基本来源。 下钻 &amp; 上钻（上卷）维度中的属性有时会以层次方式或一对多的方式相互关联，层次的最底层代表维度中描述最低级别的详细信息，最高层代表最高级别的概要信息。在属性的层次结构中进行钻取是数据钻取的方法之一。 沿着属性层次，往下即为下钻（drill down），往上可称上钻或上卷（roll up）。 如：地区维度，从全球到地区到国家到省份到市到县，层层下钻。第一级别可以根据需求而定。我看到上海市的总数，想要看各个区的数据，就用下钻。 对多维数据进行分析时还会用到： 旋转(pivot)，是维度的切换，换个角度看问题 切片(slice)，查看数据的时候，通常是不会所有维度都用到的，选择某两个维度进行分析，可以想象为一个二维平面，即切片。 切块，即选出3个维度时，切出来的自然就时块了。 至此，就可以理解OLAP中的CUBE等相关概念了。 规范化（OLTP）&amp; 反规范化（OLAP）将属性层次设计成数据库表时，规范化可以将重复属性移至其自身所属的表中，删除冗余数据，还可以有效避免数据冗余导致的不一致性。规范化常用在大多数联机事务处理系统(OLTP)的底层数据结构设计中。 当属性层次被实例化为一系列维度，而不是单一的维度时，被称为雪花模式，常出现在OLTP中。 而对于联机分析处理系统(OLAP)来说，数据是稳定的，不存在OLTP中所存在的问题。因此，在设计时常采用反规范化处理，从用户角度来看简化了模型，并且使数据库查询优化器的连接路径比完全规范化的模型简化许多。 一致性维度 &amp; 交叉探查企业级数据仓库等构建不可能一蹴而就，一般采用迭代式的构建过程。 而单独构建存在的问题是形成独立型数据集市，导致严重的不一致性。 Kimball的数据仓库总线架构：提供了一种分解企业级数据仓库规划任务的合理方法，通过构建企业范围内一致性维度和事实来构建总线架构。 如：对于日志数据域，统计了商品维度的最近一天的PV和UV; 对于交易数据域，统计了商品维度的最近一天的下单量。 现在将不同数据域的商品的事实合并在一起进行数据探查，如计算转化率等，称为交叉探查。 不用数据域的维度可能会存在不一致的问题，为避免这一问题，可通过以下方式保证维度一直性： 共享维表：基于共享维表进行交叉探查 一致性上卷：可以在两个维度相同的上层属性上，交叉探查 交叉属性：可在两个维度相同的属性上，交叉探查 维度建模维度整合 &amp; 拆分 垂直整合：不同的来源表包含相同的数据集 水平整个：不同的来源表包含不同的数据集 如：会员在源系统中可能有多个表，会员基础信息表、会员扩展信息表、会员等级信息表等，这些表都属于会员相关信息表，应尽量整合至会员维度模型中，即垂直整合。 如：对与淘宝的会员体系，分为支付宝会员、淘宝会员等，这这些会员进行整合时即水平整合，整合时需要考虑各个会员体系是否有交叉，如果存在交叉，则需要去重；如果不存在交叉，则需要考虑不同子集的自然键是否存在冲突，冲突时可添加分区字段来处理。 有整合就会有拆分，维度是维度建模的基础和灵魂，在进行维度设计时，依据维度设计的原则，尽可能丰富维度属性，同时进行反规范化处理。 维度变化如何处理维度的变化是维度设计的重要工作之一。 缓慢变化维在现实世界中，维度的属性并不是静态的，它会随着时间的流逝发生缓慢的变化。与数据增长较为快速的事实表相比，维度变化相对缓慢。 在一些情况下，保留历史数据没有什么分析价值；而在另一些情况下，保留历史数据将会起到至关重要的作用。有三种方式处理缓慢变化维： 重写维度值：不会保留历史，始终取最新的数据 插入新的维度行：保留历史数据，老的事实与老的维度值关联，新的与新的关联，不能将变化前后的事实归一化为变化前的维度或变化后的维度 添加维度列(新属性)：保留历史数据，可解决变化前后的事实按变化前后的维度归一化问题 快照维表处理缓慢变化维的方法可以采用快照方式，即基于某一周期（天），处理维度变化的方式就是每天保留一份全量快照数据。 优：简单有效，开发维护成本低；使用方便，理解性好。缺：存储极大浪费，必须要有对应的数据生命周期制度，清除无用的历史数据。 拉链表采用插入新的维度行的思想，处理方式：通过新增两个时间戳字段(start_dt和end_dt)，将所有以天为粒度的变更数据都记录下来。 拉链记历史，不需要按周期把所有数据都快照存储，只需要将发生变化的数据添加新的存储。 在查询数据时，可通过限制时间戳字段来获取历史数据。 缺：理解较困难，另外，以start_dt和end_dt做分区，一年的可能产生的分区数：365 x 364/2 = 66430个，随着时间推移，会使得分区数极度膨胀，超过分区数限制。 拉链表升级之极限存储底层基于拉链表，上层做一个视图，对下游使用者做到透明。同时，缓解分区数极度膨胀导致存储查询代价较大的问题。 优化的根源在于分区数，只需每月多一次拉链计算，就能优化十倍的性能。 微型维度通过将一部分不稳定的属性从主维度中移出，并将它们放置到拥有自己代理键的新表中。 如：用户的注册日期、年龄、性别的信息基本不会变化，但是用户等级却在不断变化，可将其独立开来。 缺：实现代价较高，ETL逻辑复杂，破坏了维度的可浏览性。 【参考】 之前的维度建模分享 拉链表与极限存储","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://guoyanlei.top/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://guoyanlei.top/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"维度建模","slug":"维度建模","permalink":"http://guoyanlei.top/tags/%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/"}]},{"title":"hive调优之控制mapper和reducer数","slug":"2018061901-hive调优之控制mapper和reducer数","date":"2018-06-19T09:30:04.000Z","updated":"2018-11-22T08:57:01.494Z","comments":true,"path":"2018/06/19/2018061901-hive调优之控制mapper和reducer数/","link":"","permalink":"http://guoyanlei.top/2018/06/19/2018061901-hive%E8%B0%83%E4%BC%98%E4%B9%8B%E6%8E%A7%E5%88%B6mapper%E5%92%8Creducer%E6%95%B0/","excerpt":"hive调优之控制mapper和reducer数由于mapreduce中没有办法直接控制map数量，所以只能曲线救国，通过设置每个map中处理的数据量进行设置；reduce是可以直接设置的。","text":"hive调优之控制mapper和reducer数由于mapreduce中没有办法直接控制map数量，所以只能曲线救国，通过设置每个map中处理的数据量进行设置；reduce是可以直接设置的。 控制mapper数1234set mapred.max.split.size&#x3D;256000000; -- 决定每个map处理的最大的文件大小，单位为Bset mapred.min.split.size.per.node&#x3D;1; -- 节点中可以处理的最小的文件大小set mapred.min.split.size.per.rack&#x3D;1; -- 机架中可以处理的最小的文件大小 mapper划分文件处理逻辑 执行HQL时，首先会读取表分区下到文件，然后分发到各个节点，此时是根据mapred.max.split.size确认要启动多少个map数。 假设有两个文件大小分别为(256M,280M)被分配到节点A，那么会启动两个map，剩余的文件大小为10MB和35MB因为每个大小都不足241MB会先做保留 根据参数set mapred.min.split.size.per.node看剩余的大小情况并进行合并。 如果值为1，表示a中每个剩余文件都会自己起一个map，这里会起两个，如果设置为大于45 * 1024 * 1024则会合并成一个块，并产生一个map。如果mapred.min.split.size.per.node为10 * 1024 * 1024，那么在这个节点上一共会有4个map，处理的大小为(245MB，245MB，10MB，10MB，10MB，10MB)，余下9MB。如果mapred.min.split.size.per.node为45 * 1024 * 1024，那么会有三个map，处理的大小为(245MB，245MB，45MB) 实际中mapred.min.split.size.per.node无法准确地设置成45 * 1024 * 1024，会有剩余并保留带下一步进行判断处理 对b中余出来的文件与其它节点余出来的文件根据mapred.min.split.size.per.rack大小进行判断是否合并，对再次余出来的文件独自产生一个map处理 场景1：控制文件生成个数通常在执行HQL时，分配的mapper和reducer个数都是根据文件的大小和任务的情况自动计算的，但有时，我们想去控制最终生成文件的个数。 前段时间在做hive表的跨集群迁移，需要将老集群上的ODS层表迁移到新集群，由于ODS表中的分区时按天和小时的，分区下小文件较多，在进行hive export和discp操作时，由于要频繁的创建和到导出小文件，因此非常慢。 能想到的解决方法：合并分区数据，减少小文件，然后再迁移 由于合并分区通常只有map阶段，没有reduce，因此有多少个mapper就会生成多少个文件，这里就可以通过设置mapper数间接控制文件个数，即加大map分配到文件大小，减少mapper个数。 12345678set mapred.max.split.size&#x3D;1024000000;set mapred.min.split.size.per.node&#x3D;1024000000;set mapred.min.split.size.per.rack&#x3D;1024000000;insert into table xxxselect * from xxx where xxx; 控制reducer数修改reduce的个数就简单很多，直接根据可能的情况作个简单的判断确认需要的reduce数量，如果无法判断，根据当前map数减小10倍，保持在1~100个reduce即可。 1234方法1set mapred.reduce.tasks&#x3D;10; -- 设置reduce的数量方法2set hive.exec.reducers.bytes.per.reducer&#x3D;1073741824 -- 每个reduce处理的数据量,默认1GB 可通过以下两个参数配合使用： set hive.merge.mapredfiles=true distribute by rand() 小文件太多的话可以打开reduce端的小文件合并，即第一个set，后面的distribute by rand()保证了记录随机分配到50个文件，不管里数据量有多小，最后这50个文件的大小应该是一致的. 有个推送的例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455推送逻辑写在UDF中，如下SQL的实际推送是在Map阶段完成推送并行度，是依据表ads_hive_rooster_ipush_users_1d分区中文件的个数和上面生成Map任务的逻辑一样，根据文件的大小和块大小进行分割add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;dmp-function-lib-test2.jar;create temporary function push as &#39;suishen.temp.udf.PushTestUDF&#39;;select result ,count(*) from ( select pust(uid) as result from ( select app_key ,uid ,item_id ,post_id ,title ,ds ,hh from prod.ads_hive_rooster_ipush_users_1d where ds &#x3D; &#39;20181120&#39; ) a ) a group by result;通过设置文件大小来这是Map个数是一种思路，但是不好把控。另一种方式就是把推送的执行放在Reduce端，通过控制reduce的个数来跟好的控制并行度可通过distribute by rand()进行重分布，然后mapred.reduce.tasks设置reduce执行个数add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;dmp-function-lib-test2.jar;create temporary function push as &#39;suishen.temp.udf.PushTestUDF&#39;;set mapred.reduce.tasks&#x3D;100;select result ,count(*) from ( select pust(uid) as result from ( select app_key ,uid ,item_id ,post_id ,title ,ds ,hh from prod.ads_hive_rooster_ipush_users_1d where ds &#x3D; &#39;20181120&#39; distribute by rand() ) a ) a group by result;","categories":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/categories/hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/tags/hive/"}]},{"title":"CentOS7.2-时间同步工具ntp&chrony","slug":"2018053001-CentOS7.2-时间同步工具ntp&chrony","date":"2018-05-30T06:30:04.000Z","updated":"2018-05-30T09:10:01.085Z","comments":true,"path":"2018/05/30/2018053001-CentOS7.2-时间同步工具ntp&chrony/","link":"","permalink":"http://guoyanlei.top/2018/05/30/2018053001-CentOS7.2-%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7ntp&chrony/","excerpt":"背景集群加了几台新节点，安装了Cloudera之后，总是提示： 不良 : 无法找到主机的 NTP 服务，或该服务未响应时钟偏差请求。 但是，ntpd服务明明开启了，而且连接正常，还是提示时钟偏差。 最终调研了chronyd，发现CentOS7.2已经推荐使用chronyd，因此试着改为chronyd，重启Cloudera agent后问题解决了。下面总结下chronyd时钟同步。","text":"背景集群加了几台新节点，安装了Cloudera之后，总是提示： 不良 : 无法找到主机的 NTP 服务，或该服务未响应时钟偏差请求。 但是，ntpd服务明明开启了，而且连接正常，还是提示时钟偏差。 最终调研了chronyd，发现CentOS7.2已经推荐使用chronyd，因此试着改为chronyd，重启Cloudera agent后问题解决了。下面总结下chronyd时钟同步。 chronydChrony是网络时间协议的 (NTP) 的另一种实现。一直以来众多发行版里标配的都是ntpd对时服务，自rhel7/centos7 起，Chrony做为了发行版里的标配服务，不过老的ntpd服务依旧在rhel7/centos7里可以找到。 Chrony可以同时做为ntp服务的客户端和服务端。默认安装完后有两个程序chronyd和chronyc。 chronyd是一个在系统后台运行的守护进程 chronyc是用来监控chronyd性能和配置其参数程序 安装1234# yum install -y chrony &#x2F;&#x2F;安装服务# systemctl start chronyd.service &#x2F;&#x2F;启动服务# systemctl enable chronyd.service &#x2F;&#x2F;设置开机自启动，默认是enable的 chrony.conf配置Chrony的配置在/etc/chrony.conf，其个是ntpd服务基本相同，主要包含了： 123456789101112131415161718192021222324252627# 时钟服务器，通过prefer提高优先级server 10.9.141.12 iburst minpoll 3 maxpoll 4 preferserver 10.9.255.1 iburst minpoll 3 maxpoll 4 server 10.9.255.2 iburst minpoll 3 maxpoll 4 # 设置当chronyd从可用源中选择同步源时，每个层应该添加多少距离到同步距离。默认情况下，CentOS中设置为0，让chronyd在选择源时忽略源的层级stratumweight 0#chronyd程序的主要行为之一，就是根据实际时间计算出计算机增减时间的比率，将它记录到一个文件中是最合理的，它会在重启后为系统时钟作出补偿，甚至可能的话，会从时钟服务器获得较好的估值；driftfile &#x2F;var&#x2F;lib&#x2F;chrony&#x2F;drift#rtcsync指令将启用一个内核模式，在该模式中，系统时间每11分钟会拷贝到实时时钟（RTC）rtcsync#chronyd将根据需求通过减慢或加速时钟，使得系统逐步纠正所有时间偏差。makestep 10 3#允许你限制chronyd监听哪个网络接口的命令包bindcmdaddress 127.0.0.1bindcmdaddress ::1keyfile &#x2F;etc&#x2F;chrony.keyscommandkey 1generatecommandkeynoclientloglogchange 0.5logdir &#x2F;var&#x2F;log&#x2F;chrony ntpd改到chronyd12345678910111213141. 停掉所有ntpd服务，停止自启动ntpd服务systemctl stop ntpd.servicesystemctl disable ntpd.servicevim &#x2F;etc&#x2F;chrony.conf2. 注释掉已有server，添加roc-master的serverserver 192.168.1.2 iburst # 这里的ip地址是NTP服务器地址3.重启服务systemctl enable chronyd.service -lsystemctl restart chronyd.service -lsystemctl status chronyd.service -lchronyc sourcestatschronyc sources -v chronyd改到ntpd1234567systemctl disable chronyd.service -lsystemctl stop chronyd.service -lsystemctl status chronyd.service -lsystemctl restart ntpd.servicesystemctl status ntpd.servicentpq -pntpstat 状态检测123456789101112# chronydsystemctl status chronyd.service -lchronyc sourcestatschronyc sources -v# ntpdsystemctl status ntpd.service -lntpq -pntpstat# 是否在系统级别得到时间同步：timedatectl chrony的优势 更快的同步只需要数分钟而非数小时时间，从而最大程度减少了时间和频率误差，这对于并非全天 24 小时运行的台式计算机或系统而言非常有用。 能够更好地响应时钟频率的快速变化，这对于具备不稳定时钟的虚拟机或导致时钟频率发生变化的节能技术而言非常有用。 在初始同步后，它不会停止时钟，以防对需要系统时间保持单调的应用程序造成影响。 在应对临时非对称延迟时（例如，在大规模下载造成链接饱和时）提供了更好的稳定性。 无需对服务器进行定期轮询，因此具备间歇性网络连接的系统仍然可以快速同步时钟。","categories":[{"name":"cloudera-manager","slug":"cloudera-manager","permalink":"http://guoyanlei.top/categories/cloudera-manager/"}],"tags":[{"name":"kudu","slug":"kudu","permalink":"http://guoyanlei.top/tags/kudu/"},{"name":"cloudera-manager","slug":"cloudera-manager","permalink":"http://guoyanlei.top/tags/cloudera-manager/"}]},{"title":"Impala优化invalidate metadata & refresh","slug":"2018052301-Impala优化invalidate metadata & refresh","date":"2018-05-23T04:30:04.000Z","updated":"2018-05-25T09:45:28.107Z","comments":true,"path":"2018/05/23/2018052301-Impala优化invalidate metadata & refresh/","link":"","permalink":"http://guoyanlei.top/2018/05/23/2018052301-Impala%E4%BC%98%E5%8C%96invalidate%20metadata%20&%20refresh/","excerpt":"背景CDH集群运行几天后主节点会报磁盘根目录空间不足警告，上机器看一下发现df和du结果差异巨大，猜测有些文件被删除但有进程并未释放文件句柄。","text":"背景CDH集群运行几天后主节点会报磁盘根目录空间不足警告，上机器看一下发现df和du结果差异巨大，猜测有些文件被删除但有进程并未释放文件句柄。 排查执行 lsof | grep “(deleted)”不出所料，发现10000多个以下文件： 1234567891011catalogd 18019 impala 252r REG 253,1 28011300 819296 &#x2F;tmp&#x2F;9a02bf7f-bf3b-461c-a4e9-b49638cbc27b.jar (deleted)catalogd 18019 impala 253r REG 253,1 28010056 819223 &#x2F;tmp&#x2F;852f0830-a9d5-4921-b0e4-3089e520b739.jar (deleted)catalogd 18019 impala 255r REG 253,1 28010056 819238 &#x2F;tmp&#x2F;bdfd341d-db28-4df8-96ab-de5a9308ddb8.jar (deleted)catalogd 18019 impala 256r REG 253,1 28010056 819218 &#x2F;tmp&#x2F;ef50fe5b-aa08-4593-bd99-abcad886216c.jar (deleted)catalogd 18019 impala 258r REG 253,1 28010056 819219 &#x2F;tmp&#x2F;fb5d5375-9514-4af1-91a8-dcdc5849165a.jar (deleted)catalogd 18019 impala 259r REG 253,1 28010056 819234 &#x2F;tmp&#x2F;2194ce13-5805-48d8-bca2-b4d3e849dca8.jar (deleted)catalogd 18019 impala 260r REG 253,1 28010056 819221 &#x2F;tmp&#x2F;a6e621c6-076b-4bed-a45a-559be5f3214e.jar (deleted)catalogd 18019 impala 261r REG 253,1 28010056 819222 &#x2F;tmp&#x2F;3e1a5627-70e7-4070-85ef-4c003405d5b9.jar (deleted)catalogd 18019 impala 262r REG 253,1 28011255 819293 &#x2F;tmp&#x2F;5c270d7c-75c2-4ce2-aa6b-af9f39d42b23.jar (deleted)catalogd 18019 impala 263r REG 253,1 28010056 819224 &#x2F;tmp&#x2F;64e4e5af-8e0e-4d12-b187-adccb24af4e9.jar (deleted)catalogd 18019 impala 264r REG 253,1 28010056 819225 &#x2F;tmp&#x2F;7d4fa926-19dd-44c5-92b6-a34ed3530265.jar (deleted) catalogd为impala负责更新mate的进程，所以对impala进行排查，经验证发现统计中有执行invalidate metadata的命令，该命令执行后会立即出现大量该问题，未关闭的文件句柄经验证为自定义的udf jar包。出现这种情况只能通过重启impala来解决。 结果经粗略研究，应避免使用invalidate metadata 全局更新。可使用invalidate metadata tableName 或者 refresh tableName代替即可。 分析INVALIDATE METADATA: 是用于刷新全库或者某个表的元数据，包括表的元数据和表内的文件数据，它会首先清除表的缓存，然后从metastore中重新加载全部数据并缓存，该操作代价比较重，主要用于在hive中修改了表的元数据，需要同步到impalad 123456789例如create tabledrop tablealter table add columns等。INVALIDATE METADATA; &#x2F;&#x2F;重新加载所有库中的所有表INVALIDATE METADATA [table] &#x2F;&#x2F;重新加载指定的某个表 REFRESH是用于刷新某个表或者某个分区的数据信息，它会重用之前的表元数据，仅仅执行文件刷新操作，它能够检测到表中分区的增加和减少，主要用于表中元数据未修改，数据的修改 12345678910例如INSERT INTOLOAD DATAALTER TABLE ADD PARTITIONLLTER TABLE DROP PARTITION等，如果直接修改表的HDFS文件（增加、删除或者重命名）也需要指定REFRESH刷新数据信息。REFRESH [table] &#x2F;&#x2F;刷新某个表REFRESH [table] PARTITION [partition] &#x2F;&#x2F;刷新某个表的某个分区 INVALIDATE METADATA原理对于INVALIDATE METADATA操作，由客户端将查询提交到某个impalad节点上，执行如下的操作： 获取需要执行INVALIDATE METADATA的表，如果没指定表则不设置表示全部表（不考虑这种情况）。 请求catalogd执行resetMetadata操作，并将isFresh参数设置为false。 catalogd接收到该请求之后执行invalidateTable操作，将该表的缓存清除，然后重新生成该表的缓存对象，新生成的对象只包含表名+库名的信息，为新生成的表对象生成一个新的catalog版本号（假设新的version=1），将这部分信息返回给调用方（impalad），然后异步执行元数据和数据的加载。 impalad收到catalogd的返回值，返回值是更新之后的表缓存对象+版本号，但是这是一个不完整的表元数据，impalad将这个元数据应用到本地元数据缓存。 INVALIDATE METADATA执行完成。 INVALIDATE METADATA操作带来的副作用是：生成一个新的未完成的元数据对象，对于操作请求的impalad（称它为impalad-A），能够立马获取到该对象，对于其它的impalad需要通过statestored同步，因此执行完该操作，处理该操作的impalad对于该表的缓存是一个新的但是不完整的对象，其余的impalad保存的是旧的元数据。 对于后续的该表查询操作，分为如下四种情况： 如果catalogd已经完成该表所有元数据加载，会对该表生成一个新的版本号（假设version=2），然后更新到statestored，由statestored广播到各个impalad节点上，此时所有的查询都查询到最新的元数据和数据。 如果catalogd尚未完成表的元数据加载或者statestored未广播完成，并且接下来请求到impalad-A（之前执行INVALIDATE METADATA的节点），此时impalad在执行语义分析的时候能够检测到表的元数据不完整（因为当前只有表名和库名，没有任何其余的元数据），impalad会直接请求catalogd获取该表最新的元数据，如果catalogd尚未完成元数据加载，则该请求会等到直到catalogd加载完成并返回impalad最新的元数据。 如果catalogd尚未完成表的元数据加载或statestored未广播完成，接下来请求到了其他的impalad节点，如果接受请求的impalad尚未通过statestored同步新的不完整的表元数据（version=1），则该impalad中缓存的关于该表的元数据是执行INVALIDATE METADATA之前的，因此根据旧的元数据处理该查询（可能因为文件被删除导致错误）。 如果catalogd尚未完成表的元数据加载，接下来请求到了其他的impalad节点，如果接受请求的impalad已经通过statestored同步新的不完整的表元数据（version=1），那么接下来会像第二种情况一样处理。从INVALIDATE METADATA的实现来看，该操作不仅仅会全量加载表的元数据和分区、文件元数据，还会影响后面关于该表的查询。 REFRESH原理对于REFRESH操作，由客户端将查询提交到某个impalad节点上，执行如下的操作： 获取需要执行REFRESH的表和分区信息。 请求catalogd执行resetMetadata操作，并将isFresh参数设置为true。 catalogd接收到该请求之后判断是否指定分区，如果指定了分区则执行reload partition操作，如果未指定则执行reload table操作，对于reloadPartition则从metastore中读取partition最新的元数据，然后刷新该partition拥有的所有文件的元数据（大小，权限，数据分布等）；对于reloadTable则从metadata中读取全部的partition信息，然后和缓存中的partition进行比对判断是否有分区需要增加和删除，对于其余的分区则执行元数据的更新。 impalad收到catalogd的返回值，返回值是更新之后该表的缓存数据，impalad会将该数据更新到自己的缓存中。因此接受请求的impalad能够将当前元数据缓存。 REFRESH执行完成。 对于后续的查询，分为如下两种情况： 如果查询提交到到执行REFRESH的impalad节点，那么查询能够使用最新的元数据。 如果查询提交到其他impalad节点，需要依赖于该表0更新后的缓存是否已经同步到impalad中，如果已经完成了同步则可以使用最新的元数据，如果未完成则使用旧的元数据。 可以看出REFRESH操作较之于INVALIDATE METADATA是轻量级的操作，如果更改只涉及到一个分区设置可以只刷新一个分区的元数据，并且它是同步的，对于之后查询的影响较小。 使用原则如果在使用过程中涉及到了元数据或者数据的更新，则需要使用这两者中的一个操作完成，具体如何选择需要根据如下原则： 12345● invalidate metadata操作比refresh要重量级● 如果涉及到表的schema改变，使用invalidate metadata [table]● 如果只是涉及到表的数据改变，使用refresh [table]● 如果只是涉及到表的某一个分区数据改变，使用refresh [table] partition [partition]● 禁止使用invalidate metadata什么都不加，宁愿重启catalogd。 总结REFRESH和INVALIDATE METADATA对于impala而言是比较重要的两个操作，分别处理数据和元数据的修改，其中REFRESH操作是同步的，INVALIDATE METADATA是异步的。","categories":[{"name":"impala","slug":"impala","permalink":"http://guoyanlei.top/categories/impala/"}],"tags":[{"name":"impala","slug":"impala","permalink":"http://guoyanlei.top/tags/impala/"}]},{"title":"Hive跨集群访问hbase和phoenix表","slug":"2018052201-Hive跨集群访问hbase和phoenix表","date":"2018-05-22T07:30:04.000Z","updated":"2018-07-27T11:12:38.766Z","comments":true,"path":"2018/05/22/2018052201-Hive跨集群访问hbase和phoenix表/","link":"","permalink":"http://guoyanlei.top/2018/05/22/2018052201-Hive%E8%B7%A8%E9%9B%86%E7%BE%A4%E8%AE%BF%E9%97%AEhbase%E5%92%8Cphoenix%E8%A1%A8/","excerpt":"在这里记录下公司前段时间对集群做的调整： 1）背景：由于之前集群是基于虚拟化的方式搭建的集群，一个物理节点上虚拟出了十多个节点，由于每个物理节点的磁盘是固定的，随着我们数据量的增长，以及数据计算频度的增加，这套集群的磁盘io已经达到瓶颈，急需搭建新的物理节点集群。 2）问题：这套老集群上部署着数据平台所用到的hbase和phoenix等，由于不能影响业务，所以短时间内不好进行迁移，但是集群的计算资源又不够用，继续把计算迁移出去。","text":"在这里记录下公司前段时间对集群做的调整： 1）背景：由于之前集群是基于虚拟化的方式搭建的集群，一个物理节点上虚拟出了十多个节点，由于每个物理节点的磁盘是固定的，随着我们数据量的增长，以及数据计算频度的增加，这套集群的磁盘io已经达到瓶颈，急需搭建新的物理节点集群。 2）问题：这套老集群上部署着数据平台所用到的hbase和phoenix等，由于不能影响业务，所以短时间内不好进行迁移，但是集群的计算资源又不够用，继续把计算迁移出去。 3）方案：一方面对新老集群实时日志收集进行双写，另一方面把hive表中的统计数据跨集群迁移，然后在新集群上进行统计操作，把统计后的数据跨集群写入老集群的hbase和phoenix中。 本文记录下跨集群写入Hbase和Phoenix的方法。 跨集群写入 hbase 表在新建Hbas外部表时，指定老集群hbase所在的zk 123456789101112131415-- hbase外部表（hive执行）set hbase.zookeeper.quorum&#x3D;10.10.95.131:2181,10.10.34.92:2181,10.10.48.188:2181;create EXTERNAL table hbase_ads_dmp_uid_module_pv_1d ( key String ,stat_date string ,pv bigint ,clk bigint ,page_view bigint)stored by &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;with serdeproperties (&quot;hbase.table.name&quot;&#x3D;&quot;hbase_ads_dmp_uid_module_pv_1d&quot;,&quot;hbase.columns.mapping&quot; &#x3D; &quot;:key,T:stat_date,T:pv#b,T:clk#b,T:page_view#b&quot;); 在执行hive写入Hbase SQL时需要加上老hbase的zk 1234567set hbase.zookeeper.quorum&#x3D;10.10.95.131:2181,10.10.34.92:2181,10.10.48.188:2181;insert overwrite table prod.ads_hbase_dmp_ad_mapper_pv_1dselect pv ,uv from prod.ads_hive_dmp_ad_mapper_pv_1d distribute by rand(); phoenix 表可在建外部表时直接指定phoenix所在的zk 12345678910111213141516171819-- phoenix外部表（hive执行）add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;phoenix-hive-4.2.2-jar-with-dependencies.jar;CREATE EXTERNAL TABLE PHOENIX_ADS_DMP_USER_START_1D( stat_date string ,app_key string ,start_type string ,rn int ,start_num_sum bigint ,uv bigint)STORED BY &quot;org.apache.phoenix.hive.PhoenixStorageHandler&quot;TBLPROPERTIES(&#39;phoenix.zookeeper.quorum&#39;&#x3D;&#39;10.10.95.131,10.10.34.92,10.10.48.188:2181&#39;,&#39;phoenix.hbase.table.name&#39;&#x3D;&#39;PHOENIX_ADS_DMP_USER_START_1D&#39;,&#39;phoenix.zookeeper.znode.parent&#39;&#x3D;&#39;hbase&#39;,&#39;phoenix.rowkeys&#39;&#x3D;&#39;stat_date,app_key,start_type,rn&#39;,&#39;phoenix.column.mapping&#39;&#x3D;&#39;start_num_sum:A.start_num_sum,uv:A.uv&#39;); 还需要再hivesever2所在的机器添加hbase配置 123456789101112131415&#x2F;etc&#x2F;hbase&#x2F;conf&#x2F;hbase-site.xml&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;&#x2F;name&gt; &lt;value&gt;10.10.95.131:2181,10.10.34.92:2181,10.10.48.188:2181&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hbase.client.scanner.caching&lt;&#x2F;name&gt; &lt;value&gt;10000&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hbase.rpc.timeout&lt;&#x2F;name&gt; &lt;value&gt;150000&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;","categories":[{"name":"hbase","slug":"hbase","permalink":"http://guoyanlei.top/categories/hbase/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/tags/hive/"},{"name":"跨集群","slug":"跨集群","permalink":"http://guoyanlei.top/tags/%E8%B7%A8%E9%9B%86%E7%BE%A4/"},{"name":"hbase","slug":"hbase","permalink":"http://guoyanlei.top/tags/hbase/"},{"name":"phoenix","slug":"phoenix","permalink":"http://guoyanlei.top/tags/phoenix/"}]},{"title":"CDH安装phoenix&表数据导入导出","slug":"2018052202-CDH安装phoenix&表数据导入导出","date":"2018-05-22T07:30:04.000Z","updated":"2019-02-18T09:38:41.453Z","comments":true,"path":"2018/05/22/2018052202-CDH安装phoenix&表数据导入导出/","link":"","permalink":"http://guoyanlei.top/2018/05/22/2018052202-CDH%E5%AE%89%E8%A3%85phoenix&%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/","excerpt":"本文记录下CDH安装Phoenix所注意到事项，以及Pheonix表数据的导入导出。","text":"本文记录下CDH安装Phoenix所注意到事项，以及Pheonix表数据的导入导出。 cloudera manager 安装pheonix下面总结Cloudera Manager安装并使用pheonix的步骤。 1234567891. 下载pheonix-CDH版本 phoenix-4.13.2-cdh5.11.22. 拷贝到一个节点，并解压3. 将phoenix-4.13.2-cdh5.11.2-server.jar 拷贝到Hbase集群的每一个Master和Region Server节点的&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;4. 重启Hbase节点5. 建立启动脚本（.&#x2F;sqlline.py node1.zk.bigdata.dmp.com,node2.zk.bigdata.dmp.com,node3.zk.bigdata.dmp.com:2181:&#x2F;hbase）6. 若想在hive中通过PhoenixStorageHandler方式读取pheonix中的数据，则需将phoenix-4.13.2-cdh5.11.2-client.jar 和 phoenix-hive-4.13.2-cdh5.11.2.jar 拷贝到每个HiveServer节点的&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hive&#x2F;lib下7. 重启HiveServer 使用Hive读写phoenix在数据仓库Hive中最常用的场景就是将计算完的数据写入到phoenix，方便之后使用phoenix来查询。 方式就是在hive中建立与phoenix映射的外部表，建立之前需完成安装步骤中的6，7，建立完成后即可读写phoenix的数据。 1234567891011121314151617181920-- phoenix外部表（hive执行）add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;phoenix-4.13.2-cdh5.11.2-hive.jar;CREATE EXTERNAL TABLE dws_phoenix_log_project_table_id( m_item_id string, app_key string, project string, p_table string, stat_date string, t_module string)STORED BY &quot;org.apache.phoenix.hive.PhoenixStorageHandler&quot;TBLPROPERTIES( &quot;phoenix.table.name&quot; &#x3D; &quot;DWS_PHOENIX_DMP_LOG_PROJECT_TABLE_ID&quot;, &quot;phoenix.zookeeper.quorum&quot; &#x3D; &quot;node1.zk.bigdata.dmp.com,node2.zk.bigdata.dmp.com,node3.zk.bigdata.dmp.com&quot;, &quot;phoenix.zookeeper.znode.parent&quot; &#x3D; &quot;&#x2F;hbase&quot;, &quot;phoenix.zookeeper.client.port&quot; &#x3D; &quot;2181&quot;, &quot;phoenix.rowkeys&quot; &#x3D; &quot;M_ITEM_ID,APP_KEY,PROJECT,P_TABLE,STAT_DATE&quot;, &quot;phoenix.column.mapping&quot; &#x3D; &quot;m_item_id:M_ITEM_ID,app_key:APP_KEY,project:PROJECT,p_table:P_TABLE,stat_date:STAT_DATE,t_module:T_MODULE&quot;); 使用Pig将phoenix表数据批量导出Phoenix提供了BulkLoad工具，使得用户可以将大数据量的csv格式数据高效地通过phoenix导入hbase，那么phoenix是否也存在高效导出csv数据的工具类呢？ 幸运的是phoenix官方确实提供了一个高效的导出工具类，但是必须依赖于pig。 官方文档：https://phoenix.apache.org/pig_integration.html Pig是SQL-like语言，是在MapReduce上构建的一种高级查询语言，把一些运算编译进MapReduce模型的Map和Reduce中，并且用户可以定义自己的功能。 Cloudera Manager默认安装pig，可在集群任意节点启动pig客户端。 123运行- pig- pig -x mapreduce 下面给出一个批量导出phoenix表数据的Pig脚本 12345678910111213141516REGISTER &#x2F;data&#x2F;dmp&#x2F;phoenix&#x2F;phoenix-4.13.2-cdh5.11.2-client.jarrows &#x3D; load &#39;hbase:&#x2F;&#x2F;table&#x2F;DWS_PHOENIX_DMP_LOG_PROJECT_TABLE_ID&#39; USING org.apache.phoenix.pig.PhoenixHBaseLoader(&#39;node1.zk.bigdata.dmp.com,node2.zk.bigdata.dmp.com,node3.zk.bigdata.dmp.com&#39;); STORE rows INTO &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;dws_phoenix_log_project_table_id&#39; USING PigStorage(&#39;,&#39;);默认的导出路径是保存在HDFS中的，之后可以建立Hive外部表读取文件中的数据# 还可以批量导出部分字段A &#x3D; load &#39;hbase:&#x2F;&#x2F;table&#x2F;DWS_PHOENIX_DMP_LOG_PROJECT_TABLE_ID&#x2F;APP_KEY,PROJECT&#39; using org.apache.phoenix.pig.PhoenixHBaseLoader(&#39;xxx&#39;);# 也可以批量某一SQL的数据A &#x3D; load &#39;hbase:&#x2F;&#x2F;query&#x2F;SELECT ID,NAME FROM DWS_PHOENIX_DMP_LOG_PROJECT_TABLE_ID WHERE APP_KEY &#x3D; 111&#39; using org.apache.phoenix.pig.PhoenixHBaseLoader(&#39;xxx&#39;);- Only a SELECT query is allowed. No DML statements such as UPSERT or DELETE.- The query may not contain any GROUP BY, ORDER BY, LIMIT, or DISTINCT clauses.- The query may not contain any AGGREGATE functions.","categories":[{"name":"pheonix","slug":"pheonix","permalink":"http://guoyanlei.top/categories/pheonix/"}],"tags":[{"name":"phoenix","slug":"phoenix","permalink":"http://guoyanlei.top/tags/phoenix/"},{"name":"pig","slug":"pig","permalink":"http://guoyanlei.top/tags/pig/"}]},{"title":"Hive表跨集群迁移","slug":"2018052201-Hive表跨集群迁移","date":"2018-05-22T06:30:04.000Z","updated":"2018-05-25T09:26:05.499Z","comments":true,"path":"2018/05/22/2018052201-Hive表跨集群迁移/","link":"","permalink":"http://guoyanlei.top/2018/05/22/2018052201-Hive%E8%A1%A8%E8%B7%A8%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB/","excerpt":"公司新建了个大数据集群，需对老集群上的hive表数据迁移到新的集群中。这里总结下整个Hive表迁移的过程。","text":"公司新建了个大数据集群，需对老集群上的hive表数据迁移到新的集群中。这里总结下整个Hive表迁移的过程。 创建数据临时目录12hdfs dfs -mkdir &#x2F;tmp&#x2F;hive-export 生成数据导出脚本123456789-- 该命令会生成所有表的导出脚本hive -e &quot;show tables&quot; | awk &#39;&#123;printf &quot;export table %s to @&#x2F;tmp&#x2F;hive-export&#x2F;%s@;\\n&quot;,$1,$1&#125;&#39; | sed &quot;s&#x2F;@&#x2F;&#39;&#x2F;g&quot; &gt; export.sql-- 若只想导出某个，可在hive中单独执行export table ods_log_event_base_data_orc_hh to &#39;&#x2F;tmp&#x2F;hive-export&#x2F;ods_log_event_base_data_orc_hh&#39;;在运行前，需要将hadoop-distcp-2.5.0-cdh5.3.3.jar 拷贝到hive的lib中，否则会报错（Cannot find DistCp class package: org.apache.hadoop.tools.DistCp）cp &#x2F;data&#x2F;dmp&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;tools&#x2F;lib&#x2F;hadoop-distcp-2.5.0-cdh5.3.3.jar &#x2F;data&#x2F;dmp&#x2F;hive&#x2F;lib&#x2F; 手工导出数据到HDFS123456hive -f export.sql完成后，hdfs的导出路径上会有相关的信息：&#x2F;tmp&#x2F;hive-export&#x2F;ods_log_event_base_data_orc_hh进去可看到_metadata和各分区信息 下载上传数据 or diskcp数据下载HDFS数据到本地，并传送到目标集群（targetDir为目标集群地址）的/tmp/hive-export目录： 123hdfs dfs -get &#x2F;tmp&#x2F;hive-export&#x2F;scp -r hive-export&#x2F; export.sql root@targetDirhdfs dfs -put hive-export&#x2F; &#x2F;tmp&#x2F;hive-export 或者使用diskcp命令，通过分布式方式拷贝到目标集群 1hadoop distcp hdfs:&#x2F;&#x2F;hadoop-dmp:8020&#x2F;tmp&#x2F;hive-export&#x2F;ods_log_event_base_data_orc_hh hdfs:&#x2F;&#x2F;10.215.24.109:8020&#x2F;tmp&#x2F;hive-export&#x2F;ods_log_event_base_data_orc_hh 生成数据导入脚本123456执行如下命令，复制导出脚本，并将脚本修改为导入脚本：cp export.sql import.sqlsed -i &#39;s&#x2F;export table&#x2F;import table&#x2F;g&#39; import.sqlsed -i &#39;s&#x2F; to &#x2F; from &#x2F;g&#39; import.sql 导入数据1hive -f import.sql","categories":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/categories/hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/tags/hive/"},{"name":"跨集群","slug":"跨集群","permalink":"http://guoyanlei.top/tags/%E8%B7%A8%E9%9B%86%E7%BE%A4/"}]},{"title":"sqoop-hive导入导出","slug":"2018052102-sqoop-hive导入导出","date":"2018-05-21T07:30:04.000Z","updated":"2018-07-06T07:49:16.077Z","comments":true,"path":"2018/05/21/2018052102-sqoop-hive导入导出/","link":"","permalink":"http://guoyanlei.top/2018/05/21/2018052102-sqoop-hive%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/","excerpt":"sqoop-hive导入导出本文为sqoop使用的记录，包括mysql到hive的导入和hive到mysql的导出。 详细可参考官方手册：https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html","text":"sqoop-hive导入导出本文为sqoop使用的记录，包括mysql到hive的导入和hive到mysql的导出。 详细可参考官方手册：https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html mysql导入hive12345678910sqoop import --connect jdbc:mysql:&#x2F;&#x2F;10.10.95.186:3306&#x2F;ss_peacock \\--table wnl_life_card_item \\--username mysqlsiud \\--password &#39;mysql!@#456&#39; \\--hive-import \\--hive-overwrite \\--hive-table prod.ods_dim_peacock_wnl_life_card_item -m 1 \\--fields-terminated-by &#39;\\t&#39; import主要属性说明： 12345678910111213141516171819202122232425262728293031323334--connect &lt;jdbc-uri&gt;--connection-manager &lt;class-name&gt;--driver &lt;class-name&gt;--hadoop-mapred-home &lt;dir&gt; 不设置会找系统配的环境变量$HADOOP_MAPRED_HOME--password-file 可执行存有密码的文件--password &lt;password&gt;--username &lt;username&gt;--table &lt;table-name&gt; Table to read--target-dir &lt;dir&gt; HDFS destination dir--warehouse-dir &lt;dir&gt; HDFS parent for table destination--append 将数据追加到hdfs中已经存在的dataset中--delete-target-dir Delete the import target directory if it exists--as-avrodatafile 将数据导入到一个Avro数据文件中--as-sequencefile 将数据导入到一个sequence文件中--as-textfile 将数据导入到一个普通文本文件中(default)--as-parquetfile 将数据导入到一个Parquet数据文件中-z or --compress Enable compression--compression-codec &lt;c&gt; Use Hadoop codec (default gzip)--where &lt;where clause&gt; WHERE clause to use during import--boundary-query &lt;statement&gt; 边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：--boundary-query &#39;select id,creationdate from person where id &#x3D; 3&#39;，--columns &lt;col,col,col…&gt; 指定要导入的字段值，格式如：--columns id,username--direct 直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快-m or --num-mappers &lt;n&gt; 设置并行度 n个map任务–null-string &lt;null-string&gt; 可选参数，如果没有指定，则字符串&quot;null&quot;将被使用–null-non-string&lt;null-string&gt; 可选参数，如果没有指定，则字符串&quot;null&quot;将被使用--fields-terminated-by &lt;char&gt; 字段分隔符--lines-terminated-by &lt;char&gt; 行分割符 增量导入属性说明： 1234--check-column (col) 用来作为判断的列名，如id，不能是(CHAR&#x2F;NCHAR&#x2F;VARCHAR&#x2F;VARNCHAR&#x2F; LONGVARCHAR&#x2F;LONGNVARCHAR)--incremental (mode) append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录--last-value (value) 指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值 hive属性说明： 12345678910111213--hive-home &lt;dir&gt; hive的安装路径，若不设会取系统变量$HIVE_HOME--hive-import 将数据从关系数据库中导入到hive表中--hive-overwrite 覆盖掉在hive表中已经存在的数据--create-hive-table 默认是false,如果目标表已经存在了，那么创建任务会失败--hive-table 后面接要创建的hive表--hive-delims-replacement &lt;arg&gt; 用自定义的字符串替换掉数据中的\\n, \\r, and \\01等字符--hive-drop-import-delims 在导入数据到hive中时，去掉数据中\\n,\\r和\\01这样的字符--map-column-hive &lt;arg&gt; 生成hive表时，可以更改生成字段的数据类型，格式如：--map-column-hiveTBL_ID&#x3D;String,LAST_ACCESS_TIME&#x3D;string--hive-partition-key 创建分区，后面直接跟分区名即可，创建完毕后，通过describe 表名可以看到分区名，默认为string型--hive-partition-value&lt;v&gt; 该值是在导入数据到hive中时，与–hive-partition-key设定的key对应的value值。 hive导出到mysql1234567891011- mysql中的目标表为两个联合主键sqoop export --connect jdbc:mysql:&#x2F;&#x2F;m1.mysql.leopard.dmp.com:3306&#x2F;ssy_leopard \\--username mysqlsiud \\--password mysql!@#456 \\--table leopard_app_channel \\-m 1 \\--export-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ads_hive_leopard_dim_app_channel \\--update-key app_key,channel \\--update-mode allowinsert \\--input-fields-terminated-by &#39;\\t&#39; 导出主要参数说明： 123456789101112131415--columns &lt;col,col,col…&gt; 要导出的列--direct 直接导入模式，更快速--export-dir &lt;dir&gt; HDFS上要导出的数据文件-m,--num-mappers &lt;n&gt; 导出并行度n map tasks--table &lt;table-name&gt; 要导出的目标表名称--call &lt;stored-proc-name&gt; 存储过程--update-key &lt;col-name&gt; 根据该列的值，判定是否需要更新或插入，通常是表的主键，或联合主键--update-mode &lt;mode&gt; 分为 updateonly (default) 和 allowinsert.--input-null-string &lt;null-string&gt; 可选参数，如果没有指定，则字符串&quot;null&quot;将被使用--input-null-non-string &lt;null-string&gt; 可选参数，如果没有指定，则字符串&quot;null&quot;将被使用--staging-table &lt;staging-table-name&gt; 导入到目标表前的缓存表--clear-staging-table 指示暂存表中存在的任何数据都可以删除.--batch 使用底层语句执行的批处理模式 mysql表导入hive分区表12345678910111213141516171819202122232425# mysql的按天分表导入hive指定的分区sqoop import --connect jdbc:mysql:&#x2F;&#x2F;xxx:3306&#x2F;zhwnl_community?tinyInt1isBit&#x3D;false \\--table coin_gold_account_flow_20180501 \\--username mysqlsiud \\--password &#39;xxx&#39; \\--hive-import \\--hive-overwrite \\--hive-table prod.ads_hive_dmp_dim_coin_account_flow_1d \\--hive-partition-key ds \\--hive-partition-value 20180501 \\--fields-terminated-by &#39;\\t&#39; -m 4# mysql表中指定日期的数据导入到hive指定分区中sqoop import --connect jdbc:mysql:&#x2F;&#x2F;xxx:3306&#x2F;ss_peacock \\--table item_dislike \\--where &quot;create_date &#x3D; &#39;20180501&#39;&quot; \\--username mysqlsiud \\--password &#39;xxx&#39; \\--hive-import \\--hive-overwrite \\--hive-table prod.ods_dim_peacock_item_dislike -m 1 \\--hive-partition-key ds \\--hive-partition-value 20180501 \\--fields-terminated-by &#39;\\t&#39; hive分区表导入到mysql12345678910111213# 从HIVE分区表导入到MySQL，需要依次导入每个分区的数据\\sqoop export \\--connect jdbc:mysql:&#x2F;&#x2F;xxx:3306&#x2F;Server74 \\--username mysqlsiud \\--password xxx \\--table prod.ods_dim_peacock_item_dislike \\--hive-partition-key ds \\--hive-partition-value 20180501 \\--export-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.ods_dim_peacock_item_dislike&#x2F;ds&#x3D;2017-11-15&#x2F; \\--input-fields-terminated-by &#39;\\t&#39; \\--input-lines-terminated-by &#39;\\n&#39;说明：--export-dir这个参数是必须的，指定hive表源文件路径后，sqoop回到路径下路径下的文件，文件不是路径否则报错。所以分区表需要单独指定每个分区的目录，分别导入","categories":[{"name":"数据同步","slug":"数据同步","permalink":"http://guoyanlei.top/categories/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"}],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"http://guoyanlei.top/tags/sqoop/"}]},{"title":"cloudera-manager中使用sqoop","slug":"2018052101-cloudera-manager中使用sqoop","date":"2018-05-21T03:30:04.000Z","updated":"2018-07-06T07:49:02.699Z","comments":true,"path":"2018/05/21/2018052101-cloudera-manager中使用sqoop/","link":"","permalink":"http://guoyanlei.top/2018/05/21/2018052101-cloudera-manager%E4%B8%AD%E4%BD%BF%E7%94%A8sqoop/","excerpt":"场景描述 sqoop 与 hdfs和hive 没有安装在一起，sqoop单独一台机器。 使用cloudera-manager管理 使用问题 如何将sqoop使用集群的hdfs和hive","text":"场景描述 sqoop 与 hdfs和hive 没有安装在一起，sqoop单独一台机器。 使用cloudera-manager管理 使用问题 如何将sqoop使用集群的hdfs和hive 解决方案环境变量配置123export HADOOP_COMMON_HOME&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoopexport HADOOP_MAPRED_HOME&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce 配置sqoop1234&#x2F;data&#x2F;dmp&#x2F;sqoop&#x2F;conf.cloudera.sqoop_client&#x2F;sqoop-env.shexport HADOOP_COMMON_HOME&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoopexport HADOOP_MAPRED_HOME&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce 添加hdfs 和yarn的配置123456789&#x2F;data&#x2F;dmp&#x2F;hadoop&#x2F;conf.cloudera.hdfsscp core-site.xml root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hadoop&#x2F;conf.dist scp hdfs-site.xml root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hadoop&#x2F;conf.dist scp hadoop-env.sh root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hadoop&#x2F;conf.dist scp ssl-client.xml root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hadoop&#x2F;conf.dist &#x2F;data&#x2F;dmp&#x2F;hadoop&#x2F;conf.cloudera.yarnscp mapred-site.xml root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hadoop&#x2F;conf.distscp yarn-site.xml root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hadoop&#x2F;conf.dist 添加hive配置1234cd &#x2F;data&#x2F;dmp&#x2F;hive&#x2F;conf.cloudera.hivescp hive-site.xml root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hive&#x2F;conf.distscp hive-env.sh root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hive&#x2F;conf.distscp log4j.properties root@node1.azkaban.bigdata.dmp.com:&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;etc&#x2F;hive&#x2F;conf.dist 使用实例12345678910111213su impalahdfs dfs -rm -r &#x2F;user&#x2F;impala&#x2F;tag_item_relationsqoop import --connect jdbc:mysql:&#x2F;&#x2F;xxx:3306&#x2F;ss_peacock \\--table tag_item_relation \\--username xxxx \\--password &#39;xxxx&#39; \\--hive-import \\--hive-overwrite \\--hive-table prod.ods_dim_peacock_tag_item_relation \\--fields-terminated-by &#39;\\t&#39; -m 1","categories":[{"name":"数据同步","slug":"数据同步","permalink":"http://guoyanlei.top/categories/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"}],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"http://guoyanlei.top/tags/sqoop/"}]},{"title":"Hive-JDBC连接HiveServer2传递参数","slug":"2018052001-Hive-JDBC连接HiveServer2传递参数","date":"2018-05-20T14:30:04.000Z","updated":"2018-05-25T04:39:11.845Z","comments":true,"path":"2018/05/20/2018052001-Hive-JDBC连接HiveServer2传递参数/","link":"","permalink":"http://guoyanlei.top/2018/05/20/2018052001-Hive-JDBC%E8%BF%9E%E6%8E%A5HiveServer2%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0/","excerpt":"HiveServer2（以下简称：HS2）是Hive提供的一种jdbc服务，用户可以通过Hive自带的Beeline连接，也可以使用Java、Python或者PHP等通过jdbc的方式连接。下面以Java连接HiveServer2为例来介绍几种向Hive传递参数的方法。","text":"HiveServer2（以下简称：HS2）是Hive提供的一种jdbc服务，用户可以通过Hive自带的Beeline连接，也可以使用Java、Python或者PHP等通过jdbc的方式连接。下面以Java连接HiveServer2为例来介绍几种向Hive传递参数的方法。 Java-JDBC连接HS2连接到HS2，一般需要提供HS2的地址、端口号、连接的Hive库、用户名和密码这几个必选项，示例代码如下： 12345678Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);Properties info &#x3D; new Properties();info.setProperty(&quot;user&quot;, &quot;user_name&quot;);info.setProperty(&quot;password&quot;, &quot;passwd&quot;);String JDBC_URL&#x3D;&quot;jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;default&quot;;Connection conn &#x3D; DriverManager.getConnection(JDBC_URL, info);HiveStatement stat &#x3D; (HiveStatement) conn.createStatement(); 只要URL、用户名和密码正确的话，通过上面的示例代码，就可以连接到HS2执行操作了。但是往往这样还不够，如果我们想通过传递一些Hive的配置信息，那该怎么办呢？ 传递参数1.类HiveClient方式使用过Hive客户端的用户都知道，如果我们想改变Hive的某一项客户端配置的话，可以通过set hive_conf_key=hive_conf_value;的方式来修改。由此，很自然的我们会想到在获得了一个JDBC的连接后，我们执行一下上面的语句不就可以了。示例代码如下： 123&#x2F;&#x2F; 我们执行一些复杂的sql的时候，往往需要制定一个队列，假设队列的名字为&quot;root.hive-server2&quot;stat.execute(&quot;set mapreduce.job.queuename&#x3D;root.hive-server2&quot;); NOTE：需要注意的是，execute中的set语句不能包含分号（不能是set mapreduce.job.queuename=root.hive-server2;），这是和客户端的区别，否则不生效 2.在JDBC URL中传对jdbc比较熟悉的用户，都知道可以在jdbc的连接中传递一些参数，hive也一样支持。对于上面的需求，可以把充分利用JDBC_URL。示例代码如下： 123String JDBC_URL&#x3D;&quot;jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;default?mapreduce.job.queuename&#x3D;root.hive-server2;hive.cli.print.header&#x3D;false&quot;;Connection conn &#x3D; DriverManager.getConnection(JDBC_URL, info); NOTE：普通的jdbc传递参数不一样的地方，那就是这里是通过使用分号来分割多个hive的配置变量的，而不是使用’&amp;’。 NOTE：另外，这里传递hive配置和hive变量还是有区别的，Hive是通过’#’来分割Hive配置列表和Hive变量列表的。 123456&#x2F;&#x2F; 源码HiveConnection.java有说明&#x2F;&#x2F; JDBC URL: jdbc:hive2:&#x2F;&#x2F;&lt;host&gt;:&lt;port&gt;&#x2F;dbName;sess_var_list?hive_conf_list#hive_var_list&#x2F;&#x2F; each list: &lt;key1&gt;&#x3D;&lt;val1&gt;;&lt;key2&gt;&#x3D;&lt;val2&gt; and so on参考：https:&#x2F;&#x2F;cwiki.apache.org&#x2F;confluence&#x2F;display&#x2F;Hive&#x2F;HiveServer2+Clients#HiveServer2Clients-UsingJDBC 3.通过连接属性配置如果需要传递的配置数目比较多，使用上面的方法，难免有点冗余和负杂，URL将会变得特别长。其实，我们可以像配置user和password一样来传递配置。区别于user和password配置方式的地方是，必须明确指出配置的是一个hive_conf还是hive_var，否则配置不会生效。示例代码如下： 12345678910Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);Properties info &#x3D; new Properties();info.setProperty(&quot;user&quot;, &quot;user_name&quot;);info.setProperty(&quot;password&quot;, &quot;passwd&quot;);&#x2F;&#x2F; 这里传递了一个队列的hive_confinfo.setProperty(&quot;hiveconf:mapreduce.job.queuename&quot;, &quot;root.hive-server2&quot;);String JDBC_URL&#x3D;&quot;jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;default&quot;;Connection conn &#x3D; DriverManager.getConnection(JDBC_URL, info);HiveStatement stat &#x3D; (HiveStatement) conn.createStatement();","categories":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/categories/hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://guoyanlei.top/tags/hive/"},{"name":"HiveServer2","slug":"HiveServer2","permalink":"http://guoyanlei.top/tags/HiveServer2/"}]},{"title":"azkaban3.0使用记录","slug":"2018051901-azkaban3.0使用记录","date":"2018-05-19T06:24:04.000Z","updated":"2018-05-25T10:23:00.461Z","comments":true,"path":"2018/05/19/2018051901-azkaban3.0使用记录/","link":"","permalink":"http://guoyanlei.top/2018/05/19/2018051901-azkaban3.0%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/","excerpt":"本文记录azkaban的安装和使用中遇到问题。","text":"本文记录azkaban的安装和使用中遇到问题。 安装 Azkaban官网:https://azkaban.github.io 软件下载地址:https://github.com/azkaban/azkaban 官方插件地址:https://github.com/azkaban/azkaban-plugins 官方文档地址:http://azkaban.github.io/azkaban/docs/latest 下载12345git clone https:&#x2F;&#x2F;github.com&#x2F;azkaban&#x2F;azkaban.git或 wget https:&#x2F;&#x2F;codeload.github.com&#x2F;azkaban&#x2F;azkaban&#x2F;zip&#x2F;mastermv master azkaban.zipunzip azkaban.zip 编译12345678910111213141516171819202122232425262728cd azkaban.&#x2F;gradlew clean.&#x2F;gradlew build.&#x2F;gradlew installDist --编译完成后mkdir &#x2F;data&#x2F;dmp&#x2F;azkabancd azkaban-solo-server&#x2F;build&#x2F;distributionscp azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz &#x2F;data&#x2F;dmp&#x2F;azkabancd azkaban-exec-server&#x2F;build&#x2F;distributionscp azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz &#x2F;data&#x2F;dmp&#x2F;azkabancd azkaban-web-server&#x2F;build&#x2F;distributionscp azkaban-web-server-0.1.0-SNAPSHOT.tar.gz &#x2F;data&#x2F;dmp&#x2F;azkabancd &#x2F;data&#x2F;dmp&#x2F;azkabantar -zxvf azkaban-solo-server-0.1.0-SNAPSHOT.tar.gztar -zxvf azkaban-exec-server-0.1.0-SNAPSHOT.tar.gztar -zxvf azkaban-web-server-0.1.0-SNAPSHOT.tar.gzmv azkaban-solo-server-0.1.0-SNAPSHOT azkaban-solomv azkaban-exec-server-0.1.0-SNAPSHOT azkaban-execmv azkaban-web-server-0.1.0-SNAPSHOT azkaban-web 配置mysql表12345678910111213--创建Azkaban数据库create database azkaban;use azkaban --创建Azkaban用户CREATE USER &#39;azkaban&#39;@&#39;%&#39; IDENTIFIED BY &#39;azkaban&#39;;--赋权限GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to &#39;azkaban&#39;@&#39;%&#39; WITH GRANT OPTION;source &#x2F;data&#x2F;dmp&#x2F;create-all-sql-0.1.0-SNAPSHOT.sql 配置conf默认 web 和 exec下的是没有conf等文件的，需要从solo模式中拷贝过来 需要： azkaban.properties azkaban-users.xml global.properties log4j.properties web 和 exec下的azkaban.properties 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364zkaban Personalization Settingsazkaban.name&#x3D;dmpazkaban.label&#x3D;azkaban 3.0azkaban.color&#x3D;#FF3601azkaban.default.servlet.path&#x3D;&#x2F;indexweb.resource.dir&#x3D;&#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;azkaban-web&#x2F;web&#x2F;default.timezone.id&#x3D;Asia&#x2F;Shanghai# Azkaban UserManager classuser.manager.class&#x3D;azkaban.user.XmlUserManageruser.manager.xml.file&#x3D;&#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;azkaban-exec&#x2F;conf&#x2F;azkaban-users.xml# Loader for projectsexecutor.global.properties&#x3D;&#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;azkaban-exec&#x2F;conf&#x2F;global.propertiesazkaban.project.dir&#x3D;projectsdatabase.type&#x3D;mysqlmysql.port&#x3D;3306mysql.host&#x3D;m1.mysql.bigdata.dmp.commysql.database&#x3D;azkabanmysql.user&#x3D;azkabanmysql.password&#x3D;azkabanmysql.numconnections&#x3D;100jetty.use.ssl&#x3D;falsejetty.maxThreads&#x3D;25jetty.ssl.port&#x3D;8443jetty.keystore&#x3D;&#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;keystorejetty.password&#x3D;azkabanjetty.keypassword&#x3D;azkabanjetty.truststore&#x3D;&#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;keystorejetty.trustpassword&#x3D;azkaban# Velocity dev modevelocity.dev.mode&#x3D;false# Azkaban Executor settingsexecutor.maxThreads&#x3D;50executor.port&#x3D;12321executor.flow.threads&#x3D;30# mail settingsmail.sender&#x3D;xxxmail.host&#x3D;xxxmail.user&#x3D;xxxmail.password&#x3D;xxxjob.failure.email&#x3D;xxxjob.success.email&#x3D;xxx# azkaban.webserver.external_hostname&#x3D;myazkabanhost.com# azkaban.webserver.external_ssl_port&#x3D;443# azkaban.webserver.external_port&#x3D;8081lockdown.create.projects&#x3D;falsecache.directory&#x3D;cache# JMX statsjetty.connector.stats&#x3D;trueexecutor.connector.stats&#x3D;true# Azkaban plugin settingsazkaban.jobtype.plugin.dir&#x3D;&#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;azkaban-exec&#x2F;plugins&#x2F;jobtypes web 和 exec下的azkaban-users.xml 123456&lt;azkaban-users&gt; &lt;user password&#x3D;&quot;123&quot; roles&#x3D;&quot;admin&quot; username&#x3D;&quot;azkaban&quot;&#x2F;&gt; &lt;role name&#x3D;&quot;admin&quot; permissions&#x3D;&quot;ADMIN&quot;&#x2F;&gt; &lt;role name&#x3D;&quot;metrics&quot; permissions&#x3D;&quot;METRICS&quot;&#x2F;&gt;&lt;&#x2F;azkaban-users&gt; web 和 exec下的log4j.properties 123456log4j.rootLogger&#x3D;INFO,Clog4j.appender.C&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.C.Target&#x3D;System.errlog4j.appender.C.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.C.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss&#125; %-5p %c&#123;1&#125;:%L - %m%n 后台启动123nohup &#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;azkaban-web&#x2F;bin&#x2F;azkaban-web-start.sh &gt; &#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;azkaban-web&#x2F;azkaban-web.log &amp;nohup &#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;azkaban-exec&#x2F;bin&#x2F;azkaban-executor-start.sh &gt; &#x2F;data&#x2F;dmp&#x2F;azkaban&#x2F;azkaban-exec&#x2F;azkaban-exec.log &amp; 使用系统环境变量由于azkaban有自己的环境变量设置，因此若想通过$使用系统环境变量时，会读取不到，需要在azkaban中定义。 如使用azkaban执行下面的脚本时，若不配置会为空 1234#!&#x2F;bin&#x2F;shecho $SPARK2_HOMEecho $SPARK2_LIB_HOME 原因使用$与azkaban中的取法冲突，导致 解决方法：在azkaban-web的conf中的global.properties配置上需要的环境变量，然后重启azkaban即可。 123-- global.propertiesSPARK2_HOME&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;SPARK2&#x2F;lib&#x2F;spark2SPARK2_LIB_HOME&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;SPARK2&#x2F;lib&#x2F;spark2&#x2F;jars","categories":[{"name":"azkaban","slug":"azkaban","permalink":"http://guoyanlei.top/categories/azkaban/"}],"tags":[{"name":"azkaban","slug":"azkaban","permalink":"http://guoyanlei.top/tags/azkaban/"}]},{"title":"博客搭建记录","slug":"2018042201-博客搭建记录","date":"2018-04-22T14:30:04.000Z","updated":"2018-05-25T04:42:49.545Z","comments":true,"path":"2018/04/22/2018042201-博客搭建记录/","link":"","permalink":"http://guoyanlei.top/2018/04/22/2018042201-%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/","excerpt":"以下内容只为记录下博客的搭建历程： 博客搭建在github pages上，是基于自己的github空间搭建的，不明白的可以去度娘。 域名是在阿里万网上购买的，觉得这个.tech域名很高大上就买了，希望之后可以在这里构建自己的technology栈。 博客所使用的模板是开源的Hexo，github上的好多开源的东西还是很棒的！详细的见documentation 所使用的主题的是hexo-theme-next，官网。","text":"以下内容只为记录下博客的搭建历程： 博客搭建在github pages上，是基于自己的github空间搭建的，不明白的可以去度娘。 域名是在阿里万网上购买的，觉得这个.tech域名很高大上就买了，希望之后可以在这里构建自己的technology栈。 博客所使用的模板是开源的Hexo，github上的好多开源的东西还是很棒的！详细的见documentation 所使用的主题的是hexo-theme-next，官网。 由于Hexo是基于NodeJs开发的，所以在发布自己的博文时首先需要在本机安装NodeJs。 Quick Start进入到下载好的Hexo进行安装1npm install hexo-cli -g 初始化博客123456hexo init blogcd blognpm installhexo server此时就可以在本地浏览器打开博客了，进入http://localhost:4000/ 进行相关配置在_config.xml文件中进行配置 123456title: 我的博客subtitle: xxxdescription: xxxauthor: xxxlanguage: zh-CN... 发布文章1$ hexo new &quot;hello-world&quot; 然后就会在sources/_posts生成一篇文件名为hello-world.md的markdown文件。编辑该文件就可以写博客了。这里有一些Front-matter需要介绍，可以配置文章使用的模板、所属的分类和tag等。 Front-matter 是文件最上方以 — 分隔的区域，用于指定个别文件的变量，举例来说： 12345678title: &quot;使用Hexo在Github上搭建自己的博客&quot;date: 2016-09-21 22:30:04tags:- Hexo- Githubcategories:- other--- 目前的categories只能有一个一级分类，如果填写多个，第二个会被解析为二级分类，以及类推。tags可以允许有多个。更多关于Front-matter请参考 http://hexo.io/zh-cn/docs/front-matter.html 。 关于markdown的编写见: Writing 产生静态文件1$ hexo generate More info: Generating 部署到github自动部署需要安装部署插件:安装 hexo-deployer-git 插件，使用如下命令： 1$ npm install hexo-deployer-git --save 然后就可以在blog目录执行以下命令发布博客了。 1$ hexo deploy More info: Deployment 之后再发布文章再发布文章就只需如下命令就可以了 123$ hexo clean 可省$ hexo genarate$ hexo deploy 其中更多关于hexo-theme-next主题的相关设置见documentation。 遇到的问题问题一：Local hexo not found in xx描述：Hexo搭建博客之后用git已经将所有的source都同步到了git上，在另一台电脑上将源代码clone下来之后，直接执行hexo generate，出现错误。 123E:\\github\\guoyanlei.github.io\\blog&gt;hexo generateERROR Local hexo not found in E:\\github\\guoyanlei.github.io\\blogERROR Try running: &#x27;npm install hexo --save&#x27; 原因： .gitignore文件里面忽略了node_modules文件夹，所以这个文件夹没有更新到git上去。所以要用npm重新安装到本地即可。 解决办法： 12cd blognpm install","categories":[{"name":"other","slug":"other","permalink":"http://guoyanlei.top/categories/other/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://guoyanlei.top/tags/Hexo/"},{"name":"Github","slug":"Github","permalink":"http://guoyanlei.top/tags/Github/"}]}]}