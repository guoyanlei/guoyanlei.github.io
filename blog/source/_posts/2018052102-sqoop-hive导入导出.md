

title: sqoop-hive导入导出
date: 2018/05/21 15:30:04
tags:
- sqoop
categories:
- 数据同步

---

### sqoop-hive导入导出

本文为sqoop使用的记录，包括mysql到hive的导入和hive到mysql的导出。

- 详细可参考官方手册：https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html

<!--more-->

#### mysql导入hive

```
sqoop import 
--connect jdbc:mysql://10.10.95.186:3306/ss_peacock \
--table wnl_life_card_item \
--username mysqlsiud \
--password 'mysql!@#456' \
--hive-import \
--hive-overwrite \
--hive-table prod.ods_dim_peacock_wnl_life_card_item  
-m 1 \
--fields-terminated-by '\t'
```

import主要属性说明：

```
--connect <jdbc-uri>
--connection-manager <class-name>
--driver <class-name>
--hadoop-mapred-home <dir> 不设置会找系统配的环境变量$HADOOP_MAPRED_HOME
--password-file	可执行存有密码的文件
--password <password>
--username <username>

--table <table-name>	Table to read
--target-dir <dir>	HDFS destination dir
--warehouse-dir <dir>	HDFS parent for table destination

--append                将数据追加到hdfs中已经存在的dataset中
--delete-target-dir	Delete the import target directory if it exists
--as-avrodatafile	将数据导入到一个Avro数据文件中
--as-sequencefile	将数据导入到一个sequence文件中
--as-textfile	        将数据导入到一个普通文本文件中(default)
--as-parquetfile	将数据导入到一个Parquet数据文件中
-z or --compress	        Enable compression
--compression-codec <c>	Use Hadoop codec (default gzip)

--where <where clause>	WHERE clause to use during import
--boundary-query <statement>	边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：--boundary-query 'select id,creationdate from person where id = 3'，
--columns <col,col,col…>	指定要导入的字段值，格式如：--columns id,username

--direct	直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快
-m or --num-mappers <n>	        设置并行度 n个map任务

–null-string <null-string>	可选参数，如果没有指定，则字符串"null"将被使用
–null-non-string<null-string>	可选参数，如果没有指定，则字符串"null"将被使用

--fields-terminated-by <char>	字段分隔符
--lines-terminated-by <char>	行分割符

```

增量导入属性说明：

```
--check-column (col)	用来作为判断的列名，如id，不能是(CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR)
--incremental (mode)	append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录
--last-value (value)	指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值

```

hive属性说明：

```
--hive-home <dir>	hive的安装路径，若不设会取系统变量$HIVE_HOME
--hive-import	        将数据从关系数据库中导入到hive表中
--hive-overwrite	覆盖掉在hive表中已经存在的数据
--create-hive-table	默认是false,如果目标表已经存在了，那么创建任务会失败
--hive-table	        后面接要创建的hive表

--hive-delims-replacement <arg>	用自定义的字符串替换掉数据中的\n, \r, and \01等字符
--hive-drop-import-delims	在导入数据到hive中时，去掉数据中\n,\r和\01这样的字符
--map-column-hive <arg>	        生成hive表时，可以更改生成字段的数据类型，格式如：--map-column-hiveTBL_ID=String,LAST_ACCESS_TIME=string

--hive-partition-key	        创建分区，后面直接跟分区名即可，创建完毕后，通过describe 表名可以看到分区名，默认为string型
--hive-partition-value<v>	该值是在导入数据到hive中时，与–hive-partition-key设定的key对应的value值。

```

#### hive导出到mysql

```
- mysql中的目标表为两个联合主键
sqoop export --connect jdbc:mysql://m1.mysql.leopard.dmp.com:3306/ssy_leopard \
--username mysqlsiud \
--password mysql!@#456 \
--table leopard_app_channel \
-m 1 \
--export-dir /user/hive/warehouse/prod.db/ads_hive_leopard_dim_app_channel \
--update-key app_key,channel \
--update-mode allowinsert \
--input-fields-terminated-by '\t'

```

导出主要参数说明：

```
--columns <col,col,col…>	要导出的列
--direct	                直接导入模式，更快速
--export-dir <dir>	        HDFS上要导出的数据文件
-m,--num-mappers <n>	        导出并行度n map tasks
--table <table-name>	        要导出的目标表名称
--call <stored-proc-name>	存储过程
--update-key <col-name>	        根据该列的值，判定是否需要更新或插入，通常是表的主键，或联合主键
--update-mode <mode>	        分为 updateonly (default) 和 allowinsert.

--input-null-string <null-string>	可选参数，如果没有指定，则字符串"null"将被使用
--input-null-non-string <null-string>	可选参数，如果没有指定，则字符串"null"将被使用

--staging-table <staging-table-name>	导入到目标表前的缓存表
--clear-staging-table	                指示暂存表中存在的任何数据都可以删除.
--batch	                                使用底层语句执行的批处理模式
```

#### mysql表导入hive分区表

```
# mysql的按天分表导入hive指定的分区
sqoop import --connect jdbc:mysql://xxx:3306/zhwnl_community?tinyInt1isBit=false \
--table coin_gold_account_flow_20180501 \
--username mysqlsiud \
--password 'xxx' \
--hive-import \
--hive-overwrite \
--hive-table prod.ads_hive_dmp_dim_coin_account_flow_1d \
--hive-partition-key   ds  \
--hive-partition-value  20180501  \
--fields-terminated-by '\t' -m 4

# mysql表中指定日期的数据导入到hive指定分区中
sqoop import --connect jdbc:mysql://xxx:3306/ss_peacock \
--table item_dislike \
--where "create_date = '20180501'" \
--username mysqlsiud \
--password 'xxx' \
--hive-import \
--hive-overwrite \
--hive-table prod.ods_dim_peacock_item_dislike -m 1 \
--hive-partition-key   ds  \
--hive-partition-value  20180501  \
--fields-terminated-by '\t'

```

#### hive分区表导入到mysql

```
# 从HIVE分区表导入到MySQL，需要依次导入每个分区的数据\
sqoop export   \
--connect jdbc:mysql://xxx:3306/Server74   \
--username mysqlsiud   \
--password xxx   \
--table prod.ods_dim_peacock_item_dislike \
--hive-partition-key ds \
--hive-partition-value 20180501 \
--export-dir /user/hive/warehouse/prod.ods_dim_peacock_item_dislike/ds=2017-11-15/  \
--input-fields-terminated-by '\t'   \
--input-lines-terminated-by '\n'

说明：--export-dir这个参数是必须的，指定hive表源文件路径后，sqoop回到路径下路径下的文件，文件不是路径否则报错。所以分区表需要单独指定每个分区的目录，分别导入
```


