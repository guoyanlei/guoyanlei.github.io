<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GuoYL&#39;s Notes</title>
  
  <subtitle>后端技术 | 大数据管理与分析</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://guoyanlei.top/"/>
  <updated>2019-05-09T10:57:26.113Z</updated>
  <id>http://guoyanlei.top/</id>
  
  <author>
    <name>Guo Yanlei</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>QCon2019大数据平台架构相关总结</title>
    <link href="http://guoyanlei.top/2019/05/08/2019050801-QCon2019%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/"/>
    <id>http://guoyanlei.top/2019/05/08/2019050801-QCon2019大数据平台架构相关总结/</id>
    <published>2019-05-08T07:30:00.000Z</published>
    <updated>2019-05-09T10:57:26.113Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-快手万亿级别Kafka集群应用实践与技术演进"><a href="#1-快手万亿级别Kafka集群应用实践与技术演进" class="headerlink" title="1. 快手万亿级别Kafka集群应用实践与技术演进"></a>1. 快手万亿级别Kafka集群应用实践与技术演进</h3><h4 id="应用实践"><a href="#应用实践" class="headerlink" title="应用实践"></a>应用实践</h4><p>快手对Kafka的三个重要应用场景：</p><ul><li>在线集群：在线服务消息中间件</li><li>Log集群：业务系统日志收集和传输的缓存介质，之后面向重要的实时消费和数据处理</li><li>离线集群：是所有各种日志的最终汇聚点，一方面落地到数仓；另一方面面向次重要的实时消费和数据处理</li></ul><a id="more"></a><p>业务场景架构如下：</p><p><img src="/img/qcon/kuaishou-kafka-1.png" alt="kuaishou-kafka-1"></p><p>其中，通过Mirror Service将多个在线集群和Log集群中数据汇总到离线集群</p><h4 id="技术演进"><a href="#技术演进" class="headerlink" title="技术演进"></a>技术演进</h4><p>对kafka的使用做了如下优化：</p><h5 id="优化一：kafka集群平滑扩容优化，解决了扩容节点时导致kafka集群物理资源大量消耗，影响producer写入"><a href="#优化一：kafka集群平滑扩容优化，解决了扩容节点时导致kafka集群物理资源大量消耗，影响producer写入" class="headerlink" title="优化一：kafka集群平滑扩容优化，解决了扩容节点时导致kafka集群物理资源大量消耗，影响producer写入"></a>优化一：kafka集群平滑扩容优化，解决了扩容节点时导致kafka集群物理资源大量消耗，影响producer写入</h5><ul><li>问题：社区kafka对partition的迁移是从最初的offset开始的，触发读盘，物理资源大量消耗 =&gt; produce延迟增高且 抖动；扩容不平滑</li><li>优化思路：从最新offset开始迁移，并同步一定时间，保障所有consumer都已经跟上（据说这一改进已经向社区提交了<a href="https://issues.apache.org/jira/browse/KAFKA-8328">issues</a></li></ul><h5 id="优化二：Mirror集群化建设，摒弃了Apache-Kafka的Mirrormaker，基于uReplicator做了改进"><a href="#优化二：Mirror集群化建设，摒弃了Apache-Kafka的Mirrormaker，基于uReplicator做了改进" class="headerlink" title="优化二：Mirror集群化建设，摒弃了Apache Kafka的Mirrormaker，基于uReplicator做了改进"></a>优化二：Mirror集群化建设，摒弃了Apache Kafka的Mirrormaker，基于uReplicator做了改进</h5><ul><li>问题：Apache Kafka的Mirrormaker是静态管理，运维成本高，易出错；当增加topic或集群时导致正在运行的数据Mirror整体断流</li><li>优化思路：基于uber的<a href="https://github.com/uber/uReplicator">UReplicator</a>做了改进，通过Mirror服务集群化管理，可减低运维，避免出错，支持快速调整，应对突增流量</li></ul><h5 id="优化三：资源隔离，将不同业务线的topic进行物理隔离，保证互不影响"><a href="#优化三：资源隔离，将不同业务线的topic进行物理隔离，保证互不影响" class="headerlink" title="优化三：资源隔离，将不同业务线的topic进行物理隔离，保证互不影响"></a>优化三：资源隔离，将不同业务线的topic进行物理隔离，保证互不影响</h5><ul><li>问题1：不同业务线topic缺少物理隔离，会相互影响</li><li>优化思路：根据不同的业务做到Broker级别物理隔离</li><li>问题2：Kafka Rpc队列缺少隔离，一旦某个topic处理慢，会导致所有请求hang住</li><li>优化思路：多RPC队列，进行隔离</li></ul><h5 id="优化四：对PageCache的改造，避免Consumer的lag读和Follower进行replica时可能产生的PageCache污染"><a href="#优化四：对PageCache的改造，避免Consumer的lag读和Follower进行replica时可能产生的PageCache污染" class="headerlink" title="优化四：对PageCache的改造，避免Consumer的lag读和Follower进行replica时可能产生的PageCache污染"></a>优化四：对PageCache的改造，避免Consumer的lag读和Follower进行replica时可能产生的PageCache污染</h5><ul><li>问题：Kafka高性能依赖page cache，但page cache不可控（由操作系统管理），可能会被污染</li><li>优化思路：让kafka自己维护数据cache，严格按照时间顺序cache，并控制follower的数据不进入cache</li></ul><h5 id="优化五：消费智能限速，解决了某个Consumer-lag延迟后读盘导致的producer写入受阻的问题"><a href="#优化五：消费智能限速，解决了某个Consumer-lag延迟后读盘导致的producer写入受阻的问题" class="headerlink" title="优化五：消费智能限速，解决了某个Consumer lag延迟后读盘导致的producer写入受阻的问题"></a>优化五：消费智能限速，解决了某个Consumer lag延迟后读盘导致的producer写入受阻的问题</h5><ul><li>问题：某个Consumer lag延迟后读盘导致的producer写入受阻</li><li>思路：当磁盘繁忙，针对lag的consumer进行限速控制</li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>从应用实践中了解了kafka的三个重要应用场景，并对多kafka集群的Mirror Service有了新的认识，之后可以对uReplicator进行一些调研，看是否可应用到跨kafka集群的数据同步中。</p><p>从技术演进中也就收获了一些kafka使用中可能遇到的问题和优化思路，看似思路都很简单，但是真正实施起来难度还是相当大的。</p><h3 id="2-滴滴大数据研发平台最佳实践"><a href="#2-滴滴大数据研发平台最佳实践" class="headerlink" title="2. 滴滴大数据研发平台最佳实践"></a>2. 滴滴大数据研发平台最佳实践</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>大数据研发平台设计的初衷是：设计一个平台满足所有数据开发人员的数据分析、数据加工、模型训练等工作，同时做到数据安全的使用和管理，以及统一式的开发和运维，开发人员只关心自己的业务，不需要过多的底层。</p><p>整个功能类似阿里的DataWorks。</p><p>整体架构如下：</p><p><img src="/img/qcon/didi-bigdata-platform-1.png" alt="didi-bigdata-platform-1"><br><img src="/img/qcon/didi-bigdata-platform-2.png" alt="didi-bigdata-platform-2"></p><h4 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a>主要工作</h4><h5 id="工作一：开发与生产隔离"><a href="#工作一：开发与生产隔离" class="headerlink" title="工作一：开发与生产隔离"></a>工作一：开发与生产隔离</h5><p>由于大数据开发的特殊性，经常会基于线上已有表的数据进行开发测试，这样就难免对线上的任务和数据有影响。</p><p>解决这一问题的思路就是将开发和生产进行逻辑上的隔离，开发环境里允许对线上数据进行查询，但是结果落地时，写入的是测试的库，避免对线上库和表造成污染，待真正上线时，将发布包提交到生产环境，数据才会写入线上的库。</p><p>这一点可以借鉴，但是需要平台化的管理，这块我们还是比较欠缺。</p><h5 id="工作二：统一的任务执行平台"><a href="#工作二：统一的任务执行平台" class="headerlink" title="工作二：统一的任务执行平台"></a>工作二：统一的任务执行平台</h5><p>其实也是个逻辑的东西，将底层的执行引擎透明化，用一个统一的任务执行平台进行管理，在这个平台上可以提交离线任务、实时任务、机器学习、提数、特征分析等多种任务，任务在真正执行时是根据任务的具体类型来提交到不同的处理引擎上的。</p><p>主要工作还是上层的抽象和封装。</p><p><img src="/img/qcon/didi-bigdata-platform-3.png" alt="didi-bigdata-platform-3"></p><h5 id="工作三：实时表元数据化"><a href="#工作三：实时表元数据化" class="headerlink" title="工作三：实时表元数据化"></a>工作三：实时表元数据化</h5><p>要实现开发实时就写SQL一样简单，首先要实现的就是实时表元数据化。</p><p>其实Flink SQL已经实现了这种的写SQL来完成实时数据的处理，但是在开发时需要人为的创建表的Schema。</p><p>他们做的工作就是提前将用到的各种实时表元数据化，用户在开发时只需要指定某个具体的表就可以实现实时任务的开发，不需要关心元数据是否创建。</p><h5 id="工作四：基于列进行权限管理"><a href="#工作四：基于列进行权限管理" class="headerlink" title="工作四：基于列进行权限管理"></a>工作四：基于列进行权限管理</h5><p>这部分工作还是为数据安全来考虑的，向手机号、身份证等敏感信息，不应该向所有大数据开发人员可见，如何做到列基本的权限管理呢，这是这部分工作的初衷。</p><p>我们其实已经基于Hue和Sentry有了对表级别的权限做了管控，但是在字段级别还没有好的解决思路。</p><p>他们的实现方案：</p><ul><li>划分列的安全等级，1,2,3,4</li><li>为列进行等级打标</li><li>借助自研的安全管家服务，为用户实现授权</li><li>底层基于Apache Ranger实现，安全管家会生成安全策略和授权传递给ranger</li></ul><p><img src="/img/qcon/didi-bigdata-platform-4.png" alt="didi-bigdata-platform-4"></p><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>这种平台化的东西也只有那些大厂才有能力和资源来做，也就收获了一些实现思路，具体怎么应用到我们的工作中，还需要很多路要走。</p><h3 id="3-苏宁OLAP引擎发展之路"><a href="#3-苏宁OLAP引擎发展之路" class="headerlink" title="3. 苏宁OLAP引擎发展之路"></a>3. 苏宁OLAP引擎发展之路</h3><p>没get到太多干货，讲的最多的就是对查询引擎SparkSQL的进行了一些优化。</p><p>主要包括：</p><ul><li>Spark-HDFS</li><li>Spark-Druid</li><li>ES-Hadoop</li><li>PG-Spark-JDBC</li></ul><p>优化思路就是将查询SQL谓词下推，要充分利用各存储自身的查询性能，尽量避免把数据全部拉到集群中进行计算。</p><p><img src="/img/qcon/suning-olap-1.png" alt="suning-olap-1"></p><p>其OLAP整体架构：</p><p><img src="/img/qcon/suning-olap-2.png" alt="suning-olap-2"></p><p>数据中台架构：</p><p><img src="/img/qcon/suning-olap-3.png" alt="suning-olap-3"></p><h3 id="4-ClickHouse在头条的技术演进"><a href="#4-ClickHouse在头条的技术演进" class="headerlink" title="4. ClickHouse在头条的技术演进"></a>4. ClickHouse在头条的技术演进</h3><p>主要从两个部分进行介绍，一是头条为什么选择了ClickHouse及如何使用的，二是主要做了哪些优化</p><p>由于自己对ClickHouse没做过多了解，第二部分优化没get到什么。</p><h5 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h5><p>ClickHouse 2016年开源的，由俄罗斯IT公司Yandex开发，是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)，而且查询性能优越。</p><p>主要特点：</p><ul><li>面向列+向量执行</li><li>自己管理存储（非Hadoop）</li><li>线性可扩展，高可靠（通过shard+replication实现）</li><li>面向SQL查询</li><li>快</li></ul><p>性能优越的原因：</p><ul><li>Data Skipping<ul><li>分区及分区剪枝</li><li>数据局部有序，类似LSM树的查询引擎</li></ul></li><li>资源垂直整合<ul><li>并发MPP+SMP架构</li><li>执行层是SIMD实现（单指令多数据流）</li></ul></li><li>底层C++实现</li></ul><p>使用的场景：</p><ul><li>单表分析，这个表可以很宽</li><li>分布式Join性能并不出色</li></ul><p>不足：</p><ul><li>没有事务</li><li>批量数据接收</li><li>update or delete支持较弱</li><li>查询重写优化较弱</li></ul><h5 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h5><p>头条选择ClickHouse的原因：</p><ul><li>产品需求<ul><li>交互式分析能力（in seconds）</li><li>查询模式多变</li><li>以大宽表为主</li><li>数据量大 </li></ul></li><li>开源MPP OLAP引擎 - （性能、特点、优质）</li></ul><p>在头条的应用：</p><p><img src="/img/qcon/toutiao-clickhouse-1.png" alt="toutiao-clickhouse-1"></p><h5 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h5><p>目前ClickHouse商业应用还较少，只有部分大厂有使用，而且他们在使用过程中如果遇到一些问题，自己有能力和资源进行二研优化；更主要的是ClickHouse不理睬Hadoop生态，走自己的路。我们暂且了解观望。</p><h3 id="5-美团点评常态、异地、多机房、单集群Hadoop架构实践"><a href="#5-美团点评常态、异地、多机房、单集群Hadoop架构实践" class="headerlink" title="5. 美团点评常态、异地、多机房、单集群Hadoop架构实践"></a>5. 美团点评常态、异地、多机房、单集群Hadoop架构实践</h3><p>原生Hadoop在跨机房，跨地域进行搭建时，网络的延迟会大大影响hadoop的管理和任务处理性能。</p><p>但是有时候随着业务发展，集群节点越来越多，同地域的机房没办法承载，但是又不想搭建多个hadoop集群，此时就需要搭建这种跨机房、跨地域的hadoop集群了。</p><p>美团通过二研实现了单集群异地多机房Hadoop服务，并且保证了管理和任务处理的性能，实现了架构前向兼容，机房对业务透明。</p><p>整体思路，通过优化Hadoop，使得其有对地区的感知，尽量避免跨机房流量</p><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><h5 id="第一步：多机房Hadoop资源管理"><a href="#第一步：多机房Hadoop资源管理" class="headerlink" title="第一步：多机房Hadoop资源管理"></a>第一步：多机房Hadoop资源管理</h5><ol><li>多机房存储资源管理</li></ol><ul><li>NameNode机房感知中增加了对地区的感知</li><li>NameNode副本分布属性增加了地区的支持</li><li>HDFS读写响应：保证吞吐，避免跨机房流量（仅向默认机房写⼊，就近读取）</li><li>具备初级多机房存储资源管理理能⼒</li></ul><ol start="2"><li>多机房计算调度</li></ol><ul><li>基于Label Scheduler的多机房计算资源调度，即在提交作业时附加上机房的标签，禁⽌止跨机房作业调度</li><li>基于YARN Federation的跨地域计算资源调度，优先请求本地机房</li></ul><h5 id="第二步：多机房Hadoop资源管理"><a href="#第二步：多机房Hadoop资源管理" class="headerlink" title="第二步：多机房Hadoop资源管理"></a>第二步：多机房Hadoop资源管理</h5><ol><li>跨机房数据Cache处理，就是基于数据⾎血缘产⽣和读写规律进行构建副本cache的规则（在本地机房冗余一些数据，减少重复跨机房的数据读取）</li><li>带宽管控，充分利用好带宽</li></ol><h5 id="第三步：HDFS机房容错"><a href="#第三步：HDFS机房容错" class="headerlink" title="第三步：HDFS机房容错"></a>第三步：HDFS机房容错</h5><ol><li>HDFS分区容忍设计粒度为节点级别，而不在是块级别</li><li>实现机房、机架粒度容错，保证网络故障时，DataNode没有故障，数据没有丢失</li></ol><h4 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h4><p>收获了一些解决这样问题的思路，具体是否值得实现还得看业务。<br>技术换运营，站在平台运营视⻆角进行架构设计，才能保证架构平稳落地。</p><h3 id="6-阿里巴巴新一代交互式分析引擎-Hologres"><a href="#6-阿里巴巴新一代交互式分析引擎-Hologres" class="headerlink" title="6. 阿里巴巴新一代交互式分析引擎-Hologres"></a>6. 阿里巴巴新一代交互式分析引擎-Hologres</h3><p>主要介绍了Hologres是啥，其设计理念是啥，性能有多么优越。。。</p><p>Hologres是啥？</p><ul><li>新一代海量数据交互式分析引擎</li><li>一套引擎支持Point Query(hbase场景)，Ad-hoc Query(Druid场景)，<br>OLAP Query(Impala场景) </li><li>快。。</li><li>存储计算分离</li><li>支持实时数据与批量数据导入</li><li>支持External Storage，与阿里云大数据产品无缝对接</li></ul><p>亮点&amp;理念？</p><ul><li>统一的引擎架构，保证数据的一致性<ul><li>解决数据在Hbase存一份、Druid存一份、xxx存一份造成的浪费和数据不一致</li></ul></li><li>存储和计算分离<ul><li>据说新的NVME SSD盘可以达到150000IOPS，磁盘IO不再是性能瓶颈，问题转变为如何把CPU高效利用起来</li><li>存储计算分离是未来大势所趋，存储和计算非对齐采购，成本更低，部署运维更方便</li></ul></li><li>更加聪明的Optimizer</li><li>使用新技术<ul><li>近几年硬件性能提升的很快，N年前的技术方案不一定能够很好的利用现在的硬件性能发挥到极致</li><li>使用全异步架构，把CPU利用到极致</li></ul></li><li>向量化计算</li></ul><p>具体架构就略了，目前还在开发迭代中，后期在看。</p><p>整体get到的点可能就是：在未来的某一天磁盘IO不再是性能瓶颈，它可能比CPU处理更快，到时候现在已有的操作系统可能就需要重新设计。</p><p>注：</p><p>对相关内容感兴趣的可以下载<a href="https://ppt.geekbang.org/list/qconbj2019?from=groupmessage&amp;isappinstalled=0">各分享PPT</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-快手万亿级别Kafka集群应用实践与技术演进&quot;&gt;&lt;a href=&quot;#1-快手万亿级别Kafka集群应用实践与技术演进&quot; class=&quot;headerlink&quot; title=&quot;1. 快手万亿级别Kafka集群应用实践与技术演进&quot;&gt;&lt;/a&gt;1. 快手万亿级别Kafka集群应用实践与技术演进&lt;/h3&gt;&lt;h4 id=&quot;应用实践&quot;&gt;&lt;a href=&quot;#应用实践&quot; class=&quot;headerlink&quot; title=&quot;应用实践&quot;&gt;&lt;/a&gt;应用实践&lt;/h4&gt;&lt;p&gt;快手对Kafka的三个重要应用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在线集群：在线服务消息中间件&lt;/li&gt;
&lt;li&gt;Log集群：业务系统日志收集和传输的缓存介质，之后面向重要的实时消费和数据处理&lt;/li&gt;
&lt;li&gt;离线集群：是所有各种日志的最终汇聚点，一方面落地到数仓；另一方面面向次重要的实时消费和数据处理&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据平台架构" scheme="http://guoyanlei.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="kafka" scheme="http://guoyanlei.top/tags/kafka/"/>
    
      <category term="架构" scheme="http://guoyanlei.top/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>基于Sentry和Hue对数仓表做权限管理</title>
    <link href="http://guoyanlei.top/2019/03/20/2019032001-%E5%9F%BA%E4%BA%8ESentry%E5%92%8CHue%E5%AF%B9%E6%95%B0%E4%BB%93%E8%A1%A8%E5%81%9A%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"/>
    <id>http://guoyanlei.top/2019/03/20/2019032001-基于Sentry和Hue对数仓表做权限管理/</id>
    <published>2019-03-20T07:30:00.000Z</published>
    <updated>2019-03-20T03:58:07.213Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Hue简介"><a href="#Hue简介" class="headerlink" title="Hue简介"></a>Hue简介</h3><h4 id="Hue是什么"><a href="#Hue是什么" class="headerlink" title="Hue是什么"></a>Hue是什么</h4><p>Hue是一个可快速开发和调试Hadoop生态系统各种应用的一个基于浏览器的图形化用户接口。 </p><p>Hue是出自CDH公司，在基于CDH的大数据集群中安装和使用非常方便。</p><a id="more"></a><h4 id="Hue能干什么"><a href="#Hue能干什么" class="headerlink" title="Hue能干什么"></a>Hue能干什么</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1.访问HDFS和文件浏览 </span><br><span class="line">2.通过web调试和开发hive以及数据结果展示 </span><br><span class="line">3.查询solr和结果展示，报表生成 </span><br><span class="line">4.通过web调试和开发impala交互式SQL Query </span><br><span class="line">5.spark调试和开发 </span><br><span class="line">6.Pig开发和调试 </span><br><span class="line">7.oozie任务的开发，监控，和工作流协调调度 </span><br><span class="line">8.Hbase数据查询和修改，数据展示 </span><br><span class="line">9.Hive的元数据（metastore）查询 </span><br><span class="line">10.MapReduce任务进度查看，日志追踪 </span><br><span class="line">11.创建和提交MapReduce，Streaming，Java job任务 </span><br><span class="line">12.Sqoop2的开发和调试 </span><br><span class="line">13.Zookeeper的浏览和编辑 </span><br><span class="line">14.数据库（MySQL，PostGres，SQlite，Oracle）的查询和展示 </span><br></pre></td></tr></table></figure><p>其架构图：</p><p><img src="/img/hue-sentry/hue-arch.png" alt="hue-arch"></p><h4 id="Hue有哪些不足"><a href="#Hue有哪些不足" class="headerlink" title="Hue有哪些不足"></a>Hue有哪些不足</h4><ul><li>虽然提供了角色权限管理，但是仅是针对Hue各功能的权限控制</li><li>没有办法从库级别、表级别对用户做权限管理</li></ul><h3 id="Sentry简介"><a href="#Sentry简介" class="headerlink" title="Sentry简介"></a>Sentry简介</h3><h4 id="Sentry是什么"><a href="#Sentry是什么" class="headerlink" title="Sentry是什么"></a>Sentry是什么</h4><p>Apache Sentry是一个适用于Hadoop，基于角色粒度的授权模块，旨在成为Hadoop组件的可插拔授权引擎。它允许定义授权规则以验证用户或应用程序对Hadoop资源的访问请求。目前可以与Apache Hive，Apache Solr，Impala和HDFS（仅限于Hive表数据）一起开箱即用。</p><p>Sentry同样是出自CDH公司，在基于CDH的大数据集群中安装和使用非常方便。</p><p><a href="https://cwiki.apache.org/confluence/display/SENTRY/Sentry+Tutorial">官网doc</a></p><p>其架构图：</p><p><img src="/img/hue-sentry/sentry-arch.png" alt="sentry-arch"></p><p>主要分为三个模块：</p><ul><li>Sentry Server：管理授权元数据。它支持安全检索和操作元数据的界面;</li><li>Data Engine：数据处理引擎（如Hive或Impala）。数据引擎执行前会加载Sentry插件，拦截所有访问资源的请求并将其路由到Sentry插件进行验证;</li><li>Sentry plugin：运行在数据引擎中，验证用户是否有权限。</li></ul><h4 id="Sentry能干什么"><a href="#Sentry能干什么" class="headerlink" title="Sentry能干什么"></a>Sentry能干什么</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- 对hive库、表、表字段做权限控制</span><br><span class="line">- 对impala库、表、表字段做权限控制，与hive的区别是可以缓存权限（在Catalog server和Impalad都有），授权验证在本地发生并且速度会更快</span><br><span class="line">- 对hdfs目录、文件做权限控制，更关注的是hive数仓的数据（任何属于Hive或Impala中表的数据）</span><br><span class="line">- 权限可分为三级：</span><br><span class="line"> - SELECT privilege -&gt; Read access on the file.</span><br><span class="line"> - INSERT privilege -&gt; Write access on the file.</span><br><span class="line"> - ALL privilege -&gt; Read and Write access on the file.</span><br></pre></td></tr></table></figure><h3 id="基于Sentry和Hue对数仓表做权限管理"><a href="#基于Sentry和Hue对数仓表做权限管理" class="headerlink" title="基于Sentry和Hue对数仓表做权限管理"></a>基于Sentry和Hue对数仓表做权限管理</h3><h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><p>基于Cloudera-manager可以直接安装Hue和Sentry，安装Hue前需要先安装Oozie。</p><p>安装Hue和Sentry都需要先创建mysql库：hue和sentry，并在安装时制定库和库地址。</p><ol><li>Hue安装遇到的问题</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 测试mysql链接时缺少jar包，找不到jdbc driver</span><br><span class="line">解决方法：需要将这个包放到这个路径下</span><br><span class="line">&#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java.jar</span><br><span class="line"></span><br><span class="line"># 安装后启动进程失败</span><br><span class="line">原因：hue所在的服务器环境没有预先安装httpd，mod_ssl服务</span><br><span class="line">解决方法：</span><br><span class="line">yum -y install httpd</span><br><span class="line">yum -y install mod_ssl</span><br></pre></td></tr></table></figure><h4 id="权限配置"><a href="#权限配置" class="headerlink" title="权限配置"></a>权限配置</h4><h5 id="修改Hive的Sentry配置"><a href="#修改Hive的Sentry配置" class="headerlink" title="修改Hive的Sentry配置"></a>修改Hive的Sentry配置</h5><ol><li>在hive配置中启用sentry</li><li>禁用“HiveServer2 启用模拟：hive.server2.enable.impersonation”</li><li>配置“sentry-site.xml 的 Hive 服务高级配置代码段（安全阀）”</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;sentry.hive.testing.mode&lt;&#x2F;name&gt;</span><br><span class="line">   &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h5 id="修改Impala的Sentry配置"><a href="#修改Impala的Sentry配置" class="headerlink" title="修改Impala的Sentry配置"></a>修改Impala的Sentry配置</h5><ol><li>在Impala配置中启用sentry</li><li>重启Impala即可</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：若启用了sentry，那么在Impala-jdbc调用时许指定用户。如</span><br><span class="line">原来的url：jdbc:impala:&#x2F;&#x2F;node1.lb.bigdata.dmp.com:21050&#x2F;prod;AuthMech&#x3D;0</span><br><span class="line">指定用户后url：jdbc:impala:&#x2F;&#x2F;node1.lb.bigdata.dmp.com:21050&#x2F;prod;AuthMech&#x3D;3;UID&#x3D;impala;PWD&#x3D;;UseSasl&#x3D;0</span><br></pre></td></tr></table></figure><h4 id="修改Hue的Sentry配置"><a href="#修改Hue的Sentry配置" class="headerlink" title="修改Hue的Sentry配置"></a>修改Hue的Sentry配置</h4><ol><li>在Hue配置中启用sentry</li><li>重启Hue即可</li></ol><h5 id="hive角色、用户组和Hue的区别"><a href="#hive角色、用户组和Hue的区别" class="headerlink" title="hive角色、用户组和Hue的区别"></a>hive角色、用户组和Hue的区别</h5><ul><li>hive role：控制表、库的select、insert权限（通过sentry可以管理）</li><li>hive group：通常被指定拥有哪些 hive role </li><li>hue group：通过操作系统上的用户组和Hive的用户组进行关联。因此，配置完Hue用户组后还需配置OS上的用户组（需要配置所有hiveserver3，hue，sentry所在的机器) </li><li>hue username：需要和hue group保持相同</li></ul><h5 id="权限配置-1"><a href="#权限配置-1" class="headerlink" title="权限配置"></a>权限配置</h5><ol><li>使用admin用户登录Hue，创建group：hive（赋予所有权限），username：hive</li><li>使用hive用户登录，即可在security菜单中管理hive roles了</li><li>下面是总结创建一个用户的过程</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1）在hue中创建group（页面操作，admin用户登录）</span><br><span class="line">2）在hue中创建用户（页面操作，admin用户登录），并指定group：</span><br><span class="line">3）所有hiveserver3，hue，sentry所在的机器上创建group（命令行操作）</span><br><span class="line">- useradd group_name</span><br><span class="line"></span><br><span class="line">4）在hue中为hive创建role（可在impala或hive的Edit页面操作，hive用户登录）</span><br><span class="line">- create role role_name</span><br><span class="line"></span><br><span class="line">5）在hue中为某个group授予角色（可在impala或hive的Edit页面操作，hive用户登录）</span><br><span class="line">- grant role role_name to group group_name;</span><br><span class="line"></span><br><span class="line">6）在hue中为创建的role指定库、表等权限（在sentry页面操作，hive用户登录）</span><br><span class="line">7）使用第二步hue创建的用户登录，然后刷新，即可看到指定权限的库、表</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><h5 id="hiveSQL执行add-jar失败"><a href="#hiveSQL执行add-jar失败" class="headerlink" title="hiveSQL执行add jar失败"></a>hiveSQL执行add jar失败</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive配置了sentry后执行下面语句失败</span><br><span class="line">add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;phoenix-4.13.2-cdh5.11.2-hive.jar;</span><br><span class="line"></span><br><span class="line">原因：</span><br><span class="line">The ADD JAR command does not work with HiveServer2 and the Beeline client when Beeline runs on a different host. </span><br></pre></td></tr></table></figure><p>解决办法：</p><p><a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cm_mc_hive_udf.html">cloudera官网</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">首先，只能借助hive的辅助jar来实现</span><br><span class="line">在hive配置中修改（配置项：Hive 辅助 JAR 目录）：</span><br><span class="line">hive.aux.jars.path&#x3D;&#x2F;data&#x2F;dmp&#x2F;udf</span><br><span class="line"></span><br><span class="line">hive-env.sh中添加（配置项：hive-env.sh 的 Gateway 客户端环境高级配置代码段（安全阀））：</span><br><span class="line">HIVE_AUX_JARS_PATH&#x3D;&#x2F;data&#x2F;dmp&#x2F;udf</span><br><span class="line"></span><br><span class="line">hive-site.xml中添加（配置项hive-site.xml 的 Hive 服务高级配置代码段（安全阀））：</span><br><span class="line">hive.reloadable.aux.jars.path&#x3D;&#x2F;data&#x2F;dmp&#x2F;udf</span><br><span class="line"></span><br><span class="line">然后将用的jar放的hiveserver2所在机器的&#x2F;data&#x2F;dmp&#x2F;udf目录下</span><br><span class="line"></span><br><span class="line">之后若有新jar需要加入，直接将jar放到目录下，然后在HiveSQL中执行reload命令即可。</span><br><span class="line"></span><br><span class="line">最后在执行SQL时就不需要add jar操作</span><br></pre></td></tr></table></figure><h5 id="hiveSQL执行insert-overwrite操作失败"><a href="#hiveSQL执行insert-overwrite操作失败" class="headerlink" title="hiveSQL执行insert overwrite操作失败"></a>hiveSQL执行insert overwrite操作失败</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ERROR : Failed with exception Directory hdfs:&#x2F;&#x2F;nameservice&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ods_hive_recommend_item_topic_nd could not be cleaned up.</span><br><span class="line">19-03-2019 14:11:17 CST ods_hive_recommend_item_topic_nd </span><br><span class="line">Directory hdfs:&#x2F;&#x2F;nameservice&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ods_hive_recommend_item_topic_nd could not be cleaned up.</span><br><span class="line"></span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">Caused by: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: Permission denied by sticky bit: </span><br><span class="line">user&#x3D;hive, path&#x3D;&quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ods_hive_recommend_item_topic_nd&#x2F;000000_0&quot;:impala:hive:-rwxrwxrwt, parent&#x3D;&quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;prod.db&#x2F;ods_hive_recommend_item_topic_nd&quot;:impala:hive:drwxrwxrwt</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>发现如下信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Permission denied by sticky bit: user&#x3D;hive,</span><br><span class="line"></span><br><span class="line">由于sticky bit导致的错误:</span><br><span class="line"># sticky bit</span><br><span class="line">不同于suid, guid，对于others的execute权限位，则可以设置sticky bit标志，</span><br><span class="line">用t来表示，如果该位置本来就有可执行权限位，即x，则t和x叠加后用大写的T来表示。</span><br><span class="line"></span><br><span class="line">sticky bit只对目录起作用，如果一个目录设置了sticky bit，则该目录下的文件只能被</span><br><span class="line">该文件的owner或者root删除，其他用户即使有删除权限也无法删除该文件。</span><br><span class="line"></span><br><span class="line">-rwxrwxrwt   3 hdfs hive        636 2018-11-06 10:19 &#x2F;user&#x2F;hive&#x2F;xxxx&#x2F;000000_0</span><br><span class="line">-rwxrwxrwt   3 hdfs hive        635 2018-11-06 10:19 &#x2F;user&#x2F;hive&#x2F;xxxx&#x2F;000001_0</span><br></pre></td></tr></table></figure><p>解决办法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">将sticky bit配置给移除:</span><br><span class="line"></span><br><span class="line">hadoop fs -chmod -R -t &#x2F;user&#x2F;hive&#x2F;xxxx&#x2F;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Hue简介&quot;&gt;&lt;a href=&quot;#Hue简介&quot; class=&quot;headerlink&quot; title=&quot;Hue简介&quot;&gt;&lt;/a&gt;Hue简介&lt;/h3&gt;&lt;h4 id=&quot;Hue是什么&quot;&gt;&lt;a href=&quot;#Hue是什么&quot; class=&quot;headerlink&quot; title=&quot;Hue是什么&quot;&gt;&lt;/a&gt;Hue是什么&lt;/h4&gt;&lt;p&gt;Hue是一个可快速开发和调试Hadoop生态系统各种应用的一个基于浏览器的图形化用户接口。 &lt;/p&gt;
&lt;p&gt;Hue是出自CDH公司，在基于CDH的大数据集群中安装和使用非常方便。&lt;/p&gt;
    
    </summary>
    
      <category term="hue" scheme="http://guoyanlei.top/categories/hue/"/>
    
    
      <category term="hue" scheme="http://guoyanlei.top/tags/hue/"/>
    
      <category term="sentry" scheme="http://guoyanlei.top/tags/sentry/"/>
    
  </entry>
  
  <entry>
    <title>HDFS和Yarn同时重启对Flink on Yarn任务的影响</title>
    <link href="http://guoyanlei.top/2019/02/14/2019021401-HDFS%E5%92%8CYarn%E5%90%8C%E6%97%B6%E9%87%8D%E5%90%AF%E5%AF%B9Flink%20on%20Yarn%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BD%B1%E5%93%8D/"/>
    <id>http://guoyanlei.top/2019/02/14/2019021401-HDFS和Yarn同时重启对Flink on Yarn任务的影响/</id>
    <published>2019-02-14T07:30:00.000Z</published>
    <updated>2019-02-14T10:09:04.927Z</updated>
    
    <content type="html"><![CDATA[<h3 id="HDFS和Yarn同时重启对Flink-on-Yarn任务的影响"><a href="#HDFS和Yarn同时重启对Flink-on-Yarn任务的影响" class="headerlink" title="HDFS和Yarn同时重启对Flink on Yarn任务的影响"></a>HDFS和Yarn同时重启对Flink on Yarn任务的影响</h3><h4 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h4><p>部分consumer的topic partition出现从Earlist开始消费的问题</p><a id="more"></a><h4 id="官网上"><a href="#官网上" class="headerlink" title="官网上"></a>官网上</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. If offsets could not be found for a partition, the auto.offset.reset setting in the properties will be used.</span><br><span class="line"></span><br><span class="line">2. Flink Kafka Consumer Offset提交行为配置:</span><br><span class="line">Flink Kafka Consumer允许配置offset提交回Kafka brokers(Kafka 0.8是写回Zookeeper)的行为，注意Flink Kafka Consumer 并不依赖于这个提交的offset来进行容错性保证，这个提交的offset仅仅作为监控consumer处理进度的一种手段。</span><br><span class="line"></span><br><span class="line">配置offset提交行为的方式有多种，主要取决于Job的checkpoint机制是否启动。</span><br><span class="line">　　1）checkpoint禁用:如果checkpoint禁用，Flink Kafka Consumer依赖于Kafka 客户端内部的自动周期性offset提交能力。因此，为了启用或者禁用offset提交，仅需在给定的Properties配置中设置enable.auto.commit(Kafka 0.8是auto.commit.enable)&#x2F;auto.commit.interval.ms为适当的值即可。</span><br><span class="line">　　2）checkpoint启用:如果checkpoint启用，当checkpoint完成之后，Flink Kafka Consumer将会提交offset保存到checkpoint State中，这就保证了kafka broker中的committed offset与 checkpoint stata中的offset相一致。用户可以在Consumer中调用setCommitOffsetsOnCheckpoints(boolean) 方法来选择启用或者禁用offset committing(默认情况下是启用的)。注意，在这种情况下，配置在Properties中的自动周期性offset提交将会被完全忽略。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>flink消费kafka的topic，为保证容错性，对offset的管理是通常是基于checkpoint机制的。</p><p>关于checkpoint存储offset机制可以参考<a href="https://www.ververica.com/blog/how-apache-flink-manages-kafka-consumer-offsets">这篇文章</a>，<a href="http://wuchong.me/blog/2018/11/04/how-apache-flink-manages-kafka-consumer-offsets/">中文可参考</a></p><p>而checkpoint状态保存在HDFS上，当HDFS重启时，checkpoint状态存在保存失败的问题，当yarn重启后，yarn会自动将flink任务重启，重启时从checkpoint开始恢复，但是存在故障的checkpoint，导致上述问题（If offsets could not be found for a partition, the auto.offset.reset setting in the properties will be used）。</p><h4 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h4><p>flink-connector-kafka目前已有kafka 0.8、0.9、0.10、0.11四个版本的实现，本文分析的是FlinkKafkaConsumer011版本代码。</p><p>FlinkKafkaConsumer011类的父类继承关系如下，FlinkKafkaConsumerBase包含了大多数实现。</p><p>FlinkKafkaConsumer011<T> extends FlinkKafkaConsumer010<T> extends FlinkKafkaConsumer09<T> extends FlinkKafkaConsumerBase<T></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span></span></span><br><span class="line"><span class="class"><span class="title">CheckpointListener</span>,</span></span><br><span class="line"><span class="class"><span class="title">ResultTypeQueryable</span>&lt;<span class="title">T</span>&gt;,</span></span><br><span class="line"><span class="class"><span class="title">CheckpointedFunction</span> </span>&#123;</span><br></pre></td></tr></table></figure><p>FlinkKafkaConsumerBase的内部实现分析：</p><ol><li>initializeState方法会在flinkkafkaconusmer初始化的时候最先调用</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">方法通过运行时上下文FunctionSnapshotContext调用getOperatorStateStore和getSerializableListState拿到了checkpoint里面的state对象</span><br><span class="line">如果这个task是从失败等过程中恢复的，context.isRestored()会被判定为<span class="keyword">true</span></span><br><span class="line">程序会试图从flink checkpoint里获取原来分配到的kafka partition以及最后提交完成的offset。</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line"></span><br><span class="line">ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; oldRoundRobinListState =</span><br><span class="line">stateStore.getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.unionOffsetStates = stateStore.getUnionListState(<span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">OFFSETS_STATE_NAME,</span><br><span class="line">TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt;() &#123;&#125;)));</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (context.isRestored() &amp;&amp; !restoredFromOldState) &#123;</span><br><span class="line">restoredState = <span class="keyword">new</span> TreeMap&lt;&gt;(<span class="keyword">new</span> KafkaTopicPartition.Comparator());</span><br><span class="line"></span><br><span class="line"><span class="comment">// migrate from 1.2 state, if there is any</span></span><br><span class="line"><span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : oldRoundRobinListState.get()) &#123;</span><br><span class="line">restoredFromOldState = <span class="keyword">true</span>;</span><br><span class="line">unionOffsetStates.add(kafkaOffset);</span><br><span class="line">&#125;</span><br><span class="line">oldRoundRobinListState.clear();</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (restoredFromOldState &amp;&amp; discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line"><span class="string">&quot;Topic / partition discovery cannot be enabled if the job is restored from a savepoint from Flink 1.2.x.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// populate actual holder for restored state</span></span><br><span class="line"><span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : unionOffsetStates.get()) &#123;</span><br><span class="line">restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">LOG.info(<span class="string">&quot;Setting restore state in the FlinkKafkaConsumer: &#123;&#125;&quot;</span>, restoredState);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">LOG.info(<span class="string">&quot;No restore state for FlinkKafkaConsumer.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="2"><li>open方法会在initializeState技术后调用，主要逻辑分为几个步骤</li></ol><ul><li>判断offsetCommitMode。根据kafka的auto commit ，setCommitOffsetsOnCheckpoints()的值（默认为true）以及flink运行时有没有开启checkpoint三个参数的组合，offsetCommitMode共有三种模式：<ul><li>ON_CHECKPOINTS  checkpoint结束后提交offset；</li><li>KAFKA_PERIODIC kafkaconsumer自带的定期提交功能；</li><li>DISABLED 不提交</li></ul></li><li>创建分区发现者</li><li>判断是否从checkpoint状态恢复，若是，则从状态中读取各partition的offset；若否，则根据启动模式来设定offset<ul><li>SPECIFIC_OFFSETS 和 TIMESTAMP 两个模式直接设置好</li><li>其他的模式（EARLIEST, LATEST 和 GROUP_OFFSETS），会在后面真正读partition数据时设置</li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// determine the offset commit mode（判断offsetCommitMode）</span></span><br><span class="line"><span class="keyword">this</span>.offsetCommitMode = OffsetCommitModes.fromConfiguration(</span><br><span class="line">getIsAutoCommitEnabled(),</span><br><span class="line">enableCommitOnCheckpoints,</span><br><span class="line">((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled());</span><br><span class="line"></span><br><span class="line"><span class="comment">// create the partition discoverer（创建分区发现者）</span></span><br><span class="line"><span class="keyword">this</span>.partitionDiscoverer = createPartitionDiscoverer(</span><br><span class="line">topicsDescriptor,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">getRuntimeContext().getNumberOfParallelSubtasks());</span><br><span class="line"><span class="keyword">this</span>.partitionDiscoverer.open();</span><br><span class="line"></span><br><span class="line">subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">List&lt;KafkaTopicPartition&gt; allPartitions = partitionDiscoverer.discoverPartitions();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//判断是否从checkpoint状态恢复</span></span><br><span class="line"><span class="keyword">if</span> (restoredState != <span class="keyword">null</span>) &#123; <span class="comment">// 是</span></span><br><span class="line"><span class="keyword">for</span> (KafkaTopicPartition partition : allPartitions) &#123; <span class="comment">// 从状态中恢复</span></span><br><span class="line"><span class="keyword">if</span> (!restoredState.containsKey(partition)) &#123; <span class="comment">//状态中没有当前分区，则从ERALIST开始消费</span></span><br><span class="line">restoredState.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; restoredStateEntry : restoredState.entrySet()) &#123;</span><br><span class="line"><span class="keyword">if</span> (!restoredFromOldState) &#123;</span><br><span class="line"><span class="comment">// seed the partition discoverer with the union state while filtering out</span></span><br><span class="line"><span class="comment">// restored partitions that should not be subscribed by this subtask</span></span><br><span class="line"><span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(</span><br><span class="line">restoredStateEntry.getKey(), getRuntimeContext().getNumberOfParallelSubtasks())</span><br><span class="line">== getRuntimeContext().getIndexOfThisSubtask())&#123;</span><br><span class="line">subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue());</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// when restoring from older 1.1 / 1.2 state, the restored state would not be the union state;</span></span><br><span class="line"><span class="comment">// in this case, just use the restored state as the subscribed partitions</span></span><br><span class="line">subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">LOG.info(<span class="string">&quot;Consumer subtask &#123;&#125; will start reading &#123;&#125; partitions with offsets in restored state: &#123;&#125;&quot;</span>,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask(), subscribedPartitionsToStartOffsets.size(), subscribedPartitionsToStartOffsets);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;  <span class="comment">// 否</span></span><br><span class="line"><span class="comment">// use the partition discoverer to fetch the initial seed partitions,</span></span><br><span class="line"><span class="comment">// and set their initial offsets depending on the startup mode.</span></span><br><span class="line"><span class="comment">// for SPECIFIC_OFFSETS and TIMESTAMP modes, we set the specific offsets now;</span></span><br><span class="line"><span class="comment">// for other modes (EARLIEST, LATEST, and GROUP_OFFSETS), the offset is lazily determined</span></span><br><span class="line"><span class="comment">// when the partition is actually read.</span></span><br><span class="line"><span class="keyword">switch</span> (startupMode) &#123;  <span class="comment">//启动模式</span></span><br><span class="line"><span class="keyword">case</span> SPECIFIC_OFFSETS: <span class="comment">//指定offset开始消费</span></span><br><span class="line"><span class="keyword">if</span> (specificStartupOffsets == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(</span><br><span class="line"><span class="string">&quot;Startup mode for the consumer set to &quot;</span> + StartupMode.SPECIFIC_OFFSETS +</span><br><span class="line"><span class="string">&quot;, but no specific offsets were specified.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (KafkaTopicPartition seedPartition : allPartitions) &#123;</span><br><span class="line">Long specificOffset = specificStartupOffsets.get(seedPartition);</span><br><span class="line"><span class="keyword">if</span> (specificOffset != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// since the specified offsets represent the next record to read, we subtract</span></span><br><span class="line"><span class="comment">// it by one so that the initial state of the consumer will be correct</span></span><br><span class="line">subscribedPartitionsToStartOffsets.put(seedPartition, specificOffset - <span class="number">1</span>);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// default to group offset behaviour if the user-provided specific offsets</span></span><br><span class="line"><span class="comment">// do not contain a value for this partition</span></span><br><span class="line">subscribedPartitionsToStartOffsets.put(seedPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> TIMESTAMP: <span class="comment">//指定produce时间戳开始消费</span></span><br><span class="line"><span class="keyword">if</span> (startupOffsetsTimestamp == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(</span><br><span class="line"><span class="string">&quot;Startup mode for the consumer set to &quot;</span> + StartupMode.TIMESTAMP +</span><br><span class="line"><span class="string">&quot;, but no startup timestamp was specified.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; partitionToOffset</span><br><span class="line">: fetchOffsetsWithTimestamp(allPartitions, startupOffsetsTimestamp).entrySet()) &#123;</span><br><span class="line">subscribedPartitionsToStartOffsets.put(</span><br><span class="line">partitionToOffset.getKey(),</span><br><span class="line">(partitionToOffset.getValue() == <span class="keyword">null</span>)</span><br><span class="line"><span class="comment">// if an offset cannot be retrieved for a partition with the given timestamp,</span></span><br><span class="line"><span class="comment">// we default to using the latest offset for the partition</span></span><br><span class="line">? KafkaTopicPartitionStateSentinel.LATEST_OFFSET</span><br><span class="line"><span class="comment">// since the specified offsets represent the next record to read, we subtract</span></span><br><span class="line"><span class="comment">// it by one so that the initial state of the consumer will be correct</span></span><br><span class="line">: partitionToOffset.getValue() - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="keyword">for</span> (KafkaTopicPartition seedPartition : allPartitions) &#123;</span><br><span class="line">subscribedPartitionsToStartOffsets.put(seedPartition, startupMode.getStateSentinel());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!subscribedPartitionsToStartOffsets.isEmpty()) &#123;</span><br><span class="line"><span class="keyword">switch</span> (startupMode) &#123;</span><br><span class="line"><span class="keyword">case</span> EARLIEST:</span><br><span class="line">LOG.info(<span class="string">&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the earliest offsets: &#123;&#125;&quot;</span>,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> LATEST:</span><br><span class="line">LOG.info(<span class="string">&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the latest offsets: &#123;&#125;&quot;</span>,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> TIMESTAMP:</span><br><span class="line">LOG.info(<span class="string">&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from timestamp &#123;&#125;: &#123;&#125;&quot;</span>,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">startupOffsetsTimestamp,</span><br><span class="line">subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> SPECIFIC_OFFSETS:</span><br><span class="line">LOG.info(<span class="string">&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the specified startup offsets &#123;&#125;: &#123;&#125;&quot;</span>,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">specificStartupOffsets,</span><br><span class="line">subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line"></span><br><span class="line">List&lt;KafkaTopicPartition&gt; partitionsDefaultedToGroupOffsets = <span class="keyword">new</span> ArrayList&lt;&gt;(subscribedPartitionsToStartOffsets.size());</span><br><span class="line"><span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line"><span class="keyword">if</span> (subscribedPartition.getValue() == KafkaTopicPartitionStateSentinel.GROUP_OFFSET) &#123;</span><br><span class="line">partitionsDefaultedToGroupOffsets.add(subscribedPartition.getKey());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (partitionsDefaultedToGroupOffsets.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">LOG.warn(<span class="string">&quot;Consumer subtask &#123;&#125; cannot find offsets for the following &#123;&#125; partitions in the specified startup offsets: &#123;&#125;&quot;</span> +</span><br><span class="line"><span class="string">&quot;; their startup offsets will be defaulted to their committed group offsets in Kafka.&quot;</span>,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">partitionsDefaultedToGroupOffsets.size(),</span><br><span class="line">partitionsDefaultedToGroupOffsets);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="keyword">case</span> GROUP_OFFSETS:</span><br><span class="line">LOG.info(<span class="string">&quot;Consumer subtask &#123;&#125; will start reading the following &#123;&#125; partitions from the committed group offsets in Kafka: &#123;&#125;&quot;</span>,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">subscribedPartitionsToStartOffsets.size(),</span><br><span class="line">subscribedPartitionsToStartOffsets.keySet());</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">LOG.info(<span class="string">&quot;Consumer subtask &#123;&#125; initially has no partitions to read from.&quot;</span>,</span><br><span class="line">getRuntimeContext().getIndexOfThisSubtask());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;HDFS和Yarn同时重启对Flink-on-Yarn任务的影响&quot;&gt;&lt;a href=&quot;#HDFS和Yarn同时重启对Flink-on-Yarn任务的影响&quot; class=&quot;headerlink&quot; title=&quot;HDFS和Yarn同时重启对Flink on Yarn任务的影响&quot;&gt;&lt;/a&gt;HDFS和Yarn同时重启对Flink on Yarn任务的影响&lt;/h3&gt;&lt;h4 id=&quot;现象&quot;&gt;&lt;a href=&quot;#现象&quot; class=&quot;headerlink&quot; title=&quot;现象&quot;&gt;&lt;/a&gt;现象&lt;/h4&gt;&lt;p&gt;部分consumer的topic partition出现从Earlist开始消费的问题&lt;/p&gt;
    
    </summary>
    
      <category term="flink" scheme="http://guoyanlei.top/categories/flink/"/>
    
    
      <category term="kafka" scheme="http://guoyanlei.top/tags/kafka/"/>
    
      <category term="flink" scheme="http://guoyanlei.top/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>kafka消息投递语义</title>
    <link href="http://guoyanlei.top/2018/11/18/2018111801-kafka%E6%B6%88%E6%81%AF%E6%8A%95%E9%80%92%E8%AF%AD%E4%B9%89/"/>
    <id>http://guoyanlei.top/2018/11/18/2018111801-kafka消息投递语义/</id>
    <published>2018-11-18T07:30:00.000Z</published>
    <updated>2018-11-19T08:29:08.218Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kafka支持3种消息投递语义"><a href="#kafka支持3种消息投递语义" class="headerlink" title="kafka支持3种消息投递语义"></a>kafka支持3种消息投递语义</h3><ul><li>At most once：最多一次，消息可能会丢失，但不会重复（不确定消息是不是丢失，但是不再发送）</li><li>At least once：最少一次，消息不会丢失，可能会重复（不确定消息是不是丢失，但是还会再发送）</li><li>Exactly once：只且一次，消息不丢失不重复，只且消费一次（借助一些手段保证发送的消息是唯一）</li></ul><p>但是整体的消息投递语义需要Producer端和Consumer端两者来保证。</p><a id="more"></a><h3 id="Producer-消息生产者端"><a href="#Producer-消息生产者端" class="headerlink" title="Producer 消息生产者端"></a>Producer 消息生产者端</h3><p>当producer向broker发送一条消息，这时网络出错了，producer无法得知broker是否接受到了这条消息。网络出错可能是发生在消息传递的过程中，也可能发生在broker已经接受到了消息，并返回ack给producer的过程中。</p><p>这时，producer会有两种选择：</p><ul><li>不管了（At most once），消息可能会丢失</li><li>再发一次（At least once），消息可能会重复</li></ul><p>想要实现Exactly once，需要给每个Producer在初始化的时候都会被分配一个唯一的PID，<br>Producer向指定的Topic的特定Partition发送的消息都携带一个sequence number（简称seqNum），从零开始的单调递增的。</p><p>Broker会将Topic-Partition对应的seqNum在内存中维护，每次接受到Producer的消息都会进行校验；<br>只有seqNum比上次提交的seqNum刚好大一，才被认为是合法的。比它大的，说明消息有丢失；比它小的，说明消息重复发送了。</p><p>通过设置ack的值，可以区分多种消息确认机制。producer端的acks设置如下：</p><ul><li>acks=0 // 消息发了就发了，不等任何响应就认为消息发送成功</li><li>acks=1 // leader分片写消息成功就返回响应给producer</li><li>acks=all（-1） // 当acks=all， min.insync.replicas=2，就要求INSRNC列表中必须要有2个副本都写成功，才返回响应给producer，如果INSRNC中已同步副本数量不足2，就会报异常，如果没有2个副本写成功，也会报异常，消息就会认为没有写成功。</li></ul><h3 id="Broker-消息接收端"><a href="#Broker-消息接收端" class="headerlink" title="Broker 消息接收端"></a>Broker 消息接收端</h3><p>acks=1，表示当leader分片副本写消息成功就返回响应给producer，此时认为消息发送成功。</p><p>如果leader写成功但马上挂了，还没有将这个写成功的消息同步给其他的分片副本，那么这个分片此时的ISR列表为空</p><ul><li>如果unclean.leader.election.enable=true，就会发生log truncation（日志截取），同样会发生消息丢失。</li><li>如果unclean.leader.election.enable=false，那么这个分片上的服务就不可用了，producer向这个分片发消息就会抛异常。</li></ul><p>（unclean.leader.election.enable 是否允许不具备ISR资格的replicas选举为leader作为不得已的措施，甚至不惜牺牲部分数据。默认允许。建议允许。数据异常重要的情况例外。）</p><p>所以我们设置min.insync.replicas=2，unclean.leader.election.enable=false，producer端的acks=all，这样发送成功的消息就绝不会丢失。</p><h3 id="Consumer-消息消费者端"><a href="#Consumer-消息消费者端" class="headerlink" title="Consumer 消息消费者端"></a>Consumer 消息消费者端</h3><p>所有分片的副本都有自己的log文件（保存消息）和相同的offset值。当consumer没挂的时候，offset直接保存在内存中，如果挂了，就会发生负载均衡，需要consumer group中另外的consumer来接管并继续消费。</p><p>consumer消费消息的方式有以下2种;</p><ul><li><p>读取消息后先保存offset，后处理消息（至少一次消费）：保存offset成功，但是消息处理失败，这时来接管的consumer<br>就只能从上次保存的offset继续消费，这种情况下就有可能丢消息，但是保证了at most once语义。</p></li><li><p>读取消息后先处理消息，后保存offset（最多一次消费）：消息处理成功，但是在保存offset时失败，这时来接管的consumer只能从上一次保存的offset开始消费，这时消息就会被重复消费，也就是保证了at least once语义。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;kafka支持3种消息投递语义&quot;&gt;&lt;a href=&quot;#kafka支持3种消息投递语义&quot; class=&quot;headerlink&quot; title=&quot;kafka支持3种消息投递语义&quot;&gt;&lt;/a&gt;kafka支持3种消息投递语义&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;At most once：最多一次，消息可能会丢失，但不会重复（不确定消息是不是丢失，但是不再发送）&lt;/li&gt;
&lt;li&gt;At least once：最少一次，消息不会丢失，可能会重复（不确定消息是不是丢失，但是还会再发送）&lt;/li&gt;
&lt;li&gt;Exactly once：只且一次，消息不丢失不重复，只且消费一次（借助一些手段保证发送的消息是唯一）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是整体的消息投递语义需要Producer端和Consumer端两者来保证。&lt;/p&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://guoyanlei.top/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://guoyanlei.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kudu-master节点迁移</title>
    <link href="http://guoyanlei.top/2018/11/11/2018111101-kudu-master%E8%8A%82%E7%82%B9%E8%BF%81%E7%A7%BB/"/>
    <id>http://guoyanlei.top/2018/11/11/2018111101-kudu-master节点迁移/</id>
    <published>2018-11-11T07:30:00.000Z</published>
    <updated>2019-01-08T03:47:23.266Z</updated>
    
    <content type="html"><![CDATA[<p>kudu遇到问题：master节点配置较低，需要迁移到性能高的节点上，迁移比较麻烦，特此记录</p><p>迁移思路：</p><p>1）先添加kudu-master（在新节点上初始化数据目录，从已有master上同步过来元数据，刷新 Raft 配置，启动所有master）</p><p>2）删除要迁移的master（停掉所有进程，删除目标master，在新节点上重写 master 的 Raft 配置，再启动所有的）</p><a id="more"></a><h3 id="迁移前准备"><a href="#迁移前准备" class="headerlink" title="迁移前准备"></a>迁移前准备</h3><h4 id="1-识别存储目录，kudu的master同tablet一样配置有两个目录"><a href="#1-识别存储目录，kudu的master同tablet一样配置有两个目录" class="headerlink" title="1. 识别存储目录，kudu的master同tablet一样配置有两个目录"></a>1. 识别存储目录，kudu的master同tablet一样配置有两个目录</h4><ul><li>fs_wal_dir：write-ahead-logs目录 /data/kudu/master/wal</li><li>fs_data_dirs：数据目录 /data/kudu/master/data （线上是/data1/kudu/master/data …）</li></ul><h4 id="2-识别master的PRC端口，默认端口值为-7051"><a href="#2-识别master的PRC端口，默认端口值为-7051" class="headerlink" title="2. 识别master的PRC端口，默认端口值为 7051"></a>2. 识别master的PRC端口，默认端口值为 7051</h4><h4 id="3-识别master的UUID"><a href="#3-识别master的UUID" class="headerlink" title="3. 识别master的UUID"></a>3. 识别master的UUID</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">打开kudu-master web页面：http:&#x2F;&#x2F;node102.bigdata.dmp.local.com:8051&#x2F;masters</span><br><span class="line"></span><br><span class="line">各master的uuid</span><br><span class="line"></span><br><span class="line">85e0c097fcf747d286f59acf2ae3cfefLEADERnode102.bigdata.dmp.local.com</span><br><span class="line">c4fc5ceda4454e00ad1257a6489cedcfFOLLOWER node101.bigdata.dmp.local.com</span><br><span class="line">c7873360d8404fe8bcb3b999b8bd3c2aFOLLOWER node103.bigdata.dmp.local.com</span><br><span class="line"></span><br><span class="line">rpc_addresses &#123; host: &quot;node102.bigdata.dmp.local.com&quot; port: 7051 &#125;</span><br><span class="line">rpc_addresses &#123; host: &quot;node101.bigdata.dmp.local.com&quot; port: 7051 &#125;</span><br><span class="line">rpc_addresses &#123; host: &quot;node103.bigdata.dmp.local.com&quot; port: 7051 &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h3><h4 id="1-停掉所有kudu进程"><a href="#1-停掉所有kudu进程" class="headerlink" title="1. 停掉所有kudu进程"></a>1. 停掉所有kudu进程</h4><h4 id="2-在新节点上格式化数据目录"><a href="#2-在新节点上格式化数据目录" class="headerlink" title="2. 在新节点上格式化数据目录"></a>2. 在新节点上格式化数据目录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;kudu&#x2F;master&#x2F;</span><br><span class="line">sudo -u kudu kudu fs format --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data</span><br><span class="line">输出：</span><br><span class="line">I1031 15:38:54.215034  3151 env_posix.cc:1460] Not raising process file limit of 1000000; it is already as high as it can go</span><br><span class="line">I1031 15:38:54.215198  3151 file_cache.cc:463] Constructed file cache lbm with capacity 400000</span><br><span class="line">I1031 15:38:54.218797  3151 fs_manager.cc:377] Generated new instance metadata in path &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;instance:</span><br><span class="line">uuid: &quot;607c73cbf5484411a6be7fb0fc0b1554&quot;</span><br><span class="line">format_stamp: &quot;Formatted at 2018-10-31 07:38:54 on node104.bigdata.dmp.local.com&quot;</span><br><span class="line">I1031 15:38:54.220115  3151 fs_manager.cc:377] Generated new instance metadata in path &#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal&#x2F;instance:</span><br><span class="line">uuid: &quot;607c73cbf5484411a6be7fb0fc0b1554&quot;</span><br><span class="line">format_stamp: &quot;Formatted at 2018-10-31 07:38:54 on node104.bigdata.dmp.local.com&quot;</span><br><span class="line"></span><br><span class="line">sudo -u kudu kudu fs dump uuid --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 2&gt;&#x2F;dev&#x2F;null</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">607c73cbf5484411a6be7fb0fc0b1554</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="3-在新节点上重写-master-的-Raft-配置"><a href="#3-在新节点上重写-master-的-Raft-配置" class="headerlink" title="3. 在新节点上重写 master 的 Raft 配置"></a>3. 在新节点上重写 master 的 Raft 配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 00000000000000000000000000000000 85e0c097fcf747d286f59acf2ae3cfef:node102.bigdata.dmp.local.com:7051 c4fc5ceda4454e00ad1257a6489cedcf:node101.bigdata.dmp.local.com:7051 c7873360d8404fe8bcb3b999b8bd3c2a:node103.bigdata.dmp.local.com:7051</span><br><span class="line"></span><br><span class="line">直接执行，会报错：</span><br><span class="line">Not found: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000: No such file or directory (error 2)</span><br><span class="line"></span><br><span class="line">从正常到master节点上scp过来：</span><br><span class="line">scp 00000000000000000000000000000000 root@node104.bigdata.dmp.local.com:&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;</span><br><span class="line">chown kudu:kudu 00000000000000000000000000000000</span><br><span class="line"></span><br><span class="line">再次执行输出：</span><br><span class="line">I1031 15:56:30.394467  7935 env_posix.cc:1460] Not raising process file limit of 1000000; it is already as high as it can go</span><br><span class="line">I1031 15:56:30.394745  7935 file_cache.cc:463] Constructed file cache lbm with capacity 400000</span><br><span class="line">I1031 15:56:30.398227  7935 fs_report.cc:345] Block manager report</span><br><span class="line">--------------------</span><br><span class="line">1 data directories: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;data</span><br><span class="line">Total live blocks: 0</span><br><span class="line">Total live bytes: 0</span><br><span class="line">Total live bytes (after alignment): 0</span><br><span class="line">Total number of LBM containers: 0 (0 full)</span><br><span class="line">Did not check for missing blocks</span><br><span class="line">Did not check for orphaned blocks</span><br><span class="line">Total full LBM containers with extra space: 0 (0 repaired)</span><br><span class="line">Total full LBM container extra space in bytes: 0 (0 repaired)</span><br><span class="line">Total incomplete LBM containers: 0 (0 repaired)</span><br><span class="line">Total LBM partial records: 0 (0 repaired)</span><br><span class="line">I1031 15:56:30.398293  7935 fs_manager.cc:263] Time spent opening block manager: real 0.001s    user 0.000s     sys 0.001s</span><br><span class="line">I1031 15:56:30.398685  7935 fs_manager.cc:266] Opened local filesystem: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data,&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal</span><br><span class="line">uuid: &quot;607c73cbf5484411a6be7fb0fc0b1554&quot;</span><br><span class="line">format_stamp: &quot;Formatted at 2018-10-31 07:38:54 on node104.bigdata.dmp.local.com&quot;</span><br><span class="line">I1031 15:56:30.401140  7935 tool_action_local_replica.cc:257] Backed up current config to &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000.pre_rewrite.1540972590398731</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4-启动现有的-master"><a href="#4-启动现有的-master" class="headerlink" title="4. 启动现有的 master"></a>4. 启动现有的 master</h4><h4 id="5-使用以下命令将-master-数据复制到每个新-master，在每台新master上执行，只需要连接到一个master上："><a href="#5-使用以下命令将-master-数据复制到每个新-master，在每台新master上执行，只需要连接到一个master上：" class="headerlink" title="5. 使用以下命令将 master 数据复制到每个新 master，在每台新master上执行，只需要连接到一个master上："></a>5. 使用以下命令将 master 数据复制到每个新 master，在每台新master上执行，只需要连接到一个master上：</h4><p>此步是关键</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">在执行前，确保所有&#x2F;data&#x2F;kudu&#x2F;master&#x2F;目录下内容都是kudu:kudu用户</span><br><span class="line"></span><br><span class="line">chown -R kudu:kudu &#x2F;data&#x2F;kudu&#x2F;master&#x2F;</span><br><span class="line">sudo -u kudu kudu local_replica copy_from_remote --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 00000000000000000000000000000000 node102.bigdata.dmp.local.com:7051</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">I1031 16:08:49.705765 11161 env_posix.cc:1460] Not raising process file limit of 1000000; it is already as high as it can go</span><br><span class="line">I1031 16:08:49.706120 11161 file_cache.cc:463] Constructed file cache lbm with capacity 400000</span><br><span class="line">I1031 16:08:49.709882 11161 fs_report.cc:345] Block manager report</span><br><span class="line">--------------------</span><br><span class="line">1 data directories: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data&#x2F;data</span><br><span class="line">Total live blocks: 0</span><br><span class="line">Total live bytes: 0</span><br><span class="line">Total live bytes (after alignment): 0</span><br><span class="line">Total number of LBM containers: 0 (0 full)</span><br><span class="line">Did not check for missing blocks</span><br><span class="line">Did not check for orphaned blocks</span><br><span class="line">Total full LBM containers with extra space: 0 (0 repaired)</span><br><span class="line">Total full LBM container extra space in bytes: 0 (0 repaired)</span><br><span class="line">Total incomplete LBM containers: 0 (0 repaired)</span><br><span class="line">Total LBM partial records: 0 (0 repaired)</span><br><span class="line">I1031 16:08:49.709954 11161 fs_manager.cc:263] Time spent opening block manager: real 0.002s    user 0.000s     sys 0.001s</span><br><span class="line">I1031 16:08:49.710440 11161 fs_manager.cc:266] Opened local filesystem: &#x2F;data&#x2F;kudu&#x2F;master&#x2F;data,&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal</span><br><span class="line">uuid: &quot;607c73cbf5484411a6be7fb0fc0b1554&quot;</span><br><span class="line">format_stamp: &quot;Formatted at 2018-10-31 07:38:54 on node104.bigdata.dmp.local.com&quot;</span><br><span class="line">I1031 16:08:49.733937 11161 tablet_copy_client.cc:166] T 00000000000000000000000000000000 P 607c73cbf5484411a6be7fb0fc0b1554: Tablet Copy client: Beginning tablet copy session from remote peer at address node102.bigdata.dmp.local.com:7051</span><br><span class="line">I1031 16:08:49.755112 11161 tablet_copy_client.cc:422] T 00000000000000000000000000000000 P 607c73cbf5484411a6be7fb0fc0b1554: Tablet Copy client: Starting download of 908 data blocks...</span><br><span class="line">I1031 16:08:51.402842 11161 tablet_copy_client.cc:385] T 00000000000000000000000000000000 P 607c73cbf5484411a6be7fb0fc0b1554: Tablet Copy client: Starting download of 1 WAL segments...</span><br><span class="line">I1031 16:08:52.008639 11161 tablet_copy_client.cc:292] T 00000000000000000000000000000000 P 607c73cbf5484411a6be7fb0fc0b1554: Tablet Copy client: Tablet Copy complete. Replacing tablet superblock.</span><br></pre></td></tr></table></figure><h4 id="3-在新的和老的上面-重写-master-的-Raft-配置"><a href="#3-在新的和老的上面-重写-master-的-Raft-配置" class="headerlink" title="3. 在新的和老的上面 重写 master 的 Raft 配置"></a>3. 在新的和老的上面 重写 master 的 Raft 配置</h4><p>此步是关键</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 00000000000000000000000000000000 85e0c097fcf747d286f59acf2ae3cfef:node102.bigdata.dmp.local.com:7051 c4fc5ceda4454e00ad1257a6489cedcf:node101.bigdata.dmp.local.com:7051 c7873360d8404fe8bcb3b999b8bd3c2a:node103.bigdata.dmp.local.com:7051 607c73cbf5484411a6be7fb0fc0b1554:node104.bigdata.dmp.local.com:7051</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="6-在cm中添加kudu-master，启动所有master"><a href="#6-在cm中添加kudu-master，启动所有master" class="headerlink" title="6. 在cm中添加kudu-master，启动所有master"></a>6. 在cm中添加kudu-master，启动所有master</h4><p>至此，已经添加新的master，之后就可以删除不想用的master</p><h4 id="7-停掉所有进程，在cm中删除kudu-master"><a href="#7-停掉所有进程，在cm中删除kudu-master" class="headerlink" title="7. 停掉所有进程，在cm中删除kudu-master"></a>7. 停掉所有进程，在cm中删除kudu-master</h4><h4 id="8-在新的和老的上面-重写-master-的-Raft-配置（排除已删除的）"><a href="#8-在新的和老的上面-重写-master-的-Raft-配置（排除已删除的）" class="headerlink" title="8. 在新的和老的上面 重写 master 的 Raft 配置（排除已删除的）"></a>8. 在新的和老的上面 重写 master 的 Raft 配置（排除已删除的）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;data 00000000000000000000000000000000 85e0c097fcf747d286f59acf2ae3cfef:node102.bigdata.dmp.local.com:7051 c4fc5ceda4454e00ad1257a6489cedcf:node101.bigdata.dmp.local.com:7051 607c73cbf5484411a6be7fb0fc0b1554:node104.bigdata.dmp.local.com:7051</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="9-分别启动master，Tablet-Server"><a href="#9-分别启动master，Tablet-Server" class="headerlink" title="9. 分别启动master，Tablet Server"></a>9. 分别启动master，Tablet Server</h4><h3 id="线上迁移记录"><a href="#线上迁移记录" class="headerlink" title="线上迁移记录"></a>线上迁移记录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">-- 迁移前准备</span><br><span class="line"></span><br><span class="line">36c7f0a6a98a45ce8472fa747d5ac1ddFOLLOWER</span><br><span class="line">rpc_addresses &#123; host: &quot;node1.ikh.bigdata.dmp.com&quot; port: 7051 &#125; </span><br><span class="line"></span><br><span class="line">7b0e8b0afc934038ac4afccb05372bb7LEADER</span><br><span class="line">rpc_addresses &#123; host: &quot;node3.ikh.bigdata.dmp.com&quot; port: 7051 &#125; </span><br><span class="line"></span><br><span class="line">86961f7799e94afa97c2f2be6773141dFOLLOWER</span><br><span class="line">rpc_addresses &#123; host: &quot;node2.ikh.bigdata.dmp.com&quot; port: 7051 &#125; </span><br><span class="line"></span><br><span class="line">-- 迁移</span><br><span class="line"></span><br><span class="line">1. 停掉所有kudu进程</span><br><span class="line">2. 在新节点上格式化数据目录</span><br><span class="line">mkdir -p &#x2F;data&#x2F;kudu&#x2F;master&#x2F;</span><br><span class="line">chown -R kudu:kudu master&#x2F;</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">mkdir -p &#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line"></span><br><span class="line">chown -R kudu:kudu &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line">chown -R kudu:kudu &#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line"></span><br><span class="line">sudo -u kudu kudu fs format --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F;</span><br><span class="line"></span><br><span class="line">从输出中获取uuid：</span><br><span class="line">c6a414aeddaa469b9953df0c71fbe245 node51.ikh.bigdata.dmp.com</span><br><span class="line">da6b236ddd48464eb064c2b8e859ce1e node52.ikh.bigdata.dmp.com</span><br><span class="line">86ec16fbeaca455b840c800631ec14c1 node53.ikh.bigdata.dmp.com</span><br><span class="line"></span><br><span class="line">3. 从正常的master节点上把Raft配置scp过来：</span><br><span class="line">scp &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000 root@node51.ikh.bigdata.dmp.com:&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;</span><br><span class="line">scp &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000 root@node52.ikh.bigdata.dmp.com:&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;</span><br><span class="line">scp &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000 root@node53.ikh.bigdata.dmp.com:&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;</span><br><span class="line"></span><br><span class="line">chown kudu:kudu &#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;consensus-meta&#x2F;00000000000000000000000000000000</span><br><span class="line"></span><br><span class="line">4. 把已有的Raft配置写入新的master节点</span><br><span class="line">sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F; 00000000000000000000000000000000 36c7f0a6a98a45ce8472fa747d5ac1dd:node1.ikh.bigdata.dmp.com:7051 7b0e8b0afc934038ac4afccb05372bb7:node3.ikh.bigdata.dmp.com:7051 86961f7799e94afa97c2f2be6773141d:node2.ikh.bigdata.dmp.com:7051</span><br><span class="line"></span><br><span class="line">5. cm中启动现有的master</span><br><span class="line"></span><br><span class="line">6. 将现有的master中的文件块数据copy到新master上，只需连接其中一个现有master</span><br><span class="line">sudo -u kudu kudu local_replica copy_from_remote --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F; 00000000000000000000000000000000 node1.ikh.bigdata.dmp.com:7051</span><br><span class="line"></span><br><span class="line">7. 停掉master</span><br><span class="line"></span><br><span class="line">8. 在新的和老的所有master上面重写 Raft 配置，其中新master的uuid是第2步中的</span><br><span class="line">sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F; 00000000000000000000000000000000 36c7f0a6a98a45ce8472fa747d5ac1dd:node1.ikh.bigdata.dmp.com:7051 7b0e8b0afc934038ac4afccb05372bb7:node3.ikh.bigdata.dmp.com:7051 86961f7799e94afa97c2f2be6773141d:node2.ikh.bigdata.dmp.com:7051 c6a414aeddaa469b9953df0c71fbe245:node51.ikh.bigdata.dmp.com:7051 da6b236ddd48464eb064c2b8e859ce1e:node52.ikh.bigdata.dmp.com:7051 86ec16fbeaca455b840c800631ec14c1:node53.ikh.bigdata.dmp.com:7051</span><br><span class="line"></span><br><span class="line">9. 在cm中添加新的kudu-master角色，启动所有master</span><br><span class="line"></span><br><span class="line">10. 停掉所有进程，在cm中删除kudu-master</span><br><span class="line"></span><br><span class="line">11. 在新的和老的上面 重写 master 的 Raft 配置（排除已删除的）</span><br><span class="line">sudo -u kudu kudu local_replica cmeta rewrite_raft_config --fs_wal_dir&#x3D;&#x2F;data&#x2F;kudu&#x2F;master&#x2F;wal --fs_data_dirs&#x3D;&#x2F;data1&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data2&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data3&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data4&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data5&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data6&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data7&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data8&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data9&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data10&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data11&#x2F;kudu&#x2F;master&#x2F;data&#x2F;,&#x2F;data12&#x2F;kudu&#x2F;master&#x2F;data&#x2F; 00000000000000000000000000000000 c6a414aeddaa469b9953df0c71fbe245:node51.ikh.bigdata.dmp.com:7051 da6b236ddd48464eb064c2b8e859ce1e:node52.ikh.bigdata.dmp.com:7051 86ec16fbeaca455b840c800631ec14c1:node53.ikh.bigdata.dmp.com:7051</span><br><span class="line"></span><br><span class="line">12. 启动master，启动tablet server</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--注意：</span><br><span class="line">迁移后之前在impala上建的kudu&quot;外部表&quot;就不能读了，因为它们默认指定的是老的kudu-master地址</span><br><span class="line"></span><br><span class="line">解决办法：</span><br><span class="line"></span><br><span class="line">- 在impala上删掉这些&quot;外部表&quot;（删除过程可能需要等几分钟）</span><br><span class="line">- 之后再重新建表</span><br><span class="line"></span><br><span class="line">drop table prod.ods_kudu_liyue_dsp_issue_log_1d</span><br><span class="line"></span><br><span class="line">create EXTERNAL table prod.ods_kudu_liyue_dsp_issue_log_1d stored as kudu</span><br><span class="line">TBLPROPERTIES(&#39;EXTERNAL&#39;&#x3D;&#39;TRUE&#39;,&#39;kudu.table_name&#39; &#x3D; &#39;ods_kudu_liyue_dsp_issue_log_1d&#39;)</span><br><span class="line">;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;kudu遇到问题：master节点配置较低，需要迁移到性能高的节点上，迁移比较麻烦，特此记录&lt;/p&gt;
&lt;p&gt;迁移思路：&lt;/p&gt;
&lt;p&gt;1）先添加kudu-master（在新节点上初始化数据目录，从已有master上同步过来元数据，刷新 Raft 配置，启动所有master）&lt;/p&gt;
&lt;p&gt;2）删除要迁移的master（停掉所有进程，删除目标master，在新节点上重写 master 的 Raft 配置，再启动所有的）&lt;/p&gt;
    
    </summary>
    
      <category term="kudu" scheme="http://guoyanlei.top/categories/kudu/"/>
    
    
      <category term="kudu" scheme="http://guoyanlei.top/tags/kudu/"/>
    
  </entry>
  
  <entry>
    <title>从0开始架构学习总结</title>
    <link href="http://guoyanlei.top/2018/10/08/2018100801-%E4%BB%8E0%E5%BC%80%E5%A7%8B%E6%9E%B6%E6%9E%84%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>http://guoyanlei.top/2018/10/08/2018100801-从0开始架构学习总结/</id>
    <published>2018-10-08T07:30:00.000Z</published>
    <updated>2018-11-19T04:00:39.260Z</updated>
    
    <content type="html"><![CDATA[<h2 id="架构基础"><a href="#架构基础" class="headerlink" title="架构基础"></a>架构基础</h2><h3 id="架构概念"><a href="#架构概念" class="headerlink" title="架构概念"></a>架构概念</h3><blockquote><p>软件架构指软件系统的顶层结构。</p></blockquote><ul><li>子系统：子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中一部分</li><li>框架：关注的是“规范”（Framework），如Spring MVC框架</li><li>架构：关注的是“结构”（Architecture）</li><li>模块：逻辑的角度来拆分系统后，得到的单元就是“模块”，主要目的是职责分离</li><li>组件：从物理的角度来拆分系统后，得到的单元就是“组件”，主要目的是单元复用</li></ul><p>一句话总结：架构是顶层设计；框架是面向编程或配置的半成品；组件是从技术维度上的复用；模块是从业务维度上职责的划分；系统是相互协同可运行的实体。</p><p><img src="/img/architecture/architecture_1.png" alt="architecture_1"></p><a id="more"></a><h3 id="架构设计的目的"><a href="#架构设计的目的" class="headerlink" title="架构设计的目的"></a>架构设计的目的</h3><blockquote><p>架构设计的主要目的是为了解决软件系统复杂度带来的问题。</p></blockquote><p>一句话总结：架构即(重要)决策，是在一个有约束的盒子里去求解或接近最合适的解。这个有约束的盒子是团队经验、成本、资源、进度、业务所处阶段等所编织、掺杂在一起的综合体(人，财，物，时间，事情等)。架构无优劣，但是存在恰当的架构用在合适的软件系统中，而这些就是决策的结果。</p><h3 id="软件系统复杂度的来源"><a href="#软件系统复杂度的来源" class="headerlink" title="软件系统复杂度的来源"></a>软件系统复杂度的来源</h3><h4 id="1-高性能"><a href="#1-高性能" class="headerlink" title="1. 高性能"></a>1. 高性能</h4><blockquote><p>单台计算机内部为了高性能带来的复杂度（垂直维度）</p><p>多台计算机集群为了高性能带来的复杂度（水平维度）</p></blockquote><p>垂直维度方案比较适合业务阶段早期和成本可接受的阶段，该方案是提升性能最简单直接的方式，但是受成本与硬件能力天花板的限制。</p><p>水平维度方案所带来的好处要在业务发展的后期才能体现出来。起初，该方案会花费更多的硬件成本，另外一方面对技术团队也提出了更高的要求；但是，没有垂直方案的天花板问题。一旦达到一定的业务阶段，水平维度是技术发展的必由之路。</p><h4 id="2-高可用"><a href="#2-高可用" class="headerlink" title="2. 高可用"></a>2. 高可用</h4><blockquote><p>系统无中断地执行其功能的能力</p></blockquote><ul><li>计算高可用（任务分配器，分配器于业务服务器的交互，分配算法等）</li><li>存储高可用（CAP定理：存储高可用不可能同时满足“一致性、可用性、分区容错性”，最多满足其中两个）</li></ul><p>实现高可用的本质：“冗余”（高性能增加机器目的在于“扩展”处理性能；高可用增加机器目的在于“冗余”处理单元）</p><p>高可用的解决方法不是解决，而是减少或者规避，而规避某个问题的时候，一般都会引发另一个问题，只是这个问题比之前的小，高可用的设计过程其实也是一个取舍的过程。</p><h4 id="3-可扩展"><a href="#3-可扩展" class="headerlink" title="3. 可扩展"></a>3. 可扩展</h4><blockquote><p>系统为了应对将来需求变化而提供的一种扩展能力，当有新的需求出现时，系统不需要或者仅需要少量修改就可以支持，无须整个系统重构或者重建。</p></blockquote><p>设计具备良好可扩展性的系统：<br>1）从业务维度。对业务深入理解，对可预计的业务变化进行预测。<br>2）从技术维度。利用扩展性好的技术，实现对变化的封装。</p><h4 id="4-低成本"><a href="#4-低成本" class="headerlink" title="4. 低成本"></a>4. 低成本</h4><blockquote><p>低成本本质上是与高性能和高可用冲突的，当无法设计出满足成本要求的方案，就只能协调并调整成本目标</p></blockquote><p>一般通过创新达到低成本目标：</p><ul><li>引入新技术</li><li>开创一个全新技术领域</li></ul><h4 id="5-安全"><a href="#5-安全" class="headerlink" title="5. 安全"></a>5. 安全</h4><blockquote><p>功能安全（防小偷）：减少系统潜在的缺陷，阻止黑客破坏行为</p><p>架构安全（防强盗）：保护系统不受恶意访问和攻击，保护系统的重要数据不被窃取</p></blockquote><h4 id="6-规模"><a href="#6-规模" class="headerlink" title="6. 规模"></a>6. 规模</h4><blockquote><p>规模带来复杂度的主要原因就是“量变引起质变”，当数量超过一定的阈值后，复杂度会发生质的变化</p></blockquote><p>规模问题需要与高性能、高可用、高扩展、高伸缩性统一考虑。常采用“分而治之，各个击破”的方法策略。</p><h3 id="架构设计三原则"><a href="#架构设计三原则" class="headerlink" title="架构设计三原则"></a>架构设计三原则</h3><h4 id="合适原则"><a href="#合适原则" class="headerlink" title="合适原则"></a>合适原则</h4><blockquote><p>合适优于业界领先</p></blockquote><h4 id="简单原则"><a href="#简单原则" class="headerlink" title="简单原则"></a>简单原则</h4><blockquote><p>简单优于复杂</p></blockquote><h4 id="演化原则"><a href="#演化原则" class="headerlink" title="演化原则"></a>演化原则</h4><blockquote><p>演化优于一步到位</p></blockquote><h3 id="架构设计流程"><a href="#架构设计流程" class="headerlink" title="架构设计流程"></a>架构设计流程</h3><h4 id="第一步：识别复杂度"><a href="#第一步：识别复杂度" class="headerlink" title="第一步：识别复杂度"></a>第一步：识别复杂度</h4><blockquote><p>将主要的复杂度问题列出来，然后根据业务、技术、团队等综合情况进行排序，优先解决当前面临的最主要的复杂度问题。</p></blockquote><p>具体做法：</p><ul><li>构建复杂度的来源清单——高性能、可用性、扩展性、安全、低成本、规模等。</li><li>结合需求、技术、团队、资源等对上述复杂度逐一分析是否需要？是否关键？</li><li>按照上述的分析结论，得到复杂度按照优先级的排序清单，越是排在前面的复杂度，就越关键，就越优先解决。</li></ul><h4 id="第二步：设计备选方案"><a href="#第二步：设计备选方案" class="headerlink" title="第二步：设计备选方案"></a>第二步：设计备选方案</h4><ul><li>备选方案不要过于详细。备选阶段解决的是技术选型问题，而不是技术细节。</li><li>备选方案的数量以 3~5个为最佳。</li><li>备选方案的技术差异要明显。</li><li>备选方案不要只局限于已经熟悉的技术。</li></ul><h4 id="第三步：评估和选择备选方案"><a href="#第三步：评估和选择备选方案" class="headerlink" title="第三步：评估和选择备选方案"></a>第三步：评估和选择备选方案</h4><blockquote><p>列出我们需要关注的质量属性点，然后分别从这些质量属性的维度去评估每个方案，再综合挑选适合当时情况的最优方案。</p></blockquote><p>质量属性：性能、可用性、硬件成本、项目投入、复杂度、安全性、可扩展性等</p><h4 id="第四步：详细方案设计"><a href="#第四步：详细方案设计" class="headerlink" title="第四步：详细方案设计"></a>第四步：详细方案设计</h4><blockquote><p>详细方案设计是将方案涉及的关键技术细节给确定下来</p></blockquote><h2 id="高性能架构模式"><a href="#高性能架构模式" class="headerlink" title="高性能架构模式"></a>高性能架构模式</h2><p><img src="/img/architecture/architecture_2.png" alt="architecture_2"></p><h3 id="高性能数据库集群"><a href="#高性能数据库集群" class="headerlink" title="高性能数据库集群"></a>高性能数据库集群</h3><h4 id="读写分离"><a href="#读写分离" class="headerlink" title="读写分离"></a>读写分离</h4><blockquote><p>基本原理是将数据库读写操作分散到不同的节点上。</p></blockquote><ul><li>主从：“从机”是需要提供读数据的功能的</li><li>主备：一般被认为仅仅提供备份功能，不提供访问功能</li></ul><p>主从基本实现：</p><ul><li>数据库服务器搭建主从集群，一主一从、一主多从都可以</li><li>数据库主机负责读写操作，从机只负责读操作。</li><li>数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据。</li><li>业务服务器将写操作发给数据库主机，将读操作发给数据库从机。</li></ul><p>需解决的问题：</p><ul><li>复制延迟：数据同步延迟<ul><li>写操作后的读操作指定发给数据库主服务器</li><li>读从机失败后再读一次主机</li><li>关键业务读写操作全部指向主机，非关键业务采用读写分离</li></ul></li><li>分配机制：将读写区分开<ul><li>程序代码封装（在代码中抽象一个数据访问层，Hibernate）</li><li>中间件封装（独立一套系统出来，实现读写操作分离和数据库服务器连接的管理，MySQL Router）</li></ul></li></ul><p>总结：并不是说一有性能问题就上读写分离，而是应该先优化，例如优化慢查询，调整不合理的业务逻辑，引入缓存等，只有确定系统没有优化空间后，才考虑读写分离或者集群。</p><h4 id="分库分表"><a href="#分库分表" class="headerlink" title="分库分表"></a>分库分表</h4><h5 id="业务分库"><a href="#业务分库" class="headerlink" title="业务分库"></a>业务分库</h5><blockquote><p>业务分库指的是按照业务模块将数据分散到不同的数据库服务器。</p></blockquote><p>需注意的问题：</p><ul><li>JOIN操作问题</li><li>事务问题</li><li>成本问题</li></ul><h5 id="分表"><a href="#分表" class="headerlink" title="分表"></a>分表</h5><blockquote><p>同一业务的单表数据进行拆分</p></blockquote><ul><li>垂直分表：拆分字段到不同的表</li><li>水平分表：拆分记录到不同的表</li></ul><p>水平分表的路由问题：</p><ul><li>范围路由：选取有序的数据列（例如，整形、时间戳等）作为路由的条件，不同分段分散到不同的数据库表中。</li><li>Hash路由：选取某个列（或者某几个列组合也可以）的值进行Hash运算，然后根据 Hash 结果分散到不同的数据库表中。</li><li>配置路由：配置路由就是路由表，用一张独立的表来记录路由信息。</li></ul><h4 id="高性能NoSQL"><a href="#高性能NoSQL" class="headerlink" title="高性能NoSQL"></a>高性能NoSQL</h4><p>常见的NoSQL方案分为4类。</p><ul><li>K-V 存储：解决关系数据库无法存储数据结构的问题，以Redis为代表<ul><li>Redis 的 Value 是具体的数据结构，包括string、hash、list、set、sorted set、bitmap 和 hyperloglog，常被称为数据结构服务器</li><li>Redis 的事务只能保证隔离性和一致性（I 和 C），无法保证原子性和持久性（A 和 D）</li></ul></li><li>文档数据库：解决关系数据库强schema约束的问题，以MongoDB为代表<ul><li>新增字段简单，历史数据不回出错</li><li>很容易存储复杂数据</li><li>代价是不支持事物、无法实现关系数据库的 join 操作</li></ul></li><li>列式数据库：解决关系数据库大数据场景下的I/O问题，以HBase为代表<ul><li>读取表中某些列时，只需要把需要的列读取到内存中（节省I/O）</li><li>列式存储还具备更高的存储压缩比，能够节省更多的存储空间</li><li>需要频繁地更新多个列时，磁盘是随机写操作，效率不高</li></ul></li><li>全文搜索引擎：解决关系数据库的全文搜索性能问题，以Elasticsearch为代表<ul><li>技术原理被称为“倒排索引”，建立单词到文档的索引</li><li>全文搜索引擎的索引对象是单词和文档，而关系数据库的索引对象是键和行</li></ul></li></ul><h4 id="高性能缓存"><a href="#高性能缓存" class="headerlink" title="高性能缓存"></a>高性能缓存</h4><blockquote><p>缓存就是为了弥补存储系统在复杂业务场景（经过复杂运算过的数据、读多写少等）下的不足，其基本原理是将可能重复使用的数据放到内存中，一次生成、多次使用，避免每次使用都去访问存储系统。</p></blockquote><p>缓存架构的设计要点</p><h5 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h5><blockquote><p>缓存没有发挥作用，业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据。</p></blockquote><h5 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h5><blockquote><p>当缓存失效（过期）后引起系统性能急剧下降的情况。由于旧的缓存已经被清除，新的缓存还未生成，并且处理这些请求的线程都不知道另外有一个线程正在生成缓存，因此所有的请求都会去重新生成缓存，都会去访问存储系统，从而对存储系统造成巨大的性能压力。这些压力又会拖慢整个系统。</p></blockquote><p>解放方法：</p><ul><li>更新锁：对缓存更新操作进行加锁保护，保证只有一个线程能够进行缓存更新，未能获取更新锁的线程要么等待锁释放后重新读取缓存，要么就返回空值或默认值。</li><li>后台更新：后台线程来更新缓存，而不是由业务线程来更新缓存，缓存本身的有效期设置为永久，后台线程定时更新缓存。</li></ul><h5 id="缓存热点"><a href="#缓存热点" class="headerlink" title="缓存热点"></a>缓存热点</h5><blockquote><p>如果大部分甚至所有的业务请求都命中同一份缓存数据，则这份数据所在的缓存服务器的压力也很大。</p></blockquote><p>解决方法：复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力。（不同的缓存副本不要设置统一的过期时间，否则就会出现所有缓存副本同时生成同时失效的情况，从而引发缓存雪崩）</p><h3 id="高性能服务器"><a href="#高性能服务器" class="headerlink" title="高性能服务器"></a>高性能服务器</h3><ul><li>尽量提升单服务器的性能，将单服务器的性能发挥到极致。</li><li>如果单服务器无法支撑性能，设计服务器集群方案。</li></ul><h4 id="单服务器的高性能"><a href="#单服务器的高性能" class="headerlink" title="单服务器的高性能"></a>单服务器的高性能</h4><blockquote><p>单服务器高性能的关键之一就是设计服务器采取的 “并发模型”</p></blockquote><ul><li>服务器如何处理连接：I/O 模型：阻塞、非阻塞、同步、异步</li><li>服务器如何处理请求：进程模型：单进程、多进程、多线程</li></ul><p>单服务器高性能模式（PPC 和 TPC 模式）：</p><ul><li>PPC（Process Per Connection）：指每次有新的连接就新建一个 “进程” 去专门处理这个连接的请求</li><li>prefork：系统在启动的时候就预先创建好进程，然后才开始接受用户的请求，省去fork进程操作</li><li>TPC（Thread Per Connection）：指每次有新的连接就新建一个 “线程” 去专门处理这个连接的请求</li><li>prethread：预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作</li></ul><p>单服务器高性能模式（Reactor 和 Proactor 模式）：</p><blockquote><p>资源复用，即不再单独为每个连接创建进程，而是创建一个进程池，将连接分配给进程，一个进程可以处理多个连接的业务.</p></blockquote><p>IO操作分两个阶段【去饭店点餐排队】：</p><ul><li><ol><li>等待数据准备好(读到内核缓存) 【准备菜品】</li></ol></li><li><ol start="2"><li>将数据从内核缓存读到用户空间(进程空间) 【端走吃到肚里】</li></ol></li></ul><p>一般来说1花费的时间远远大于2。 </p><ul><li>同步阻塞IO：1上阻塞2上也阻塞【付完钱在收银台等着，菜好之后取走离开】</li><li>同步非阻塞IO：1上非阻塞2上阻塞（Reactor模型）【付完钱领号，等待叫号，自己来取】</li><li>异步非阻塞IO：1上非阻塞2上非阻塞是（Proactor模型）【付完钱领号，回到座位干别的，菜好后服务员把菜端来】</li></ul><h4 id="服务器集群的高性能"><a href="#服务器集群的高性能" class="headerlink" title="服务器集群的高性能"></a>服务器集群的高性能</h4><blockquote><p>高性能集群的复杂性：需要一个任务分配器（负载均衡），以及任务分配算法</p></blockquote><h5 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h5><p>负载均衡的分类：</p><ul><li>DNS负载均衡：实现地理级别的均衡（简单、成本低）</li><li>硬件负载均衡：用于负载均衡的基础网络设备（价格昂贵、扩展能力差）</li><li>软件负载均衡：常见的Nginx（网络7层负载均衡）和 LVS（4层负载均衡）</li></ul><p>DNS 负载均衡用于实现地理级别的负载均衡；硬件负载均衡用于实现集群级别的负载均衡；软件负载均衡用于实现机器级别的负载均衡。</p><h5 id="任务分配算法"><a href="#任务分配算法" class="headerlink" title="任务分配算法"></a>任务分配算法</h5><ul><li>轮询：按照顺序轮流分配到服务器上</li><li>加权轮询：根据服务器权重进行任务分配</li><li>负载最低优先：将任务分配给当前负载最低的服务器（站在服务器的角度）</li><li>性能最优类：优先将任务分配给处理速度最快的服务器（站在客户端的角度）</li><li>Hash类：根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上</li></ul><h2 id="高可用架构模式"><a href="#高可用架构模式" class="headerlink" title="高可用架构模式"></a>高可用架构模式</h2><p><img src="/img/architecture/architecture_3.png" alt="architecture_3"></p><h3 id="分布式系统架构CAP定理"><a href="#分布式系统架构CAP定理" class="headerlink" title="分布式系统架构CAP定理"></a>分布式系统架构CAP定理</h3><blockquote><p>在一个分布式系统（指互相连接并共享数据的节点的集合）中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲。</p></blockquote><ul><li>一致性（C）：对某个指定的客户端来说，读操作保证能够返回最新的写操作结果</li><li>可用性（A）：非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）</li><li>分区容错型（P）：当出现网络分区后，系统能够继续“履行职责”。</li></ul><blockquote><p>设计分布式系统架构时必须选择 P（分区容忍），因为网络本身无法做到 100%可靠，有可能出故障，所以分区是一个必然的现象。</p></blockquote><ul><li>CP：发生分区现象后，有问题的节点不能同步数据，为了保证一致性（C），请求数据时只能返回Error，就不能满足可用性（A）</li><li>AP：发生分区现象后，有问题的节点不能同步数据，为了保证可用性（A），请求数据时只能返回旧数据，就不能满足一致性（C）</li></ul><p>注意：</p><ul><li>CAP 关注的粒度是数据，而不是整个系统。</li><li>CAP 是忽略网络延迟的（数据能够瞬间复制到所有节点）</li><li>既要考虑分区发生时选择 CP 还是 AP，也要考虑分区没有发生时如何保证 CA。</li><li>放弃并不等于什么都不做，需要为分区恢复后做准备。</li></ul><p>BASE理论：即使无法做到强一致性（CAP），但应用可以采用适合的方式达到最终一致性。</p><ul><li>基本可用（Basically Available）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用</li><li>软状态（Soft State）：允许系统存在中间状态，而该中间状态不会影响系统整体可用性</li><li>最终一致性（Eventual Consistency）：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态</li></ul><p>ACID：数据库管理系统为了保证 “事务” 的正确性而提出来的一个理论。</p><ul><li>Atomicity（原子性）：要么全部完成，要么全部不完成</li><li>Consistency（一致性）：开始和结束后数据库完整型没有破坏</li><li>Isolation（隔离性）：多个并发事务同时对数据进行读写和修改的能力</li><li>Durability（持久性）：事务处理结束后，对数据的修改就是永久的</li></ul><h3 id="高可用存储架构"><a href="#高可用存储架构" class="headerlink" title="高可用存储架构"></a>高可用存储架构</h3><h4 id="双机高可用架构（隐含的假设：主机能够存储所有数据）"><a href="#双机高可用架构（隐含的假设：主机能够存储所有数据）" class="headerlink" title="双机高可用架构（隐含的假设：主机能够存储所有数据）"></a>双机高可用架构（隐含的假设：主机能够存储所有数据）</h4><ul><li>主备复制（备机仅仅只为备份，并没有提供读写操作，硬件成本上有浪费）</li><li>主从复制（主机负责读写操作，从机只负责读操作，不负责写操作）</li><li>主备/主从切换（在原有方案的基础上增加“切换”功能，即系统自动决定主机角色，并完成角色切换）</li><li>主主复制（两台机器都是主机，互相将数据复制给对方，任意连接一台进行读写操作）</li></ul><p>主备切换架构：</p><ul><li>互连式：主备机直接建立状态传递的渠道</li><li>中介式：主备机之间不直接连接，而都去连接中介，并且通过中介来传递状态</li><li>模拟式：主备机之间并不传递任何状态数据，而是备机模拟成一个客户端，向主机发起模拟的读写操作，根据读写操作的响应情况来判断主机的状态</li></ul><h4 id="集群式高可用存储架构"><a href="#集群式高可用存储架构" class="headerlink" title="集群式高可用存储架构"></a>集群式高可用存储架构</h4><h5 id="数据集群"><a href="#数据集群" class="headerlink" title="数据集群"></a>数据集群</h5><ul><li>数据集中集群（数据都只能往主机中写，而读操作可以参考主备、主从架构进行灵活多变，如zookeeper集群-ZAB算法）</li><li>数据分散集群（每台服务器都会负责存储一部分数据，每台服务器又会备份一部分数据，如Hadoop集群）</li></ul><h5 id="数据分区"><a href="#数据分区" class="headerlink" title="数据分区"></a>数据分区</h5><ul><li>集中式：所有的分区都将数据备份到备份中心</li><li>互备式：每个分区备份另外一个分区的数据</li><li>独立式：每个分区自己有独立的备份中心</li></ul><h3 id="计算高可用"><a href="#计算高可用" class="headerlink" title="计算高可用"></a>计算高可用</h3><ul><li>主备：主机执行所有计算任务，当主机故障时，任务分配器不会自动将计算任务发送给备机</li><li>主从：任务分配器需要将任务进行分类，确定哪些任务可以发送给主机执行，哪些任务可以发送给备机执行</li><li>集群<ul><li>对称集群：负载均衡集群（集群中每个服务器的角色都是一样的，都可以执行所有任务）</li><li>非对称集群：集群中的服务器分为多个不同的角色，不同的角色执行不同的任务</li></ul></li></ul><h3 id="业务高可用"><a href="#业务高可用" class="headerlink" title="业务高可用"></a>业务高可用</h3><p>存储高可用和计算高可用都是为了解决部分服务器故障的场景下，如何保证系统能够继续提供服务。在一些极端场景下，有可能所有服务器都出现故障，此时就需要设计异地多活架构。</p><blockquote><p>异地：指地理位置上不同的地方</p><p>多活：指不同地理位置上的系统都能够提供业务服务</p></blockquote><ul><li>同城异区（同一个城市不同区的多个机房）</li><li>跨城异地（不同城市的多个机房）</li><li>跨国异地（不同国家的多个机房）</li></ul><p>四大设计技巧:</p><ul><li>保证 “核心” 业务的异地多活</li><li>保证核心数据 “最终” 一致性</li><li>采用多种手段同步数据</li><li>只保证绝大部分用户的异地多活</li></ul><blockquote><p>采用多种手段，保证绝大部分用户的核心业务异地多活</p></blockquote><p>四步：</p><ul><li>业务分级（挑选出核心的业务，只为核心业务设计异地多活）</li><li>数据分类（识别所有的数据及数据特征）</li><li>数据同步</li><li>异常处理</li></ul><h4 id="接口级的故障"><a href="#接口级的故障" class="headerlink" title="接口级的故障"></a>接口级的故障</h4><p>接口级故障表现为：系统并没有宕机，网络也没有中断，但业务却出现问题了（业务响应缓慢、大量访问超时、大量访问出现异常）</p><blockquote><p>优先保证核心业务和优先保证绝大部分用户</p></blockquote><p>解决方案：</p><ul><li>降级：（应对系统自身的故障）将某些业务或者接口的功能降低，可以是只提供部分功能，也可以是完全停掉所有功能<ul><li>系统后门降级</li><li>独立降级系统</li></ul></li><li>熔断：（应对依赖的外部系统故障的情况）壮士断腕，停掉外部依赖的调用</li><li>限流：（直接拒绝用户）从系统功能优先级的角度考虑如何应对故障，只允许系统能够承受的访问量进来，超出系统访问能力的请求将被丢弃<ul><li>基于请求限流</li><li>基于资源限流</li></ul></li><li>排队：（让用户等待一段时间）使用 Kafka 这类消息队列来缓存用户请求</li></ul><h2 id="可扩展架构模式"><a href="#可扩展架构模式" class="headerlink" title="可扩展架构模式"></a>可扩展架构模式</h2><p><img src="/img/architecture/architecture_4.png" alt="architecture_4"></p><h3 id="可扩展的基本思想"><a href="#可扩展的基本思想" class="headerlink" title="可扩展的基本思想"></a>可扩展的基本思想</h3><blockquote><p>总结为一个字：拆！</p></blockquote><p>流程 &gt; 服务 &gt; 功能</p><ul><li>面向流程拆分：将整个业务流程拆分为几个阶段，每个阶段作为一部分（得到分层架构）</li><li>面向服务拆分：将系统提供的服务拆分，每个服务作为一部分（SOA、微服务）</li><li>面向功能拆分：将系统提供的功能拆分，每个功能作为一部分（微内核架构）</li></ul><h3 id="分层架构"><a href="#分层架构" class="headerlink" title="分层架构"></a>分层架构</h3><blockquote><p>本质在于隔离关注点，即每个层中的组件只会处理本层的逻辑。</p></blockquote><ul><li>C/S 架构、B/S 架构（划分的对象是整个业务系统，划分的维度是用户交互）</li><li>MVC 架构、MVP 架构（划分的对象是单个业务子系统，划分的维度是职责）</li><li>逻辑分层架构（划分的对象可以是整个或单个业务子系统，划分的维度也是职责，逻辑分层架构中的层是自顶向下依赖的）</li></ul><h3 id="SOA架构（面向服务架构）"><a href="#SOA架构（面向服务架构）" class="headerlink" title="SOA架构（面向服务架构）"></a>SOA架构（面向服务架构）</h3><blockquote><p>SOA 出现的背景是企业内部的系统重复建设且效率低下，SOA 解决了传统 IT 系统重复建设和扩展效率低的问题。</p></blockquote><ul><li>SOA是集成的思想，是解决服务孤岛打通链条，是无奈之举。</li><li>ESB集中化的管理带来了性能不佳，厚重等问题。也无法快速扩展。不适合互联网的业务特点</li></ul><h3 id="微服务架构"><a href="#微服务架构" class="headerlink" title="微服务架构"></a>微服务架构</h3><blockquote><p>SOA 和微服务本质上是两种不同的架构设计理念，只是在“服务”这个点上有交集而已。</p></blockquote><ul><li>微服务是 SOA 的实现方式（服务粒度比SOA细）</li><li>微服务是去掉 ESB 后的 SOA（更为轻量级，例如HTTP RESTful）</li><li>微服务是一种和 SOA 相似但本质上不同的架构理念（服务交付快，适合互联网）</li></ul><h4 id="微服务拆分方法（维度）："><a href="#微服务拆分方法（维度）：" class="headerlink" title="微服务拆分方法（维度）："></a>微服务拆分方法（维度）：</h4><ol><li>基于业务逻辑拆分（系统中的业务模块按照职责范围识别出来，每个单独的业务模块拆分为一个独立的服务）</li><li>基于可扩展拆分（将系统中的业务模块按照稳定性排序，将已经成熟和改动不大的服务拆分为稳定服务，将经常变化和迭代的服务拆分为变动服务）</li><li>基于可靠性拆分（将系统中的业务模块按照优先级排序，将可靠性要求高的核心服务和可靠性要求低的非核心服务拆分开来，然后重点保证核心服务的高可用）</li><li>基于性能拆分（将性能要求高或者性能压力大的模块拆分出来，避免性能压力大的服务影响其他服务）</li></ol><h4 id="微服务基础设施："><a href="#微服务基础设施：" class="headerlink" title="微服务基础设施："></a>微服务基础设施：</h4><ul><li>自动化测试（通过自动化测试系统来完成绝大部分测试回归的工作）</li><li>自动化部署（自动化部署的系统来完成大量的部署操作）</li><li>配置中心（有的运行期配置需要动态修改并且所有节点即时生效，人工操作是无法做到的）</li><li>接口框架（HTTP/REST 或者 RPC 方式）</li><li>API网关（内部的微服务之间是互联互通的，相互之间的访问都是点对点的，需要一个统一的 API 网关，负责外部系统的访问操作）</li><li>服务发现<ul><li>自理式：每个微服务自己完成服务发现</li><li>代理式：由负载均衡系统来完成微服务之间的服务发现</li></ul></li><li>服务路由（进行某次调用请求时，我们还需要从所有符合条件的可用微服务节点中挑选出一个具体的节点发起请求）</li><li>服务容错（从整体上来看，系统中某个微服务出故障的概率会大大增加）</li><li>服务监控（需要服务监控系统来完成微服务节点的监控）</li><li>服务跟踪（跟踪某一个请求在微服务中的完整路径）</li><li>服务安全（接入安全、数据安全、传输安全）</li></ul><h3 id="微内核架构（插件化架构）"><a href="#微内核架构（插件化架构）" class="headerlink" title="微内核架构（插件化架构）"></a>微内核架构（插件化架构）</h3><p>两类组件：</p><ul><li>核心系统（core system）：负责和具体业务功能无关的通用功能（模块加载、模块间通信等）</li><li>插件模块（plug-in modules）：负责实现具体的业务逻辑</li></ul><h2 id="互联网架构模式"><a href="#互联网架构模式" class="headerlink" title="互联网架构模式"></a>互联网架构模式</h2><p><img src="/img/architecture/architecture_5.png" alt="architecture_5"></p><h3 id="“存储层”技术"><a href="#“存储层”技术" class="headerlink" title="“存储层”技术"></a>“存储层”技术</h3><ul><li>SQL存储：以对业务透明的形式提供资源分配、数据备份、迁移、容灾、读写分离、分库分表等一系列服务</li><li>NoSQL存储：存储复杂数据、高性能</li><li>小文件存储（图片等）：HBase、Hadoop</li><li>大文件存储（业务大数据如视频等、海量日志数据）：HBase、HDFS</li></ul><h3 id="“开发层”技术"><a href="#“开发层”技术" class="headerlink" title="“开发层”技术"></a>“开发层”技术</h3><ul><li>开发框架：SSH、SpringMVC等（只是负责完成业务功能的开发）</li><li>Web服务器：Tomcat、Nginx等（真正能够运行起来给用户提供服务）</li><li>容器：Docker等（一个虚拟化或者容器技术）</li></ul><h3 id="“服务层”技术"><a href="#“服务层”技术" class="headerlink" title="“服务层”技术"></a>“服务层”技术</h3><ul><li>配置中心：集中管理各个系统的配置</li><li>服务中心：解决跨系统依赖的“配置”和“调度”问题</li><li>消息队列：为了实现这种跨系统异步通知的中间件系统</li></ul><h3 id="“网络层”技术"><a href="#“网络层”技术" class="headerlink" title="“网络层”技术"></a>“网络层”技术</h3><ul><li>负载均衡（将请求均衡地分配到多个系统上）<ul><li>DNS：实现地理级别的均衡</li><li>Nginx（7层）、LVS（4层）、F5（4层）：用于同一地点内机器级别的负载均衡</li><li>CDN：将内容缓存在离用户最近的地方，用户访问的是缓存的内容（“以空间换时间”）</li></ul></li><li>多机房（主要目标是灾备）<ul><li>同城多机房</li><li>跨城多机房</li><li>跨国多机房</li></ul></li><li>多中心（以多机房为前提，要求每个中心都同时对外提供服务，且业务能够自动在多中心之间切换，故障后不需人工干预自动恢复）</li></ul><h3 id="“用户层”和“业务层”技术"><a href="#“用户层”和“业务层”技术" class="headerlink" title="“用户层”和“业务层”技术"></a>“用户层”和“业务层”技术</h3><ul><li>用户管理（单点登录，授权登陆）</li><li>消息推送（分为短信、邮件、站内信、App推送）</li><li>存储云、图片云（买云服务最快最经济）</li></ul><h3 id="“平台”技术"><a href="#“平台”技术" class="headerlink" title="“平台”技术"></a>“平台”技术</h3><ul><li>运维平台：配置、部署、监控、应急<ul><li>标准化（制定运维标准，规范配置管理、部署流程、监控指标、应急能力等）</li><li>平台化（在运维标准化的基础上，将运维的相关操作都集成到运维平台中）</li><li>自动化（将重复操作固化下来，由系统自动完成）</li><li>可视化（为了提升数据查看效率）</li></ul></li><li>测试平台：单元测试、集成测试、接口测试、性能测试<ul><li>用例管理（为了能够重复执行这些测试用例，测试平台需要将用例管理起来）</li><li>资源管理（具体的运行环境：硬件、软件、业务系统）</li><li>任务管理（将测试用例分配到具体的资源上执行，跟踪任务的执行情况）</li><li>数据管理（测试完成后，记录各种相关的数据）</li></ul></li><li>数据平台<ul><li>数据管理（数据采集、数据存储、数据访问和数据安全）</li><li>数据分析（数据统计、数据挖掘、机器学习、深度学习）</li><li>数据应用（包括在线业务如推荐、广告等，也包括离线业务如报表等）</li></ul></li><li>管理平台<ul><li>身份认证</li><li>权限管理</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;架构基础&quot;&gt;&lt;a href=&quot;#架构基础&quot; class=&quot;headerlink&quot; title=&quot;架构基础&quot;&gt;&lt;/a&gt;架构基础&lt;/h2&gt;&lt;h3 id=&quot;架构概念&quot;&gt;&lt;a href=&quot;#架构概念&quot; class=&quot;headerlink&quot; title=&quot;架构概念&quot;&gt;&lt;/a&gt;架构概念&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;软件架构指软件系统的顶层结构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;子系统：子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中一部分&lt;/li&gt;
&lt;li&gt;框架：关注的是“规范”（Framework），如Spring MVC框架&lt;/li&gt;
&lt;li&gt;架构：关注的是“结构”（Architecture）&lt;/li&gt;
&lt;li&gt;模块：逻辑的角度来拆分系统后，得到的单元就是“模块”，主要目的是职责分离&lt;/li&gt;
&lt;li&gt;组件：从物理的角度来拆分系统后，得到的单元就是“组件”，主要目的是单元复用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一句话总结：架构是顶层设计；框架是面向编程或配置的半成品；组件是从技术维度上的复用；模块是从业务维度上职责的划分；系统是相互协同可运行的实体。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/architecture/architecture_1.png&quot; alt=&quot;architecture_1&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学架构" scheme="http://guoyanlei.top/categories/%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="架构" scheme="http://guoyanlei.top/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>kafka手动修改副本数ReplicationFactor</title>
    <link href="http://guoyanlei.top/2018/09/24/2018092401-kafka%E6%89%8B%E5%8A%A8%E4%BF%AE%E6%94%B9%E5%89%AF%E6%9C%AC%E6%95%B0ReplicationFactor/"/>
    <id>http://guoyanlei.top/2018/09/24/2018092401-kafka手动修改副本数ReplicationFactor/</id>
    <published>2018-09-24T07:30:00.000Z</published>
    <updated>2018-09-25T09:33:12.299Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kafka手动修改副本数ReplicationFactor"><a href="#kafka手动修改副本数ReplicationFactor" class="headerlink" title="kafka手动修改副本数ReplicationFactor"></a>kafka手动修改副本数ReplicationFactor</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>上周对kafka集群添加了一个broker（104），然后又删掉了，但是这个被删掉的broker一直存在于一个topic的ReplicationFactor中。</p><p>在kafka-manager中的表现是：</p><p><img src="/img/kafka/kafka-rep-factor-1.png" alt="kafka-rep-factor-1"></p><p>使用如下命令查看这个topic</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;kafka-topics.sh --zookeeper node101.zk.dmp.dmp.com:2181,node102.zk.dmp.dmp.com:2181,node103.zk.dmp.dmp.com:2181 --describe --topic pv-event</span><br></pre></td></tr></table></figure><p><img src="/img/kafka/kafka-rep-factor-2.png" alt="kafka-rep-factor-2"></p><a id="more"></a><h4 id="尝试"><a href="#尝试" class="headerlink" title="尝试"></a>尝试</h4><p>在kafka-manager中（Manual Partition Assignments）进行手动修改，发现并不能实现，如图所示：</p><p><img src="/img/kafka/kafka-rep-factor-3.png" alt="kafka-rep-factor-3"></p><p>因此只能选择其他方案，经搜索发现好多是增加副本数的方法，但是不确定减少副本数是不是生效。</p><p><a href="https://www.iteblog.com/archives/1614.html">增加副本参考方案</a></p><p>将验证参考文章的方法是可以的，特此总结如下。</p><h4 id="解决方案：借助配置文件手动修改副本数"><a href="#解决方案：借助配置文件手动修改副本数" class="headerlink" title="解决方案：借助配置文件手动修改副本数"></a>解决方案：借助配置文件手动修改副本数</h4><p>首先，新建一个json配置文件replication.json</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;: 1, </span><br><span class="line">    &quot;partitions&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;: &quot;pv-event&quot;, </span><br><span class="line">            &quot;partition&quot;: 0, </span><br><span class="line">            &quot;replicas&quot;: [</span><br><span class="line">                103,102,101</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;: &quot;pv-event&quot;, </span><br><span class="line">            &quot;partition&quot;: 1, </span><br><span class="line">            &quot;replicas&quot;: [</span><br><span class="line">                102,103,101</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;: &quot;pv-event&quot;, </span><br><span class="line">            &quot;partition&quot;: 2, </span><br><span class="line">            &quot;replicas&quot;: [</span><br><span class="line">                103,102,101</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        ... ...</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;: &quot;pv-event&quot;, </span><br><span class="line">            &quot;partition&quot;: 17, </span><br><span class="line">            &quot;replicas&quot;: [</span><br><span class="line">                103,104,102,101</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其次，使用kafka-reassign-partitions.sh工具来执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;kafka-reassign-partitions.sh --zookeeper node101.zk.dmp.dmp.com:2181,node102.zk.dmp.dmp.com:2181,node103.zk.dmp.dmp.com:2181 --reassignment-json-file replication.json --execute</span><br><span class="line"></span><br><span class="line"># 运行过程中可以通过以下命令查看</span><br><span class="line">bin&#x2F;kafka-reassign-partitions.sh --zookeeper node101.zk.dmp.dmp.com:2181,node102.zk.dmp.dmp.com:2181,node103.zk.dmp.dmp.com:2181 --reassignment-json-file replication.json --verify</span><br></pre></td></tr></table></figure><p>运行过程：</p><p><img src="/img/kafka/kafka-rep-factor-4.png" alt="kafka-rep-factor-4"></p><p>运行结果：</p><p><img src="/img/kafka/kafka-rep-factor-5.png" alt="kafka-rep-factor-5"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;kafka手动修改副本数ReplicationFactor&quot;&gt;&lt;a href=&quot;#kafka手动修改副本数ReplicationFactor&quot; class=&quot;headerlink&quot; title=&quot;kafka手动修改副本数ReplicationFactor&quot;&gt;&lt;/a&gt;kafka手动修改副本数ReplicationFactor&lt;/h3&gt;&lt;h4 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h4&gt;&lt;p&gt;上周对kafka集群添加了一个broker（104），然后又删掉了，但是这个被删掉的broker一直存在于一个topic的ReplicationFactor中。&lt;/p&gt;
&lt;p&gt;在kafka-manager中的表现是：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/kafka/kafka-rep-factor-1.png&quot; alt=&quot;kafka-rep-factor-1&quot;&gt;&lt;/p&gt;
&lt;p&gt;使用如下命令查看这个topic&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;.&amp;#x2F;kafka-topics.sh --zookeeper node101.zk.dmp.dmp.com:2181,node102.zk.dmp.dmp.com:2181,node103.zk.dmp.dmp.com:2181 --describe --topic pv-event&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/img/kafka/kafka-rep-factor-2.png&quot; alt=&quot;kafka-rep-factor-2&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://guoyanlei.top/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://guoyanlei.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka在zk中的目录结构</title>
    <link href="http://guoyanlei.top/2018/09/20/2018092001-kafka%E5%9C%A8zk%E4%B8%AD%E7%9A%84%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/"/>
    <id>http://guoyanlei.top/2018/09/20/2018092001-kafka在zk中的目录结构/</id>
    <published>2018-09-20T07:30:00.000Z</published>
    <updated>2018-09-21T06:49:59.543Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kafka在zk中的目录结构"><a href="#kafka在zk中的目录结构" class="headerlink" title="kafka在zk中的目录结构"></a>kafka在zk中的目录结构</h3><p><img src="/img/kafka/kafka-in-zk.png" alt="kafka-in-zk"></p><h4 id="brokers"><a href="#brokers" class="headerlink" title="/brokers"></a>/brokers</h4><p>当一个broker启动时,会向zookeeper注册自己持有的topic和partitions信息</p><h5 id="brokers-ids"><a href="#brokers-ids" class="headerlink" title="/brokers/ids"></a>/brokers/ids</h5><p>每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复)，此节点为临时znode(EPHEMERAL)</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&gt; ls &#x2F;brokers&#x2F;ids&#x2F;101</span><br><span class="line">[101, 102, 103]</span><br><span class="line"></span><br><span class="line">&gt; get &#x2F;brokers&#x2F;ids&#x2F;101</span><br><span class="line">&#123;</span><br><span class="line">&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,</span><br><span class="line">&quot;endpoints&quot;:[&quot;PLAINTEXT:&#x2F;&#x2F;node104.kafka.bigdata.dmp.com:9092&quot;],</span><br><span class="line">&quot;jmx_port&quot;:8999,&#x2F;&#x2F;jmx端口号</span><br><span class="line">&quot;host&quot;:&quot;node104.kafka.bigdata.dmp.com&quot;,  &#x2F;&#x2F;主机名或ip地址</span><br><span class="line">&quot;timestamp&quot;:&quot;1537425599417&quot;,  &#x2F;&#x2F;broker初始启动时的时间戳</span><br><span class="line">&quot;port&quot;:9092,   &#x2F;&#x2F;broker的服务端端口号，由server.properties中参数port确定</span><br><span class="line">&quot;version&quot;:4    &#x2F;&#x2F;版本编号默认为1</span><br><span class="line">&#125;</span><br><span class="line">cZxid &#x3D; 0x7183581d4</span><br><span class="line">ctime &#x3D; Thu Sep 20 14:39:59 CST 2018</span><br><span class="line">mZxid &#x3D; 0x7183581d4</span><br><span class="line">mtime &#x3D; Thu Sep 20 14:39:59 CST 2018</span><br><span class="line">pZxid &#x3D; 0x7183581d4</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x36324802b64f5d0</span><br><span class="line">dataLength &#x3D; 230</span><br><span class="line">numChildren &#x3D; 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="brokers-topics"><a href="#brokers-topics" class="headerlink" title="/brokers/topics"></a>/brokers/topics</h5><p>包含各topic的partition状态信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&gt; ls &#x2F;brokers&#x2F;topics  </span><br><span class="line">[pv-event, pv-event-nginx-log, gdt-request, leopard-module-stats-app, wolves-event, ad-track-nginx-log, ...]</span><br><span class="line"></span><br><span class="line">&gt; ls &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log</span><br><span class="line">[partitions]</span><br><span class="line"></span><br><span class="line">&gt; get &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:</span><br><span class="line">&#123;</span><br><span class="line">&quot;8&quot;:[103,101,102],  &#x2F;&#x2F;同步副本组brokerId列表(ISR)</span><br><span class="line">&quot;4&quot;:[102,101,103],&quot;11&quot;:[103,102,101],&quot;9&quot;:[101,103,102],&quot;5&quot;:[103,102,101],&quot;10&quot;:[102,101,103],&quot;6&quot;:[101,102,103],&quot;1&quot;:[102,103,101],&quot;0&quot;:[101,102,103],&quot;2&quot;:[103,101,102],&quot;7&quot;:[102,103,101],&quot;3&quot;:[101,103,102]</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">cZxid &#x3D; 0x200000143</span><br><span class="line">ctime &#x3D; Thu Oct 12 16:07:33 CST 2017</span><br><span class="line">mZxid &#x3D; 0x71306a4ee</span><br><span class="line">mtime &#x3D; Sun Aug 26 03:25:57 CST 2018</span><br><span class="line">pZxid &#x3D; 0x200000146</span><br><span class="line">cversion &#x3D; 1</span><br><span class="line">dataVersion &#x3D; 13843</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 246</span><br><span class="line">numChildren &#x3D; 1</span><br><span class="line"></span><br><span class="line">&gt; ls &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log&#x2F;partitions</span><br><span class="line">[0, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span><br><span class="line"></span><br><span class="line">&gt; ls &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log&#x2F;partitions&#x2F;0</span><br><span class="line">[state]</span><br><span class="line"></span><br><span class="line">&gt; get &#x2F;brokers&#x2F;topics&#x2F;pv-event-nginx-log&#x2F;partitions&#x2F;0&#x2F;state</span><br><span class="line">&#123;</span><br><span class="line">&quot;controller_epoch&quot;:28,&#x2F;&#x2F;表示kafka集群中的中央控制器选举次数</span><br><span class="line">&quot;leader&quot;:101,&#x2F;&#x2F;该partition选举leader的brokerId</span><br><span class="line">&quot;version&quot;:1,&#x2F;&#x2F;版本编号默认为1,</span><br><span class="line">&quot;leader_epoch&quot;:382,&#x2F;&#x2F;该partition leader选举次数</span><br><span class="line">&quot;isr&quot;:[101,103,102]     &#x2F;&#x2F;[同步副本组brokerId列表]</span><br><span class="line">&#125;</span><br><span class="line">cZxid &#x3D; 0x200000154</span><br><span class="line">ctime &#x3D; Thu Oct 12 16:07:33 CST 2017</span><br><span class="line">mZxid &#x3D; 0x7183c9936</span><br><span class="line">mtime &#x3D; Thu Sep 20 17:49:19 CST 2018</span><br><span class="line">pZxid &#x3D; 0x200000154</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 1426</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 87</span><br><span class="line">numChildren &#x3D; 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="brokers-seqid"><a href="#brokers-seqid" class="headerlink" title="/brokers/seqid"></a>/brokers/seqid</h5><p>该目录的作用是帮助kafka自动生成broker.id的。</p><p>自动生成broker.id的原理是先往/brokers/seqid节点中写入一个空字符串，然后获取返回的Stat信息中的version的值，然后将version的值和reserved.broker.max.id参数配置的值相加可得。之所以是先往节点中写入数据再获取Stat信息，这样可以确保返回的version值大于0，进而就可以确保生成的broker.id值大于reserved.broker.max.id参数配置的值，符合非自动生成的broker.id的值在[0, reserved.broker.max.id]区间的设定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; get &#x2F;brokers&#x2F;seqid</span><br><span class="line">null</span><br><span class="line">cZxid &#x3D; 0x20000000f</span><br><span class="line">ctime &#x3D; Thu Oct 12 15:35:21 CST 2017</span><br><span class="line">mZxid &#x3D; 0x20000000f</span><br><span class="line">mtime &#x3D; Thu Oct 12 15:35:21 CST 2017</span><br><span class="line">pZxid &#x3D; 0x20000000f</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 0  &#x2F;&#x2F;基于此version的值</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 0</span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure><h4 id="consumers"><a href="#consumers" class="headerlink" title="/consumers"></a>/consumers</h4><p>每个consumer都有一个唯一的ID，此id用来标记消费者信息.</p><p>该目录下仅展示使用zk进行消费的consumers，如果之间指定kafka节点进行消费，不会在此展示</p><h5 id="consumers-groupId-ids"><a href="#consumers-groupId-ids" class="headerlink" title="/consumers/{groupId}/ids"></a>/consumers/{groupId}/ids</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt; ls &#x2F;consumers</span><br><span class="line">[console-consumer-84155, console-consumer-32194, wolves_report, console-consumer-9761, wolves_v2_gdt, console-consumer-63530, wolves, wolves_feedback, wolves_kuaishou, console-consumer-62629, ftrl1, console-consumer-56068, wolves_tuia]</span><br><span class="line"></span><br><span class="line">&gt; ls &#x2F;consumers&#x2F;wolves_report</span><br><span class="line">[ids, owners, offsets]</span><br><span class="line"></span><br><span class="line">&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;ids</span><br><span class="line">[wolves_report_node1.tc.wolves.dmp.com-1536837975646-39504764, wolves_report_node1.tc.wolves.dmp.com-1536838003051-182cc752,...]</span><br><span class="line"></span><br><span class="line">&gt; get &#x2F;consumers&#x2F;wolves_report&#x2F;ids&#x2F;wolves_report_node1.tc.wolves.dmp.com-1536837975646-39504764</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;:1,   &#x2F;&#x2F;版本编号，默认为1</span><br><span class="line">&quot;subscription&quot;:&#x2F;&#x2F;订阅topic列表</span><br><span class="line">&#123;</span><br><span class="line">&quot;wolves-event&quot;:3&#x2F;&#x2F;consumer中topic消费者线程数</span><br><span class="line">&#125;,</span><br><span class="line">&quot;pattern&quot;:&quot;static&quot;,</span><br><span class="line">&quot;timestamp&quot;:&quot;1537128878487&quot;  &#x2F;&#x2F;consumer启动时的时间戳</span><br><span class="line">&#125;</span><br><span class="line">cZxid &#x3D; 0x717782b21</span><br><span class="line">ctime &#x3D; Mon Sep 17 04:14:38 CST 2018</span><br><span class="line">mZxid &#x3D; 0x717782b21</span><br><span class="line">mtime &#x3D; Mon Sep 17 04:14:38 CST 2018</span><br><span class="line">pZxid &#x3D; 0x717782b21</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x36324802b64ea62</span><br><span class="line">dataLength &#x3D; 94</span><br><span class="line">numChildren &#x3D; 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="consumers-groupId-owner"><a href="#consumers-groupId-owner" class="headerlink" title="/consumers/{groupId}/owner"></a>/consumers/{groupId}/owner</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;owners                                                           </span><br><span class="line">[wolves-event]   &#x2F;&#x2F; topic</span><br><span class="line"></span><br><span class="line">&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;owners&#x2F;wolves-event</span><br><span class="line">[0, 1, 2]&#x2F;&#x2F; partitionId</span><br><span class="line"></span><br><span class="line">&gt; get &#x2F;consumers&#x2F;wolves_report&#x2F;owners&#x2F;wolves-event&#x2F;0</span><br><span class="line">wolves_report_node1.tc.wolves.dmp.com-1536837527210-1310d8f9-0</span><br><span class="line">cZxid &#x3D; 0x717782ba9</span><br><span class="line">ctime &#x3D; Mon Sep 17 04:14:40 CST 2018</span><br><span class="line">mZxid &#x3D; 0x717782ba9</span><br><span class="line">mtime &#x3D; Mon Sep 17 04:14:40 CST 2018</span><br><span class="line">pZxid &#x3D; 0x717782ba9</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x26324802b69ea62</span><br><span class="line">dataLength &#x3D; 62</span><br><span class="line">numChildren &#x3D; 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="consumers-groupId-offset"><a href="#consumers-groupId-offset" class="headerlink" title="/consumers/{groupId}/offset"></a>/consumers/{groupId}/offset</h5><p>用来跟踪每个consumer目前所消费的partition中最大的offset</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;offsets               </span><br><span class="line">[wolves-event] &#x2F;&#x2F; topic</span><br><span class="line"></span><br><span class="line">&gt; ls &#x2F;consumers&#x2F;wolves_report&#x2F;offsets&#x2F;wolves-event</span><br><span class="line">[0, 1, 2] &#x2F;&#x2F; partitionId</span><br><span class="line"></span><br><span class="line">&gt; get &#x2F;consumers&#x2F;wolves_report&#x2F;offsets&#x2F;wolves-event&#x2F;0</span><br><span class="line">48800</span><br><span class="line">cZxid &#x3D; 0x200e97e36</span><br><span class="line">ctime &#x3D; Thu Nov 23 17:22:10 CST 2017</span><br><span class="line">mZxid &#x3D; 0x718665858</span><br><span class="line">mtime &#x3D; Fri Sep 21 12:02:39 CST 2018</span><br><span class="line">pZxid &#x3D; 0x200e97e36</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 11910567</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 5</span><br><span class="line">numChildren &#x3D; 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="admin"><a href="#admin" class="headerlink" title="/admin"></a>/admin</h4><h5 id="admin-reassign-partitions"><a href="#admin-reassign-partitions" class="headerlink" title="/admin/reassign_partitions"></a>/admin/reassign_partitions</h5><p>用以partitions重分区，reassign结束后会删除该目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ls &#x2F;admin&#x2F;reassign_partitions</span><br><span class="line">[]</span><br></pre></td></tr></table></figure><h5 id="admin-preferred-replica-election"><a href="#admin-preferred-replica-election" class="headerlink" title="/admin/preferred_replica_election"></a>/admin/preferred_replica_election</h5><p>用以partitions各副本leader选举，replica election结束后会删除该目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ls &#x2F;admin&#x2F;reassign_partitions</span><br><span class="line">[]</span><br></pre></td></tr></table></figure><h5 id="admin-delete-topics"><a href="#admin-delete-topics" class="headerlink" title="/admin/delete_topics"></a>/admin/delete_topics</h5><p>管理已删除的topics，broker启动时检查并确保存在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ls &#x2F;admin&#x2F;delete_topics</span><br><span class="line">[]</span><br></pre></td></tr></table></figure><h4 id="controller"><a href="#controller" class="headerlink" title="/controller"></a>/controller</h4><p>存储center controller中央控制器所在kafka broker的信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; get &#x2F;controller</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;:1,&#x2F;&#x2F;版本编号默认为1</span><br><span class="line">&quot;brokerid&quot;:101,&#x2F;&#x2F;broker唯一编号</span><br><span class="line">&quot;timestamp&quot;:&quot;1537425633921&quot;&#x2F;&#x2F;broker中央控制器变更时的时间戳</span><br><span class="line">&#125;</span><br><span class="line">cZxid &#x3D; 0x7183583bd</span><br><span class="line">ctime &#x3D; Thu Sep 20 14:40:33 CST 2018</span><br><span class="line">mZxid &#x3D; 0x7183583bd</span><br><span class="line">mtime &#x3D; Thu Sep 20 14:40:33 CST 2018</span><br><span class="line">pZxid &#x3D; 0x7183583bd</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x36324802b64f5d0</span><br><span class="line">dataLength &#x3D; 56</span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure><h4 id="controller-epoch"><a href="#controller-epoch" class="headerlink" title="/controller_epoch"></a>/controller_epoch</h4><p>此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1; </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; get &#x2F;controller_epoch</span><br><span class="line">28</span><br><span class="line">cZxid &#x3D; 0x200000017</span><br><span class="line">ctime &#x3D; Thu Oct 12 15:35:21 CST 2017</span><br><span class="line">mZxid &#x3D; 0x7183583be</span><br><span class="line">mtime &#x3D; Thu Sep 20 14:40:33 CST 2018</span><br><span class="line">pZxid &#x3D; 0x200000017</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 27</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 2</span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure><h4 id="config"><a href="#config" class="headerlink" title="/config"></a>/config</h4><ul><li>/config/changes  broker启动时检查并确保存在，所有broker全程监控child change</li><li>/config/clients  broker启动时检查并确保存在</li><li>/config/topics   broker启动时检查并确保存在</li></ul><h3 id="zookeeper操作命令"><a href="#zookeeper操作命令" class="headerlink" title="zookeeper操作命令"></a>zookeeper操作命令</h3><p>在确保zookeeper服务启动状态下，通过 bin/zkCli.sh -server xxx:2181 连接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. 显示根目录下、文件： ls &#x2F;  使用 ls 命令来查看当前 ZooKeeper 中所包含的内容</span><br><span class="line">2. 显示根目录下、文件： ls2 &#x2F; 查看当前节点数据并能看到更新次数等数据</span><br><span class="line">3. 创建文件，并设置初始内容： create &#x2F;zk &quot;test&quot; 创建一个新的 znode节点“ zk ”以及与它关联的字符串</span><br><span class="line">4. 获取文件内容： get &#x2F;zk 确认 znode 是否包含我们所创建的字符串</span><br><span class="line">5. 修改文件内容： set &#x2F;zk &quot;zkbak&quot; 对 zk 所关联的字符串进行设置</span><br><span class="line">6. 删除文件： delete &#x2F;zk 将刚才创建的 znode 删除</span><br><span class="line">7. 退出客户端： quit</span><br><span class="line">8. 帮助命令： help </span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;kafka在zk中的目录结构&quot;&gt;&lt;a href=&quot;#kafka在zk中的目录结构&quot; class=&quot;headerlink&quot; title=&quot;kafka在zk中的目录结构&quot;&gt;&lt;/a&gt;kafka在zk中的目录结构&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/img/kafka/kafka-in-zk.png&quot; alt=&quot;kafka-in-zk&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;brokers&quot;&gt;&lt;a href=&quot;#brokers&quot; class=&quot;headerlink&quot; title=&quot;/brokers&quot;&gt;&lt;/a&gt;/brokers&lt;/h4&gt;&lt;p&gt;当一个broker启动时,会向zookeeper注册自己持有的topic和partitions信息&lt;/p&gt;
&lt;h5 id=&quot;brokers-ids&quot;&gt;&lt;a href=&quot;#brokers-ids&quot; class=&quot;headerlink&quot; title=&quot;/brokers/ids&quot;&gt;&lt;/a&gt;/brokers/ids&lt;/h5&gt;&lt;p&gt;每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复)，此节点为临时znode(EPHEMERAL)&lt;/p&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://guoyanlei.top/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://guoyanlei.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>CDH离线安装记录及常见错误处理</title>
    <link href="http://guoyanlei.top/2018/09/11/2018091201-CDH%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/"/>
    <id>http://guoyanlei.top/2018/09/11/2018091201-CDH离线安装记录及常见错误处理/</id>
    <published>2018-09-11T07:30:00.000Z</published>
    <updated>2020-11-01T13:47:33.809Z</updated>
    
    <content type="html"><![CDATA[<h3 id="准备安装包"><a href="#准备安装包" class="headerlink" title="准备安装包"></a>准备安装包</h3><p><a href="http://archive.cloudera.com/cm5/cm/5/">cloudera manager下载地址</a></p><p><a href="http://archive.cloudera.com/cdh5/parcels/5.12.2/">CDH下载地址</a>选择自己合适的版本</p><p><a href="http://archive.cloudera.com/cdh5/parcels/5.12.2/manifest.json">manifest.json下载地址</a></p><a id="more"></a><h3 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h3><p>对于集群中搜索节点都需要做：</p><ol><li>修改主机名称</li><li>关闭防火墙</li></ol><h3 id="配置免密码登陆"><a href="#配置免密码登陆" class="headerlink" title="配置免密码登陆"></a>配置免密码登陆</h3><p>对于集群中所有节点，任意两个节点实现免密码登陆。并且开启ssh服务。</p><h3 id="安装jdk"><a href="#安装jdk" class="headerlink" title="安装jdk"></a>安装jdk</h3><p>对于集群中节点安装jdk，可以自己安装，也可以通过cloudera manager安装。安装完成之后，配置<code>JAVA_HOME</code>。对于线上，建议自己安装。</p><h3 id="安装和配置NTP服务"><a href="#安装和配置NTP服务" class="headerlink" title="安装和配置NTP服务"></a>安装和配置NTP服务</h3><p>cloudera manager通过ntp服务，保持集群内的时钟同步。时钟偏差会应用服务，特别是kudu，因为kudu的事务需要以来时钟一致。</p><p>安装NTP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ntp</span><br></pre></td></tr></table></figure><p>集群内，建议在集群内做一个NTP服务器，作为优先级最高的NTP服务器，其它的作为备用。</p><p>手动同步可以如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate -d NTP服务器的host或者IP</span><br></pre></td></tr></table></figure><p>可以通过如下命令查看NTP的统计</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpstat</span><br></pre></td></tr></table></figure><p>还可以通过ntpq查看ntp服务同步各个服务器的情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpq -p</span><br></pre></td></tr></table></figure><p>若系统是CentOS7的同步时钟的方式建议使用chrony，<a href="http://guoyanlei.top/2018/05/30/2018053001-CentOS7.2-%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7ntp&chrony/">将ntp切换为chrony参考之前的文章</a></p><h3 id="准备MySQL"><a href="#准备MySQL" class="headerlink" title="准备MySQL"></a>准备MySQL</h3><p>cloudera-manager、hive、hue、oozie都需要MySQL来存储元数据。所以需要提前准备好一台MySQL。</p><p>对于不同不同的服务，采用不同的kudu，不同的用户，进行授权。</p><h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><p>通过yum安装cloudera-manager的依赖，命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">yum -y install chkconfig</span><br><span class="line">yum -y install bind-utils</span><br><span class="line">yum -y install psmisc</span><br><span class="line">yum -y install libxslt</span><br><span class="line">yum -y install zlib</span><br><span class="line">yum -y install sqlite</span><br><span class="line">yum -y install cyrus-sasl-plain</span><br><span class="line">yum -y install cyrus-sasl-gssapi</span><br><span class="line">yum -y install fuse</span><br><span class="line">yum -y install portmap</span><br><span class="line">yum -y install fuse-libs</span><br><span class="line">yum -y install redhat-lsb</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="安装cloudera-manager-server"><a href="#安装cloudera-manager-server" class="headerlink" title="安装cloudera manager server"></a>安装cloudera manager server</h3><ol><li><p>在cloudera manager server所在节点创建cloudera-manager安装目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;dmp&#x2F;cloudera-manager</span><br></pre></td></tr></table></figure></li><li><p>将下载好的cloudera-manager安装包cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz解压放在改目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz -C &#x2F;data&#x2F;dmp&#x2F;cloudera-manager</span><br></pre></td></tr></table></figure><ol start="3"><li>创建<code>cloudera-scm</code>用户</li></ol><p>Cloudera管理器服务器和托管服务被配置为在默认情况下使用用户帐户Cloudera-scm，创建具有这个名称的用户是最简单的方法。创建用户，在安装完成后自动使用。</p><p>执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd --system --home&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;run&#x2F;cloudera-scm-server&#x2F; --no-create-home --shell&#x3D;&#x2F;bin&#x2F;false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br></pre></td></tr></table></figure></li><li><p>下载MySQL驱动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd  &#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;share&#x2F;cmf&#x2F;lib</span><br><span class="line"></span><br><span class="line">wget http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;service&#x2F;local&#x2F;repositories&#x2F;hongkong-nexus&#x2F;content&#x2F;Mysql&#x2F;mysql-connector-java&#x2F;5.1.38&#x2F;mysql-connector-java-5.1.38.jar</span><br></pre></td></tr></table></figure></li><li><p>配置cloudera manager server的数据库</p><p>进入cloudera-manager安装目录下的<code>cm-5.12.2/share/cmf/schema/</code>目录中，找到<code>scm_prepare_database.sh</code>，这个脚本是执行cloudera-amanger数据库初始化的脚本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;share&#x2F;cmf&#x2F;schema&#x2F;</span><br><span class="line"></span><br><span class="line">.&#x2F;scm_prepare_database.sh mysql cm -h $&#123;MySQL节点的host或者ip&#125; -u$&#123;MySQL用户&#125; -p$&#123;MySQL密码&#125; --scm-host $&#123;cloudera manager server的ip或者host&#125; scm scm scm</span><br></pre></td></tr></table></figure><p>看到如下信息，就配置成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[main] DbCommandExecutor              INFO  Successfully connected to database.</span><br><span class="line">All done, your SCM database is configured correctly!</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scm_prepare_database.sh mysql cm -h &lt;hostName&gt; -u&lt;username&gt;  -p&lt;password&gt; --scm-host &lt;hostName&gt;  scm scm scm</span><br><span class="line"></span><br><span class="line">对应于：数据库类型  数据库 服务器 用户名 密码  –scm-host  Cloudera_Manager_Server 所在节点……</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>Manager节点上创建Parcel目录</p><p>(1). 创建目录<code>/data/dmp/cloudera/parcel-repo</code>，并且将该目录授权给cloudera-scm用户，执行命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcel-repo</span><br><span class="line"></span><br><span class="line">chown cloudera-scm:cloudera-scm &#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcel-repo</span><br></pre></td></tr></table></figure><p>(2) 将爱在好的parcel复制到<code>/data/dmp/cloudera/parcel-repo</code>目录下，parcel列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CDH-5.12.2-1.cdh5.12.2.p0.4-el7.parcel</span><br><span class="line">CDH-5.12.2-1.cdh5.12.2.p0.4-el7.parcel.sha</span><br><span class="line">manifest.json</span><br><span class="line">KUDU-1.4.0-1.cdh5.12.2.p0.8-el7.parcel</span><br><span class="line">KUDU-1.4.0-1.cdh5.12.2.p0.8-el7.parcel.sha</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意，需要将<code>CDH-5.12.2-1.cdh5.12.2.p0.4-el7.parcel.sha1</code>重命名为<code>CDH-5.12.2-1.cdh5.12.2.p0.4-el7.parcel.sha</code>，否则系统会重新下载。</p></li></ol><h3 id="安装cloudera-manager的agent"><a href="#安装cloudera-manager的agent" class="headerlink" title="安装cloudera-manager的agent"></a>安装cloudera-manager的agent</h3><p>对于通过cloudera-manager管理的主机，都需要安装agent，cloudera-manager的安装软件和监控都是通过agent完成的。所有加入的节点，都需要cloudera-manager的server节点保持通信，实现ssh免密码登陆。</p><p>头3步和cloudera manager server节点相同</p><ol><li><p>在cloudera manager server所在节点创建cloudera-manager安装目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;dmp&#x2F;cloudera-manager</span><br></pre></td></tr></table></figure></li><li><p>将下载好的cloudera-manager安装包cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz解压放在改目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz -C &#x2F;data&#x2F;dmp&#x2F;cloudera-manager</span><br></pre></td></tr></table></figure></li><li><p>创建<code>cloudera-scm</code>用户</p><p>Cloudera管理器服务器和托管服务被配置为在默认情况下使用用户帐户Cloudera-scm，创建具有这个名称的用户是最简单的方法。创建用户，在安装完成后自动使用。</p><p>执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd --system --home&#x3D;&#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;run&#x2F;cloudera-scm-server&#x2F; --no-create-home --shell&#x3D;&#x2F;bin&#x2F;false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br></pre></td></tr></table></figure></li><li><p>配置agent</p><p>在<code>/data/dmp/cloudera-manager/cm-5.12.2/etc/cloudera-scm-agent/config.ini</code>中可以pacel、各个服务的安装目录、cloudera-manager server的地址。</p></li></ol><ol start="5"><li><p>创建parcels目录</p><p>在agent节点上，创建<code>/data/dmp/cloudera/parcels</code>，并且将改目录用户组授权给cloudera-scm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels</span><br><span class="line"></span><br><span class="line">chown cloudera-scm:cloudera-scm &#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels</span><br></pre></td></tr></table></figure></li></ol><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><ol><li><p>在manager节点启动cloudera-scm-server服务。执行命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-server start</span><br></pre></td></tr></table></figure></li><li><p>在agent节点启动agent</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;data&#x2F;dmp&#x2F;cloudera-manager&#x2F;cm-5.12.2&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-agent start</span><br></pre></td></tr></table></figure></li></ol><h3 id="登陆cloudera-manager安装服务"><a href="#登陆cloudera-manager安装服务" class="headerlink" title="登陆cloudera manager安装服务"></a>登陆cloudera manager安装服务</h3><p>访问cloudera manager server节点的7180端口，这里的地址<code>http://cloudera-manager.etouch.cn/cmf/login</code></p><p>通过用户名和密码登陆，默认的用户名和密码都是admin。</p><p>安装提示一步一步进行配置，在安装会有机器监测，对于监测的问题，cloudera manager会有提示，提示如何修改。</p><h4 id="向集群添加安装新的节点agent"><a href="#向集群添加安装新的节点agent" class="headerlink" title="向集群添加安装新的节点agent"></a>向集群添加安装新的节点agent</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/dmp/cloudera-manager</span><br><span class="line">tar -zxvf cloudera-manager-centos7-cm5.12.2_x86_64.tar.gz -C /data/dmp/cloudera-manager</span><br><span class="line"></span><br><span class="line">useradd --system --home=/data/dmp/cloudera-manager/cm-5.12.2/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br><span class="line"></span><br><span class="line">cd /data/dmp/cloudera-manager/cm-5.12.2/etc/cloudera-scm-agent</span><br><span class="line">rm -f config.ini</span><br><span class="line"></span><br><span class="line">scp /data/dmp/cloudera-manager/cm-5.12.2/etc/cloudera-scm-agent/config.ini root@node104.bigdata.dmp.local.com:/data/dmp/cloudera-manager/cm-5.12.2/etc/cloudera-scm-agent/config.ini</span><br><span class="line"></span><br><span class="line">mkdir -p /data/dmp/cloudera/parcels</span><br><span class="line">chown cloudera-scm:cloudera-scm /data/dmp/cloudera/parcels</span><br></pre></td></tr></table></figure><h3 id="Cloudera-Manager安装常见错误和脚本"><a href="#Cloudera-Manager安装常见错误和脚本" class="headerlink" title="Cloudera Manager安装常见错误和脚本"></a>Cloudera Manager安装常见错误和脚本</h3><h4 id="1-防火墙和selinux关闭"><a href="#1-防火墙和selinux关闭" class="headerlink" title="1. 防火墙和selinux关闭"></a>1. 防火墙和selinux关闭</h4><ul><li>出现连接拒绝，用telnet测试相关端口，server：7180、agent：7182</li></ul><h4 id="2-CM-agent的日志在-data-dmp-log下面"><a href="#2-CM-agent的日志在-data-dmp-log下面" class="headerlink" title="2. CM agent的日志在/data/dmp/log下面"></a>2. CM agent的日志在/data/dmp/log下面</h4><ul><li>新装agent起不来，可能需要手动创建相关文件夹。见：<a href="http://community.cloudera.com/t5/Cloudera-Manager-Installation/Failed-to-receive-heartbeat-from-agent-CM-server-guid/m-p/64228">http://community.cloudera.com/t5/Cloudera-Manager-Installation/Failed-to-receive-heartbeat-from-agent-CM-server-guid/m-p/64228</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;准备安装包&quot;&gt;&lt;a href=&quot;#准备安装包&quot; class=&quot;headerlink&quot; title=&quot;准备安装包&quot;&gt;&lt;/a&gt;准备安装包&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://archive.cloudera.com/cm5/cm/5/&quot;&gt;cloudera manager下载地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://archive.cloudera.com/cdh5/parcels/5.12.2/&quot;&gt;CDH下载地址&lt;/a&gt;选择自己合适的版本&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://archive.cloudera.com/cdh5/parcels/5.12.2/manifest.json&quot;&gt;manifest.json下载地址&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="cloudera-manager" scheme="http://guoyanlei.top/categories/cloudera-manager/"/>
    
    
      <category term="cloudera-manager" scheme="http://guoyanlei.top/tags/cloudera-manager/"/>
    
  </entry>
  
  <entry>
    <title>CDH-HDFS误删HA的namenode后无法启动</title>
    <link href="http://guoyanlei.top/2018/08/10/2018081001-CDH-HDFS%E8%AF%AF%E5%88%A0HA%E7%9A%84namenode%E5%90%8E%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/"/>
    <id>http://guoyanlei.top/2018/08/10/2018081001-CDH-HDFS误删HA的namenode后无法启动/</id>
    <published>2018-08-10T07:30:00.000Z</published>
    <updated>2018-08-10T07:25:18.271Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>今天干了一件现在想还有点害怕的是，记录下。</p><p>新加了几天集群，想迁移一下Journal node，然后就顺手点了cloudera-manager上的角色迁移，在迁的过程中报错了，一系列操作后，不小心禁用了HA，禁用过程失败了，hdfs就无法启动了，报的错误如下图所示。随后又删除了3个journal node，添加了一个secondary namenode，hdfs的实例页面还是报同样的错误，重启hdfs也报这个错。突然不知道咋办有点慌了，唉。</p><a id="more"></a><p><img src="/img/cm/hdfs-ha-error.png" alt="hdfs-ha-error"></p><p>冷静了一段时间后，从网上找到了解决方案，还是暗自庆幸的，但是下次不该这么鲁莽了。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>1）删除了如下图所示的hdfs的配置文件。<br>因为把namenode 的ha删除之后，hdfs已经不再具有ha功能了，原来的ha设置就没有用了，但是配置上如果还存在，就会报错。</p><p><img src="/img/cm/hdfs-ha-error-conf.png" alt="hdfs-ha-error-conf"></p><p>2）删除journal node，添加secondary namenode<br>3）先重启name node，再重启data node，到此，问题已经得到解决。<br>4）最后启动secondaryNamenode</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;今天干了一件现在想还有点害怕的是，记录下。&lt;/p&gt;
&lt;p&gt;新加了几天集群，想迁移一下Journal node，然后就顺手点了cloudera-manager上的角色迁移，在迁的过程中报错了，一系列操作后，不小心禁用了HA，禁用过程失败了，hdfs就无法启动了，报的错误如下图所示。随后又删除了3个journal node，添加了一个secondary namenode，hdfs的实例页面还是报同样的错误，重启hdfs也报这个错。突然不知道咋办有点慌了，唉。&lt;/p&gt;
    
    </summary>
    
      <category term="cloudera-manager" scheme="http://guoyanlei.top/categories/cloudera-manager/"/>
    
    
      <category term="cloudera-manager" scheme="http://guoyanlei.top/tags/cloudera-manager/"/>
    
      <category term="hdfs" scheme="http://guoyanlei.top/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>PyCharm本地开发pyspark并提交远程执行</title>
    <link href="http://guoyanlei.top/2018/08/01/2018080201-PyCharm%E6%9C%AC%E5%9C%B0%E5%BC%80%E5%8F%91pyspark%E5%B9%B6%E6%8F%90%E4%BA%A4%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C/"/>
    <id>http://guoyanlei.top/2018/08/01/2018080201-PyCharm本地开发pyspark并提交远程执行/</id>
    <published>2018-08-01T07:30:00.000Z</published>
    <updated>2018-08-02T04:56:01.835Z</updated>
    
    <content type="html"><![CDATA[<p>最近在学习pyspark的开发，遇到些问题记录下。</p><p>我们在开发pyspark时经常需要进行测试，自己电脑上安装搭建一个spark环境代价有点高，目前有的同事在开发时，通常是开发完把代码贴出到本地测试集群进行测试，因此，能不能借助pycharm里的一个功能，连接本地测试集群的pyspark进行执行呢，经过一番搜索终于实现了这一个功能。</p><a id="more"></a><h3 id="新建带有Virtualenv的工程"><a href="#新建带有Virtualenv的工程" class="headerlink" title="新建带有Virtualenv的工程"></a>新建带有Virtualenv的工程</h3><p>Virtualenv是什么？</p><p>Python 的第三方包成千上万，在一个 Python 环境下开发时间越久、安装依赖越多，就越容易出现依赖包冲突的问题。为了解决这个问题，开发者们开发出了 virtualenv，可以搭建虚拟且独立的 Python 环境。这样就可以使每个项目环境与其他项目独立开来，保持环境的干净，解决包冲突问题。</p><p>下面我们先见一个project，在pycharm中默认的是Virtualenv管理环境，当然还有conda，功能和Virtualenv类似。</p><p><img src="/img/pycharm/new-project.png" alt="Virtualenv的工程"></p><h3 id="开发-amp-导入pyspark"><a href="#开发-amp-导入pyspark" class="headerlink" title="开发&amp;导入pyspark"></a>开发&amp;导入pyspark</h3><p>新建一个py文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark.sql import HiveContext</span><br><span class="line"></span><br><span class="line">APP_NAME &#x3D; &#39;APP_TEST&#39;</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(appName&#x3D;APP_NAME)</span><br><span class="line">sqlContext &#x3D; HiveContext(sc)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(&quot;show databases&quot;).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>引入未安装的包时，pycharm会提示安装，安装需借助pip安装，若为安装pip需先安装，安装完成后会在项目目录：venv/lib/python2.7/site-packages 中出现安装好的第三方库。</p><p>至此，我们就可以开发和使用pyspark中的一些文件了。</p><p>开发是方便，但是我们还想变开发变测试，由于本地并没有装大数据相关环境，因此，我们还需要做些配置，来远程提交我们的代码并执行测试。</p><h3 id="提交远程执行"><a href="#提交远程执行" class="headerlink" title="提交远程执行"></a>提交远程执行</h3><h4 id="配置远程project-interpreter（程序解释器）"><a href="#配置远程project-interpreter（程序解释器）" class="headerlink" title="配置远程project interpreter（程序解释器）"></a>配置远程project interpreter（程序解释器）</h4><p>打开pycharm的prefrences配置，从中找到我们项目的project interpreter。</p><p><img src="/img/pycharm/interpreter.png" alt="project interpreter"></p><p>图中展示的是我们本机中导入的程序解释器，我们还可以点击右上方配置按钮，添加远程的程序解释器。</p><p><img src="/img/pycharm/ssh-interpreter.png" alt="ssh-interpreter"></p><p>在配置中添加远程服务器的主机ip和用户、密码</p><p><img src="/img/pycharm/ssh-interpreter-2.png" alt="ssh-interpreter-2"></p><p>在最后一页，可以看到，远程服务器中python的位置。我们在执行程序时，pycharm会自动的将我们的最新代码提交的远程的目录下（/tmp/pycharm_project_237）</p><p>当我们切换到远程的project interpreter时，就会看到远程的一些库。我们在开发时，远程服务器缺少哪些库，就需要先到服务器上安装好那些库，之后才可以提交到远程执行，否则会报no module find。</p><p><img src="/img/pycharm/interpreter-2.png" alt="project interpreter remote"></p><h4 id="远程执行"><a href="#远程执行" class="headerlink" title="远程执行"></a>远程执行</h4><p>上面的配置完成后，就可以在当前文件的Edit Configuration中进行执行配置，在project interpreter选项中可以选择是使用本地的程序解释器还是远程程序解释器执行了。</p><p><img src="/img/pycharm/run-configuration.png" alt="Edit Configuration"></p><p>执行上面定义好的的程序。</p><p><img src="/img/pycharm/running.png" alt="Running"></p><p>可以看出，实际程序执行的是同步到远程服务器的代码。并借助ssh登录到远程服务器执行，并返回执行的结果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh:&#x2F;&#x2F;root@node102.bigdata.dmp.local.com:22&#x2F;usr&#x2F;bin&#x2F;python -u &#x2F;tmp&#x2F;pycharm_project_568&#x2F;suishen&#x2F;uid_to_mongo.py</span><br></pre></td></tr></table></figure><p>这样就很方便了，不需要每次边写代码，边复制粘贴进行测试了。</p><h3 id="CDH-spark在执行时遇到的问题"><a href="#CDH-spark在执行时遇到的问题" class="headerlink" title="CDH-spark在执行时遇到的问题"></a>CDH-spark在执行时遇到的问题</h3><p>由于服务器安装的是CDH版本的spark，因此，在执行pyspark程序时需要指定SPARK_HOME。</p><p>解决方案有很多种，可参考（<a href="https://blog.csdn.net/syani/article/details/72851425%EF%BC%89">https://blog.csdn.net/syani/article/details/72851425）</a></p><p>我们使用的是第二种，在代码中灵活的配置SPARK_HOME和其他的环境地址。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;] &#x3D; &quot;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.12.2-1.cdh5.12.2.p0.4&#x2F;lib&#x2F;spark&quot;</span><br><span class="line">sys.path.append(&quot;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.12.2-1.cdh5.12.2.p0.4&#x2F;lib&#x2F;spark&#x2F;python&quot;)</span><br><span class="line">sys.path.append(&quot;&#x2F;data&#x2F;dmp&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.12.2-1.cdh5.12.2.p0.4&#x2F;lib&#x2F;spark&#x2F;python&#x2F;lib&#x2F;py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在学习pyspark的开发，遇到些问题记录下。&lt;/p&gt;
&lt;p&gt;我们在开发pyspark时经常需要进行测试，自己电脑上安装搭建一个spark环境代价有点高，目前有的同事在开发时，通常是开发完把代码贴出到本地测试集群进行测试，因此，能不能借助pycharm里的一个功能，连接本地测试集群的pyspark进行执行呢，经过一番搜索终于实现了这一个功能。&lt;/p&gt;
    
    </summary>
    
      <category term="pyspark" scheme="http://guoyanlei.top/categories/pyspark/"/>
    
    
      <category term="pyspark" scheme="http://guoyanlei.top/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>分布式数据仓库在公司的应用与演变</title>
    <link href="http://guoyanlei.top/2018/07/18/2018071801-%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%9C%A8%E5%85%AC%E5%8F%B8%E7%9A%84%E5%BA%94%E7%94%A8%E4%B8%8E%E6%BC%94%E5%8F%98/"/>
    <id>http://guoyanlei.top/2018/07/18/2018071801-分布式数据仓库在公司的应用与演变/</id>
    <published>2018-07-18T07:30:00.000Z</published>
    <updated>2018-07-26T04:09:33.501Z</updated>
    
    <content type="html"><![CDATA[<p>本文只关注与分布式数据仓库在公司的使用和演变，不涉及任何技术细节。希望通过本文的总结，能让你对数据仓库在公司的应用与演变，以及相关技术栈有初步的认识。</p><a id="more"></a><h3 id="数据仓库生态"><a href="#数据仓库生态" class="headerlink" title="数据仓库生态"></a>数据仓库生态</h3><p><img src="/img/dw/dw-system.png" alt="数据仓库生态"></p><p>如图为公司内部数据仓库生态，主要包括了三部分：数据采集，数据聚合和数据应用</p><h4 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h4><ol><li><p>客户端用户行为日志采集，通过采集和分析用户行为日志，可以帮助运营团队合理化运营，商务团队智能化广告投放，数据挖掘团队精准化文章推荐，产品团队动态化把握版本迭代。</p></li><li><p>业务系统数据同步，主要同步有统计需求的业务数据和相关维度数据。业务数据，如微鲤看看金币流水，可定时将金币流水数据同步到数据仓库中，减轻每日频繁的统计对业务系统的影响，减少业务开发团队对数据统计需求的工作量。维度数据，如peacock投放的文章，广告等详情数据，同步这些数据是为了在基于条目统计时可更好的分析各维度下的文章或广告投放。</p></li><li><p>广告平台日志采集，为了更好的分析广告平台中广告投放ctr，广告请求响应效率，以及广告素材质量等。数据仓库主要采集了广告曝光、点击数据，请求第三方的广告素材数据，以及广告请求响应状态码、响应时间等数据。</p></li></ol><h4 id="数据聚合"><a href="#数据聚合" class="headerlink" title="数据聚合"></a>数据聚合</h4><p>数据聚合中，通过维度建模、数据分层、统计任务的定时调度，将客户端、业务系统、广告平台采集的数据进行规范化的聚合、统计。最终，生成面向各主题需求的统计结果，共数据应用层使用。</p><p>这里不在阐述维度建模、数据分层相关技术，可参考之前的文章。</p><h4 id="数据应用"><a href="#数据应用" class="headerlink" title="数据应用"></a>数据应用</h4><p>在数据应用层中，主要列举了公司内部正在依托于数据仓库而运行的各系统，下面简单介绍各系统的职能。</p><ul><li>DMP：数据报表平台。统计展示最频繁的统计需求结果，同时为其他平台提供数据接口</li><li>Peacock &amp; 好学：内容、广告投放系统。需借助DMP展示各投放内容的实时曝光、点击等统计</li><li>Leopard：正在研发的即席查询平台。将逐渐替代DMP，可在该平台即席查询任意组合条件的统计数据</li><li>ADX &amp; DSP：广告交易平台，借助数据仓库统计广告曝光、点击，请求响应等数据</li><li>Wolves：市场渠道质量评估平台，基于真实的用户行为数据，动态评估第三方推广渠道所带来的用户质量</li><li>Rooster：智能推送系统。借助数据仓库完成自定义的统计需求（即面向各维度的用户群进行推送），统计推送下发、到达和曝光结果。</li><li>用户画像 &amp; 蜂巢推荐：借助数据仓库可为各app用户建立自己的用户画像，并基于用户画像为用户提供个性化的内容推荐。</li><li>临时需求：当在各数据平台中无法查询的统计需求，可临时提交给数据仓库工程师，由其构建特殊的查询SQL，来获取统计结果。</li><li>邮件订阅：对于一些常规的，只是阶段性的需求，可以邮件订阅的方式来获取统计数据。</li></ul><h3 id="数据仓库技术架构-v1-0"><a href="#数据仓库技术架构-v1-0" class="headerlink" title="数据仓库技术架构-v1.0"></a>数据仓库技术架构-v1.0</h3><p><img src="/img/dw/technical-architecture-old.jpg" alt="数据仓库技术架构v1"></p><p>下面列举V1.0使用的数据仓库技术栈：</p><ul><li>Flume：分布式、可靠、高可用的海量日志收集的系统。支持各种各样的数据源（HTTP、log等Source），能将这些数据源的数据高效的收集，聚合、移动，最后存储到指定存储系统（Kafka or HDFS）中。图中是使用Flume去收集各日志采集服务器中用户行为日志数据。</li><li>Kafka：分布式的、基于发布/订阅的消息系统。可以将Flume收集到日志，按照不同的主题按序存储到不同的Topic队列中，然后多个Consumer可以同时去消费Topic中的数据。</li><li>Storm：分布式实时大数据处理系统。用以实时的消费Kafka中的数据，并做相关计算处理。可直接将数据存储到HDFS中，也可基于某些统计逻辑，将统计后到数据存储到HBase中。</li><li>HDFS：分布式文件系统，能提供高吞吐量的数据访问。</li><li>Hive：基于HDFS的一个数据仓库工具，可以将结构化的数据文件映射为数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。适合用来对一段时间内的数据进行分析查询。</li><li>HBase：分布式的、面向列的开源数据库，是一种Key/Value系统，其数据文件都存储在HDFS中，非常适合用来进行大数据的实时查询。</li><li>Phoenix：本质是用Java写的基于JDBC API操作HBase的开源SQL引擎。</li><li>Yarn：一个通用资源管理系统，可为上层应用提供统一的资源管理和调度。</li><li>Sqoop：用在Hive与传统关系型数据库间的数据传递，可以将一个关系型数据库中的数据导进到Hive中，也可以将Hive表中的数据导进到关系型数据库中。</li><li>Redis &amp; Mysql：与业务紧密相联的存储。可将Hive or HBase中统计的数据同步到这些业务库中。</li></ul><h3 id="数据仓库技术架构-v2-0"><a href="#数据仓库技术架构-v2-0" class="headerlink" title="数据仓库技术架构-v2.0"></a>数据仓库技术架构-v2.0</h3><p><img src="/img/dw/technical-architecture-new.jpg" alt="数据仓库技术架构v2"></p><p>下面列举V2.0新增的数据仓库技术栈：</p><ul><li>Logstash：一款强大的数据处理工具，可以实现数据传输，格式处理，格式化输出，还有强大的插件功能，常用于日志处理。它结合Elasticsearch和kibana可实现一套完整的日志分析系统，其中，ES进行存储、建立搜索索引，kibana调用ES接口进行数据可视化。</li><li>Spark：拥有DAG执行引擎，支持在内存中对数据进行迭代计算，适合大数据分析统计，实时数据处理，图计算及机器学习。</li><li>Spark-Streaming：将持续不断输入的数据流转换成多个batch分片，使用一批spark应用实例进行处理。</li><li>Hive on Spark：目的是把Spark作为Hive的一个计算引擎，将Hive的查询作为Spark的任务提交到Spark集群上进行计算。与SparkSQL结构类似，只是SQL引擎不同，但是计算引擎都是spark</li><li>Kudu：新一代面向实时分析的存储引擎，底层使用类似Parquet的存储结构，支持实时写入，实时更新，实时查询。扫描性能比Parquet略差。</li><li>Impala：全新的执行引擎（真正的MPP查询引擎），在执行SQL语句的时候，Impala不会把中间数据写入到磁盘，而是在内存中完成了所有的处理。使用Impala的时候，查询任务会马上执行而不是生产Mapreduce任务，这会节约大量的初始化时间。</li><li>DataX：阿里开源的，一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。</li><li>ES &amp; MongoDB：借助Hive的StorageHandler插件，可将hive的数据通过外部表的形式关联到ES or MongoDB中，可以实现双向读写。</li></ul><h3 id="即席查询系统"><a href="#即席查询系统" class="headerlink" title="即席查询系统"></a>即席查询系统</h3><p>即席查询是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。</p><h4 id="即席查询技术架构"><a href="#即席查询技术架构" class="headerlink" title="即席查询技术架构"></a>即席查询技术架构</h4><p>基于数据仓库中事实表的设计，我们的Ad-Hoc简单的架构如下图：</p><p><img src="/img/impala/ad-hoc-query.png" alt="Ad-Hoc架构"></p><p>架构比较简单，主要借助Impala + parquet去查询HDFS中的数据，并借助kudu接收实时写入的数据，</p><p>在T+1日后，会将kudu中实时写入的数据转存到HDFS中，并转换成parquet格式。</p><p>Parquet存储格式</p><ul><li>按列存储</li><li>可按时间分区</li><li>局部排序</li><li>适配多种计算框架</li><li>只支持批量写入，无法追加，无法实时写入</li></ul><p>kudu 新一代面向实时分析的存储引擎</p><ul><li>底层使用类似Parquet的存储结构</li><li>支持实时写入，实时更新，实时查询</li><li>扫描性能比Parquet略差</li></ul><h4 id="即席查询中动态视图的使用"><a href="#即席查询中动态视图的使用" class="headerlink" title="即席查询中动态视图的使用"></a>即席查询中动态视图的使用</h4><p>下面介绍动态视图的使用，结合上面说的，我们有时候即需要查询hive中T-1的历史数据，又要查询今天实时写入的数据，怎么实现？</p><p>Impala就可以帮助我们，即可以查询Hive又可以查询Kudu，但是每次我们都需要判断日期是不是今天，来判断要不要查kudu中的数据，因此就想到了视图，并结合Union all来实现。ok，到这这里就下面的视图创建语句。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create view prod.ods_view_pv_event_1d as</span><br><span class="line">select xxx</span><br><span class="line">      ,xxx</span><br><span class="line">  from prod.ods_hive_pv_event_1d</span><br><span class="line"> where ds &lt;&#x3D; &#39;20180704&#39;</span><br><span class="line"> union all</span><br><span class="line">select xxx</span><br><span class="line">      ,xxx</span><br><span class="line">  from prod.ods_kudu_pv_event_1d</span><br><span class="line"> where nginx_date &#x3D; &#39;20180705&#39;</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>虽然视图创建成功，也满足了对于20180705的需求，但是如果过了这一天，20180706应该怎么查呢，还需要再创建一个视图？</p><p>其实，我们可以如下这么做，下面的语句只是将上面写死的日期改成了可根据now()动态生成的日期。这样我们就不需要每次使用的时候创建一个视图了。</p><p>视图并不是数据库中以存储的数据值集形式存在，而仅仅是存储在数据库中具有关联名称的查询语句！！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create view prod.ods_view_pv_event_1d as</span><br><span class="line">select xxx</span><br><span class="line">      ,xxx</span><br><span class="line">  from prod.ods_hive_pv_event_1d</span><br><span class="line"> where ds &lt;&#x3D; from_timestamp(date_sub(now(), 1),&#39;yyyyMMdd&#39;)</span><br><span class="line"> union all</span><br><span class="line">select xxx</span><br><span class="line">      ,xxx</span><br><span class="line">  from prod.ods_kudu_pv_event_1d</span><br><span class="line"> where nginx_date &#x3D; from_timestamp(now(),&#39;yyyyMMdd&#39;)</span><br><span class="line">;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>结束语：</p><p>以上总结了分布式数据仓库在公司的应用与演变，以及相关技术栈，最后简单介绍了我们正在努力研发的即席查询系统，以及即席查询系统中动态视图的妙用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文只关注与分布式数据仓库在公司的使用和演变，不涉及任何技术细节。希望通过本文的总结，能让你对数据仓库在公司的应用与演变，以及相关技术栈有初步的认识。&lt;/p&gt;
    
    </summary>
    
      <category term="数据仓库" scheme="http://guoyanlei.top/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
      <category term="hive" scheme="http://guoyanlei.top/tags/hive/"/>
    
      <category term="impala" scheme="http://guoyanlei.top/tags/impala/"/>
    
      <category term="kudu" scheme="http://guoyanlei.top/tags/kudu/"/>
    
  </entry>
  
  <entry>
    <title>Hive-Mongo外部表使用记录</title>
    <link href="http://guoyanlei.top/2018/07/05/2018070501-Hive-Mongo%E5%A4%96%E9%83%A8%E8%A1%A8%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"/>
    <id>http://guoyanlei.top/2018/07/05/2018070501-Hive-Mongo外部表使用记录/</id>
    <published>2018-07-05T07:30:00.000Z</published>
    <updated>2018-07-06T14:40:50.630Z</updated>
    
    <content type="html"><![CDATA[<h3 id="统计需求"><a href="#统计需求" class="headerlink" title="统计需求"></a>统计需求</h3><p>需要在数据仓库中统计业务系统的数据，而这些数据存储在mongodb中，如何获取mongo中的数据并统计呢？下面记录下官网所提供的mongo-hadoop-hive方式来获取mongo中的数据。</p><p>官方：<a href="https://github.com/mongodb/mongo-hadoop/wiki/Hive-Usage">https://github.com/mongodb/mongo-hadoop/wiki/Hive-Usage</a></p><a id="more"></a><h3 id="hive-mongo外部表"><a href="#hive-mongo外部表" class="headerlink" title="hive-mongo外部表"></a>hive-mongo外部表</h3><p>使用MongoStorageHandler实现hive-mongo外部表的映射，建表语句如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">use business;</span><br><span class="line">add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;mongo-hadoop-hive-1.5.1.jar;</span><br><span class="line">add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;mongo-hadoop-core-1.5.1.jar;</span><br><span class="line">add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;mongo-java-driver-3.2.2.jar;</span><br><span class="line">create external table ods_dim_mongodb_volunteer_stat_info</span><br><span class="line">(</span><br><span class="line">    id                    bigint,</span><br><span class="line">    date                  string,</span><br><span class="line">    volunteer_nums        int,</span><br><span class="line">    grap_totals           int,</span><br><span class="line">    grap_success_totals   int,</span><br><span class="line">    rescuit_success       int,</span><br><span class="line">    avg_match             double,</span><br><span class="line">    avg_new               double</span><br><span class="line">)</span><br><span class="line">STORED BY &#39;com.mongodb.hadoop.hive.MongoStorageHandler&#39;</span><br><span class="line">with SERDEPROPERTIES (&#39;mongo.columns.mapping&#39;&#x3D;&#39;&#123;&quot;id&quot;:&quot;_id&quot;,&quot;date&quot;:&quot;date&quot;,&quot;volunteer_nums&quot;:&quot;volunteerNums&quot;,&quot;grap_totals&quot;:&quot;grapTotals&quot;,&quot;grap_success_totals&quot;:&quot;grapSuccessTotals&quot;,&quot;rescuit_success&quot;:&quot;rescuitSuccess&quot;,&quot;avg_match&quot;:&quot;avgMatch&quot;,&quot;avg_new&quot;:&quot;avgNew&quot;&#125;&#39;)</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">    &#39;mongo.uri&#39;&#x3D;&#39;mongodb:&#x2F;&#x2F;&#123;username&#125;:&#123;password&#125;@&#123;hostname&#125;:27017&#x2F;&#123;db&#125;.&#123;collection&#125;&#39;</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="添加splitVector权限"><a href="#添加splitVector权限" class="headerlink" title="添加splitVector权限"></a>添加splitVector权限</h3><p>创建外部表完毕后，在hive上进行查询时，报错如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">not authorized on certificate to execute command </span><br><span class="line">&#123; splitVector: &quot;certificate.certificate.access_log_test&quot;, </span><br><span class="line">  keyPattern: &#123; _id: 1 &#125;, </span><br><span class="line">  min: &#123;&#125;, </span><br><span class="line">  max: &#123;&#125;, </span><br><span class="line">  force: false, </span><br><span class="line">  maxChunkSize: 8 </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="分析原因"><a href="#分析原因" class="headerlink" title="分析原因"></a>分析原因</h4><ul><li>线上mongo使用分片</li><li>hive MongoStorageHandler 在拆分非分片集合时需要splitVector命令的，该命令仅限于管理员用户。mongo.input.split.create_input_splits的默认设置是true，也就是会对数据进行拆分，根据集群数，cpu核数然后将数据进行拆分成多个InputSplits,以允许Hadoop并行处理，也就是说，Hadoop为每个映射器分配一个InputSplits。</li></ul><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>没有splitVector权限，则需要为该用户加上该权限</p><p>首先添加一个新role，并给予splitVector权限，之后把这个role分配至指定的user上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># 添加角色</span><br><span class="line">db.createRole(</span><br><span class="line">    &#123;</span><br><span class="line">        role: &quot;hadoopSplitVector&quot;,</span><br><span class="line">        privileges: [</span><br><span class="line">            &#123;</span><br><span class="line">                resource: &#123;</span><br><span class="line">                    db: &quot;&#123;db&#125;&quot;,</span><br><span class="line">                    collection: &quot;&#123;collection&#125;&quot;</span><br><span class="line">                &#125;,</span><br><span class="line">                actions: [</span><br><span class="line">                    &quot;splitVector&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        roles:[]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 若需要对一个新的colletion添加splitVector权限，则只需更新角色即可</span><br><span class="line">db.updateRole(</span><br><span class="line">    &quot;hadoopSplitVector&quot;,</span><br><span class="line">    &#123;</span><br><span class="line">        privileges: [</span><br><span class="line">            &#123;</span><br><span class="line">                resource: &#123;</span><br><span class="line">                    db: &quot;suishen_lizhi&quot;,</span><br><span class="line">                    collection: &quot;wltt_invite_stats&quot;</span><br><span class="line">                &#125;,</span><br><span class="line">                actions: [</span><br><span class="line">                    &quot;splitVector&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                resource: &#123;</span><br><span class="line">                    db: &quot;suishen_lizhi&quot;,</span><br><span class="line">                    collection: &quot;volunteer_stat_info&quot;</span><br><span class="line">                &#125;,</span><br><span class="line">                actions: [</span><br><span class="line">                    &quot;splitVector&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">        ],</span><br><span class="line">        roles:[]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># 更新用户</span><br><span class="line">db.updateUser(</span><br><span class="line">    &quot;dmp&quot;,</span><br><span class="line">    &#123;</span><br><span class="line">        roles: [</span><br><span class="line">            &#123;</span><br><span class="line">                role:&quot;read&quot;,</span><br><span class="line">                db:&quot;&#123;db&#125;&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                role:&quot;hadoopSplitVector&quot;,</span><br><span class="line">                db:&quot;&#123;db&#125;&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 若没有可用户用户可以创建一个只读用户</span><br><span class="line">db.createUser(</span><br><span class="line">    &#123;</span><br><span class="line">        user: &quot;xxx&quot;,</span><br><span class="line">        pwd: &quot;xxx&quot;,</span><br><span class="line">        roles: [ </span><br><span class="line">            &#123; </span><br><span class="line">                role: &quot;read&quot;, </span><br><span class="line">                db: &quot;&#123;db&#125;&quot; </span><br><span class="line">            &#125; </span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注：上面添加更新用户、角色都需要较高权限的用户进行操作。</p><p>至此，已经可以使用外部表来查询mongo中的数据了。</p><h3 id="查Mongo时’-’的坑"><a href="#查Mongo时’-’的坑" class="headerlink" title="查Mongo时’=’的坑"></a>查Mongo时’=’的坑</h3><p>在使用外部表查询mongo时，在条件中使用 where app = ‘zhwnl’，发现查询的结果包含了其他的app，并不是我们限制的查询条件，这是因为MongoStorageHandler并没有将我们的’=’转成mongo中的’$.eq’。</p><p>但是，我们可以’like’来解决，功能和’=’等同。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;统计需求&quot;&gt;&lt;a href=&quot;#统计需求&quot; class=&quot;headerlink&quot; title=&quot;统计需求&quot;&gt;&lt;/a&gt;统计需求&lt;/h3&gt;&lt;p&gt;需要在数据仓库中统计业务系统的数据，而这些数据存储在mongodb中，如何获取mongo中的数据并统计呢？下面记录下官网所提供的mongo-hadoop-hive方式来获取mongo中的数据。&lt;/p&gt;
&lt;p&gt;官方：&lt;a href=&quot;https://github.com/mongodb/mongo-hadoop/wiki/Hive-Usage&quot;&gt;https://github.com/mongodb/mongo-hadoop/wiki/Hive-Usage&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="http://guoyanlei.top/categories/hive/"/>
    
    
      <category term="hive" scheme="http://guoyanlei.top/tags/hive/"/>
    
      <category term="mongodb" scheme="http://guoyanlei.top/tags/mongodb/"/>
    
  </entry>
  
  <entry>
    <title>Impala动态视图在即席查询中的妙用</title>
    <link href="http://guoyanlei.top/2018/07/03/2018070301-Impala%E5%8A%A8%E6%80%81%E8%A7%86%E5%9B%BE%E5%9C%A8%E5%8D%B3%E5%B8%AD%E6%9F%A5%E8%AF%A2%E4%B8%AD%E7%9A%84%E5%A6%99%E7%94%A8/"/>
    <id>http://guoyanlei.top/2018/07/03/2018070301-Impala动态视图在即席查询中的妙用/</id>
    <published>2018-07-03T07:30:00.000Z</published>
    <updated>2018-07-06T14:37:40.980Z</updated>
    
    <content type="html"><![CDATA[<h3 id="查询视图"><a href="#查询视图" class="headerlink" title="查询视图"></a>查询视图</h3><p>视图仅仅是存储在数据库中具有关联名称的Impala查询语言的语句。 它是以预定义的SQL查询形式的表的组合。视图可以包含表的所有行或选定的行。 可以从一个或多个表创建视图。</p><p>创建视图没什么难度，这里总结下是视图在我们Ad-Hoc查询是怎么用的。</p><a id="more"></a><h3 id="Ad-Hoc即席查询"><a href="#Ad-Hoc即席查询" class="headerlink" title="Ad-Hoc即席查询"></a>Ad-Hoc即席查询</h3><p>即席查询是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。</p><p>基于数据仓库中事实表的设计，我们的Ad-Hoc简单的架构如下图：</p><p><img src="/img/impala/ad-hoc-query.png" alt="Ad-Hoc架构"></p><p>架构比较简单，主要借助Impala + parquet去查询HDFS中的数据，并借助kudu接收实时写入的数据，</p><p>在T+1日后，会将kudu中实时写入的数据转存到HDFS中，并转换成parquet格式。</p><p>Parquet存储格式</p><ul><li>按列存储</li><li>可按时间分许</li><li>局部排序</li><li>只支持批量写入，无法追加，无法实时写入</li></ul><p>Kudu：新一代面向实时分析的存储引擎</p><ul><li>底层使用类似Parquet的存储结构</li><li>支持实时写入，实时更新，实时查询</li><li>扫描性能比Parquet略差</li></ul><h3 id="动态视图的使用"><a href="#动态视图的使用" class="headerlink" title="动态视图的使用"></a>动态视图的使用</h3><p>下面介绍动态视图的使用，结合上面说的，我们有时候即需要查询hive中T-1的历史数据，又要查询今天实时写入的数据，怎么实现呢？</p><p>Impala就可以帮助我们，即可以查询Hive又可以查询Kudu，但是每次我们都需要判断日期是不是今天，来判断要不要查kudu中的数据，因此就想到了视图，并结合Union all来实现。ok，到这这里就下面的视图创建语句。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create view prod.ods_view_pv_event_1d as</span><br><span class="line">select xxx</span><br><span class="line">      ,xxx</span><br><span class="line">  from prod.ods_hive_pv_event_1d</span><br><span class="line"> where ds &lt;&#x3D; &#39;20180704&#39;</span><br><span class="line"> union all</span><br><span class="line">select xxx</span><br><span class="line">      ,xxx</span><br><span class="line">  from prod.ods_kudu_pv_event_1d</span><br><span class="line"> where nginx_date &#x3D; &#39;20180705&#39;</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>虽然视图创建成功，也满足了对于20180705的需求，但是如果过了这一天，20180706应该怎么查呢，还需要再创建一个视图？</p><p>其实，我们可以如下这么做，下面的语句只是将上面写死的日期改成了可根据now()动态的日期。这样我们就不需要每次使用的时候创建一个视图了。</p><p>主要是视图，并不是数据库中以存储的数据值集形式存在，而仅仅是存储在数据库中具有关联名称的查询语句！！（这一点需要仔细参悟，自己当时就想了很久）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create view prod.ods_view_pv_event_1d as</span><br><span class="line">select xxx</span><br><span class="line">      ,xxx</span><br><span class="line">  from prod.ods_hive_pv_event_1d</span><br><span class="line"> where ds &lt;&#x3D; from_timestamp(date_sub(now(), 1),&#39;yyyyMMdd&#39;)</span><br><span class="line"> union all</span><br><span class="line">select xxx</span><br><span class="line">      ,xxx</span><br><span class="line">  from prod.ods_kudu_pv_event_1d</span><br><span class="line"> where nginx_date &#x3D; from_timestamp(now(),&#39;yyyyMMdd&#39;)</span><br><span class="line">;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;查询视图&quot;&gt;&lt;a href=&quot;#查询视图&quot; class=&quot;headerlink&quot; title=&quot;查询视图&quot;&gt;&lt;/a&gt;查询视图&lt;/h3&gt;&lt;p&gt;视图仅仅是存储在数据库中具有关联名称的Impala查询语言的语句。 它是以预定义的SQL查询形式的表的组合。视图可以包含表的所有行或选定的行。 可以从一个或多个表创建视图。&lt;/p&gt;
&lt;p&gt;创建视图没什么难度，这里总结下是视图在我们Ad-Hoc查询是怎么用的。&lt;/p&gt;
    
    </summary>
    
      <category term="impala" scheme="http://guoyanlei.top/categories/impala/"/>
    
    
      <category term="impala" scheme="http://guoyanlei.top/tags/impala/"/>
    
  </entry>
  
  <entry>
    <title>使用DataX实现离线同步分库分表数据</title>
    <link href="http://guoyanlei.top/2018/07/02/2018070201-%E4%BD%BF%E7%94%A8DataX%E5%AE%9E%E7%8E%B0%E7%A6%BB%E7%BA%BF%E5%90%8C%E6%AD%A5%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E6%95%B0%E6%8D%AE/"/>
    <id>http://guoyanlei.top/2018/07/02/2018070201-使用DataX实现离线同步分库分表数据/</id>
    <published>2018-07-02T06:30:00.000Z</published>
    <updated>2018-09-12T02:56:09.945Z</updated>
    
    <content type="html"><![CDATA[<h3 id="DataX简介"><a href="#DataX简介" class="headerlink" title="DataX简介"></a>DataX简介</h3><p>DataX 是阿里巴巴内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。</p><p>DataX更像是一个数据枢纽，它可以读取多种数据源中数据，经过内部的转换又可以输出到多种数据源中。</p><p>其架构设计主要包含三部分：</p><ul><li>Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework</li><li>Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。</li><li>Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。</li></ul><p>更详细的介绍，请移步 <a href="https://yq.aliyun.com/articles/59373">https://yq.aliyun.com/articles/59373</a></p><p>这里只总结DataX在同步MySQL分库分表的使用。</p><a id="more"></a><h3 id="同步分库中表的数据"><a href="#同步分库中表的数据" class="headerlink" title="同步分库中表的数据"></a>同步分库中表的数据</h3><p>我们业务系统的用户基本信息表，被分库存储在不同的库中，下面介绍使用DataX如何将这些分库的数据同步到HDFA中。</p><h4 id="下载-amp-使用"><a href="#下载-amp-使用" class="headerlink" title="下载 &amp; 使用"></a>下载 &amp; 使用</h4><p>从<a href="https://github.com/alibaba/DataX">Datax Github</a> 中下载源码编译，或直接下载已编译好的工具包。</p><p>下载后解压至本地某个目录，进入bin目录，即可运行同步作业：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd  &#123;DATAX_HOME&#125;&#x2F;bin</span><br><span class="line">$ python datax.py &#123;YOUR_JOB.json&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>配置和使用很简单，只需配置YOUR_JOB.json，在这个文件中配置输入源.</p><p>其中 datax.py可以配置多个参数，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-j &lt;jvm parameters&gt;  # 设置jvm参数，如-Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">--jobid&#x3D;&lt;job unique id&gt; # 设置job唯一id</span><br><span class="line">-m &lt;job runtime mode&gt; # 设置运行模式: standalone(默认), local, distribute.</span><br><span class="line">-p &lt;parameter used in job config&gt; # 动态设置一些参数，当在配置文件设置了一些变量时，如$&#123;tableName&#125;，那么可以通过-p&quot;-DtableName&#x3D;your-table-name&quot;来动态设置。</span><br><span class="line">-r &lt;parameter used in view job config[reader] template&gt;  # 使用一些reader模版，eg: mysqlreader,streamreader</span><br><span class="line">-w &lt;parameter used in view job config[writer] template&gt;, # 使用一些reader模版，eg: mysqlwriter,streamwriter</span><br></pre></td></tr></table></figure><h4 id="同步分库的配置"><a href="#同步分库的配置" class="headerlink" title="同步分库的配置"></a>同步分库的配置</h4><p>下面给出同步分库中表数据的配置，content中包含两部分内容，reader和writer，其中reader中配置了mysql的分库地址。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"># user_info.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;job&quot;: &#123;</span><br><span class="line">    &quot;setting&quot;: &#123;</span><br><span class="line">      &quot;speed&quot;: &#123;</span><br><span class="line">        &quot;channel&quot;:60</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;content&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;reader&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;mysqlreader&quot;,</span><br><span class="line">          &quot;parameter&quot;: &#123;</span><br><span class="line">            &quot;username&quot;: &quot;xxx&quot;,</span><br><span class="line">            &quot;password&quot;: &quot;xxx&quot;,</span><br><span class="line">            &quot;column&quot;: [</span><br><span class="line">                 &quot;uid&quot;</span><br><span class="line">                ,&quot;nick_name&quot;</span><br><span class="line">                ,&quot;accounts&quot;</span><br><span class="line">                ,&quot;email&quot;</span><br><span class="line">                , ... ...</span><br><span class="line">            ],</span><br><span class="line">            &quot;connection&quot;: [</span><br><span class="line">              &#123;&quot;table&quot;: [&quot;user_info&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;metastore_1000&quot;]&#125;,</span><br><span class="line">              &#123;&quot;table&quot;: [&quot;user_info&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_2:3306&#x2F;metastore_1001&quot;]&#125;,</span><br><span class="line">              &#123;&quot;table&quot;: [&quot;user_info&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_3:3306&#x2F;metastore_1002&quot;]&#125;,</span><br><span class="line">              ... ...</span><br><span class="line">            ]</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;writer&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;hdfswriter&quot;,</span><br><span class="line">          &quot;parameter&quot;: &#123;</span><br><span class="line">            &quot;defaultFS&quot;: &quot;hdfs:&#x2F;&#x2F;nameservice:8020&quot;,</span><br><span class="line">            &quot;hadoopConfig&quot;:&#123;</span><br><span class="line">              &quot;dfs.nameservices&quot;: &quot;nameservice&quot;,</span><br><span class="line">              &quot;dfs.ha.namenodes.nameservice&quot;: &quot;namenode44,namenode46&quot;,</span><br><span class="line">              &quot;dfs.namenode.rpc-address.nameservice.namenode44&quot;: &quot;nn1.hadoop.bigdata.dmp.com:8020&quot;,</span><br><span class="line">              &quot;dfs.namenode.rpc-address.nameservice.namenode46&quot;: &quot;nn2.hadoop.bigdata.dmp.com:8020&quot;,</span><br><span class="line">              &quot;dfs.client.failover.proxy.provider.nameservice&quot;: &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;fileType&quot;: &quot;orc&quot;,</span><br><span class="line">            &quot;path&quot;: &quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_info&quot;,</span><br><span class="line">            &quot;fileName&quot;: &quot;user_info&quot;,</span><br><span class="line">            &quot;column&quot;: [</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;uid&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;BIGINT&quot;</span><br><span class="line">              &#125;,</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;nick_name&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;STRING&quot;</span><br><span class="line">              &#125;,</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;accounts&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;STRING&quot;</span><br><span class="line">              &#125;,</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;email&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;STRING&quot;</span><br><span class="line">              &#125;,</span><br><span class="line">              ... ...</span><br><span class="line">            ],</span><br><span class="line">            &quot;writeMode&quot;: &quot;append&quot;,</span><br><span class="line">            &quot;fieldDelimiter&quot;: &quot;\t&quot;,</span><br><span class="line">            &quot;compress&quot;:&quot;NONE&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="同步分表中的数据"><a href="#同步分表中的数据" class="headerlink" title="同步分表中的数据"></a>同步分表中的数据</h3><p>和同步分库的结构类似，同步分表的conf如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"># user_relation.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;job&quot;: &#123;</span><br><span class="line">    &quot;setting&quot;: &#123;</span><br><span class="line">      &quot;speed&quot;: &#123;</span><br><span class="line">        &quot;channel&quot;:60</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;content&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;reader&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;mysqlreader&quot;,</span><br><span class="line">          &quot;parameter&quot;: &#123;</span><br><span class="line">            &quot;username&quot;: &quot;xxx&quot;,</span><br><span class="line">            &quot;password&quot;: &quot;xxx&quot;,</span><br><span class="line">            &quot;column&quot;: [</span><br><span class="line">                 &quot;uid&quot;</span><br><span class="line">                ,&quot;attention_uid&quot;</span><br><span class="line">                ,&quot;status&quot;</span><br><span class="line">                ,&quot;create_time&quot;</span><br><span class="line">                ,&quot;update_time&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;connection&quot;: [</span><br><span class="line">              &#123;&quot;table&quot;: [&quot;user_relation_0&quot;],&quot;jdbcUrl&quot;:   [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;,</span><br><span class="line">              &#123;&quot;table&quot;: [&quot;user_relation_1&quot;],&quot;jdbcUrl&quot;:   [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;,</span><br><span class="line">              &#123;&quot;table&quot;: [&quot;user_relation_10&quot;],&quot;jdbcUrl&quot;:  [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;,</span><br><span class="line">              &#123;&quot;table&quot;: [&quot;user_relation_100&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;,</span><br><span class="line">              &#123;&quot;table&quot;: [&quot;user_relation_101&quot;],&quot;jdbcUrl&quot;: [&quot;jdbc:mysql:&#x2F;&#x2F;ip_1:3306&#x2F;user_attention&quot;]&#125;,</span><br><span class="line">              ... ...</span><br><span class="line">            ]</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;writer&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;hdfswriter&quot;,</span><br><span class="line">          &quot;parameter&quot;: &#123;</span><br><span class="line">            &quot;defaultFS&quot;: &quot;hdfs:&#x2F;&#x2F;nameservice:8020&quot;,</span><br><span class="line">            &quot;hadoopConfig&quot;:&#123;</span><br><span class="line">              &quot;dfs.nameservices&quot;: &quot;nameservice&quot;,</span><br><span class="line">              &quot;dfs.ha.namenodes.nameservice&quot;: &quot;namenode44,namenode46&quot;,</span><br><span class="line">              &quot;dfs.namenode.rpc-address.nameservice.namenode44&quot;: &quot;nn1.hadoop.bigdata.dmp.com:8020&quot;,</span><br><span class="line">              &quot;dfs.namenode.rpc-address.nameservice.namenode46&quot;: &quot;nn2.hadoop.bigdata.dmp.com:8020&quot;,</span><br><span class="line">              &quot;dfs.client.failover.proxy.provider.nameservice&quot;: &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;fileType&quot;: &quot;orc&quot;,</span><br><span class="line">            &quot;path&quot;: &quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_relation&quot;,</span><br><span class="line">            &quot;fileName&quot;: &quot;user_info&quot;,</span><br><span class="line">            &quot;column&quot;: [</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;uid&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;BIGINT&quot;</span><br><span class="line">              &#125;,</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;attention_uid&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;BIGINT&quot;</span><br><span class="line">              &#125;,</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;status&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;int&quot;</span><br><span class="line">              &#125;,</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;create_time&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;BIGINT&quot;</span><br><span class="line">              &#125;,</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;name&quot;: &quot;update_time&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;BIGINT&quot;</span><br><span class="line">              &#125;</span><br><span class="line">            ],</span><br><span class="line">            &quot;writeMode&quot;: &quot;append&quot;,</span><br><span class="line">            &quot;fieldDelimiter&quot;: &quot;\t&quot;,</span><br><span class="line">            &quot;compress&quot;:&quot;NONE&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体的hdfs写入，参考<a href="https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md">文档说明</a></p><p>在执行时，若hdfs中的目录为空，需要先创建，若不为空，需根据writeMode来判断，若模式是：append，写入前不做任何处理，DataX hdfswriter直接使用filename写入，并保证文件名不冲突；若模式是：nonConflict，如果目录下有fileName前缀的文件，直接报错。</p><p>可以建立一个执行脚本，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">hdfs dfs -rm -r &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_relation</span><br><span class="line">hdfs dfs -rm -r &#x2F;user&#x2F;impala&#x2F;.Trash&#x2F;Current&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_relation</span><br><span class="line">hdfs dfs -mkdir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;ads_dim_user_relation</span><br><span class="line"></span><br><span class="line">python datax.py user_relation.json -j &#39;-Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="创建Hive表"><a href="#创建Hive表" class="headerlink" title="创建Hive表"></a>创建Hive表</h3><p>按指定格式导入到hdfs后，就可以建立hive表，并指定hdfs目录，就可以读取其中的数据了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table business.ods_dim_weili_user_relation(</span><br><span class="line">     uid              bigint</span><br><span class="line">    ,attention_uid    bigint</span><br><span class="line">    ,status           int</span><br><span class="line">    ,create_time      bigint</span><br><span class="line">    ,update_time      bigint</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">stored as orc;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;DataX简介&quot;&gt;&lt;a href=&quot;#DataX简介&quot; class=&quot;headerlink&quot; title=&quot;DataX简介&quot;&gt;&lt;/a&gt;DataX简介&lt;/h3&gt;&lt;p&gt;DataX 是阿里巴巴内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。&lt;/p&gt;
&lt;p&gt;DataX更像是一个数据枢纽，它可以读取多种数据源中数据，经过内部的转换又可以输出到多种数据源中。&lt;/p&gt;
&lt;p&gt;其架构设计主要包含三部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework&lt;/li&gt;
&lt;li&gt;Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。&lt;/li&gt;
&lt;li&gt;Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更详细的介绍，请移步 &lt;a href=&quot;https://yq.aliyun.com/articles/59373&quot;&gt;https://yq.aliyun.com/articles/59373&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里只总结DataX在同步MySQL分库分表的使用。&lt;/p&gt;
    
    </summary>
    
      <category term="数据同步" scheme="http://guoyanlei.top/categories/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"/>
    
    
      <category term="DataX" scheme="http://guoyanlei.top/tags/DataX/"/>
    
  </entry>
  
  <entry>
    <title>浅析数据仓库维度建模</title>
    <link href="http://guoyanlei.top/2018/06/20/2018062001-%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/"/>
    <id>http://guoyanlei.top/2018/06/20/2018062001-浅析数据仓库维度建模/</id>
    <published>2018-06-20T07:30:00.000Z</published>
    <updated>2018-06-20T10:09:38.682Z</updated>
    
    <content type="html"><![CDATA[<h3 id="维度建模基础"><a href="#维度建模基础" class="headerlink" title="维度建模基础"></a>维度建模基础</h3><h4 id="维度-amp-维度属性"><a href="#维度-amp-维度属性" class="headerlink" title="维度 &amp; 维度属性"></a>维度 &amp; 维度属性</h4><p>维度是维度建模的基础和灵魂，在维度建模中，将度量称为“事实”，将环境描述为“维度”，维度是用来分析事实所需要的多样环境。</p><p>如：将用户交易数据看做一个“事实”，可通过买家、卖家、商品和时间等多个维度来描述整个交易过程所发生的环境。</p><p>维度所包含的表示维度的列，称为维度属性。维度属性是查询约束条件（where）、分组（group by）和报表标签生成的基本来源。</p><a id="more"></a><h4 id="下钻-amp-上钻（上卷）"><a href="#下钻-amp-上钻（上卷）" class="headerlink" title="下钻 &amp; 上钻（上卷）"></a>下钻 &amp; 上钻（上卷）</h4><p>维度中的属性有时会以层次方式或一对多的方式相互关联，层次的最底层代表维度中描述最低级别的详细信息，最高层代表最高级别的概要信息。在属性的层次结构中进行钻取是数据钻取的方法之一。</p><p>沿着属性层次，往下即为下钻（drill down），往上可称上钻或上卷（roll up）。</p><p>如：地区维度，从全球到地区到国家到省份到市到县，层层下钻。第一级别可以根据需求而定。我看到上海市的总数，想要看各个区的数据，就用下钻。</p><p>对多维数据进行分析时还会用到：</p><ul><li><p>旋转(pivot)，是维度的切换，换个角度看问题</p></li><li><p>切片(slice)，查看数据的时候，通常是不会所有维度都用到的，选择某两个维度进行分析，可以想象为一个二维平面，即切片。</p></li><li><p>切块，即选出3个维度时，切出来的自然就时块了。</p></li></ul><p>至此，就可以理解OLAP中的CUBE等相关概念了。</p><h4 id="规范化（OLTP）-amp-反规范化（OLAP）"><a href="#规范化（OLTP）-amp-反规范化（OLAP）" class="headerlink" title="规范化（OLTP）&amp; 反规范化（OLAP）"></a>规范化（OLTP）&amp; 反规范化（OLAP）</h4><p>将属性层次设计成数据库表时，规范化可以将重复属性移至其自身所属的表中，删除冗余数据，还可以有效避免数据冗余导致的不一致性。规范化常用在大多数联机事务处理系统(OLTP)的底层数据结构设计中。</p><p>当属性层次被实例化为一系列维度，而不是单一的维度时，被称为雪花模式，常出现在OLTP中。</p><p>而对于联机分析处理系统(OLAP)来说，数据是稳定的，不存在OLTP中所存在的问题。因此，在设计时常采用反规范化处理，从用户角度来看简化了模型，并且使数据库查询优化器的连接路径比完全规范化的模型简化许多。</p><h4 id="一致性维度-amp-交叉探查"><a href="#一致性维度-amp-交叉探查" class="headerlink" title="一致性维度 &amp; 交叉探查"></a>一致性维度 &amp; 交叉探查</h4><p>企业级数据仓库等构建不可能一蹴而就，一般采用迭代式的构建过程。 而单独构建存在的问题是形成独立型数据集市，导致严重的不一致性。</p><p>Kimball的数据仓库总线架构：提供了一种分解企业级数据仓库规划任务的合理方法，通过构建企业范围内一致性维度和事实来构建总线架构。</p><p>如：对于日志数据域，统计了商品维度的最近一天的PV和UV; 对于交易数据域，统计了商品维度的最近一天的下单量。</p><p>现在将不同数据域的商品的事实合并在一起进行数据探查，如计算转化率等，称为交叉探查。</p><p>不用数据域的维度可能会存在不一致的问题，为避免这一问题，可通过以下方式保证维度一直性：</p><ul><li>共享维表：基于共享维表进行交叉探查</li><li>一致性上卷：可以在两个维度相同的上层属性上，交叉探查</li><li>交叉属性：可在两个维度相同的属性上，交叉探查</li></ul><h3 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h3><h4 id="维度整合-amp-拆分"><a href="#维度整合-amp-拆分" class="headerlink" title="维度整合 &amp; 拆分"></a>维度整合 &amp; 拆分</h4><ul><li>垂直整合：不同的来源表包含相同的数据集</li><li>水平整个：不同的来源表包含不同的数据集</li></ul><p>如：会员在源系统中可能有多个表，会员基础信息表、会员扩展信息表、会员等级信息表等，这些表都属于会员相关信息表，应尽量整合至会员维度模型中，即垂直整合。</p><p>如：对与淘宝的会员体系，分为支付宝会员、淘宝会员等，这这些会员进行整合时即水平整合，整合时需要考虑各个会员体系是否有交叉，如果存在交叉，则需要去重；如果不存在交叉，则需要考虑不同子集的自然键是否存在冲突，冲突时可添加分区字段来处理。</p><p>有整合就会有拆分，维度是维度建模的基础和灵魂，在进行维度设计时，依据维度设计的原则，尽可能丰富维度属性，同时进行反规范化处理。</p><h4 id="维度变化"><a href="#维度变化" class="headerlink" title="维度变化"></a>维度变化</h4><p>如何处理维度的变化是维度设计的重要工作之一。</p><h5 id="缓慢变化维"><a href="#缓慢变化维" class="headerlink" title="缓慢变化维"></a>缓慢变化维</h5><p>在现实世界中，维度的属性并不是静态的，它会随着时间的流逝发生缓慢的变化。与数据增长较为快速的事实表相比，维度变化相对缓慢。</p><p>在一些情况下，保留历史数据没有什么分析价值；而在另一些情况下，保留历史数据将会起到至关重要的作用。有三种方式处理缓慢变化维：</p><ul><li>重写维度值：不会保留历史，始终取最新的数据</li><li>插入新的维度行：保留历史数据，老的事实与老的维度值关联，新的与新的关联，不能将变化前后的事实归一化为变化前的维度或变化后的维度</li><li>添加维度列(新属性)：保留历史数据，可解决变化前后的事实按变化前后的维度归一化问题</li></ul><h5 id="快照维表"><a href="#快照维表" class="headerlink" title="快照维表"></a>快照维表</h5><p>处理缓慢变化维的方法可以采用快照方式，即基于某一周期（天），处理维度变化的方式就是每天保留一份全量快照数据。</p><p>优：简单有效，开发维护成本低；使用方便，理解性好。<br>缺：存储极大浪费，必须要有对应的数据生命周期制度，清除无用的历史数据。</p><h5 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h5><p>采用插入新的维度行的思想，处理方式：通过新增两个时间戳字段(start_dt和end_dt)，将所有以天为粒度的变更数据都记录下来。</p><p>拉链记历史，不需要按周期把所有数据都快照存储，只需要将发生变化的数据添加新的存储。</p><p>在查询数据时，可通过限制时间戳字段来获取历史数据。</p><p>缺：理解较困难，另外，以start_dt和end_dt做分区，一年的可能产生的分区数：365 x 364/2 = 66430个，随着时间推移，会使得分区数极度膨胀，超过分区数限制。</p><h5 id="拉链表升级之极限存储"><a href="#拉链表升级之极限存储" class="headerlink" title="拉链表升级之极限存储"></a>拉链表升级之极限存储</h5><p>底层基于拉链表，上层做一个视图，对下游使用者做到透明。同时，缓解分区数极度膨胀导致存储查询代价较大的问题。</p><p>优化的根源在于分区数，只需每月多一次拉链计算，就能优化十倍的性能。</p><h5 id="微型维度"><a href="#微型维度" class="headerlink" title="微型维度"></a>微型维度</h5><p>通过将一部分不稳定的属性从主维度中移出，并将它们放置到拥有自己代理键的新表中。</p><p>如：用户的注册日期、年龄、性别的信息基本不会变化，但是用户等级却在不断变化，可将其独立开来。</p><p>缺：实现代价较高，ETL逻辑复杂，破坏了维度的可浏览性。</p><p>【参考】</p><p><a href="http://tech.weli.cn/2017/12/29/dataware-intro/">之前的维度建模分享</a></p><p><a href="http://www.zdingke.com/2018/05/24/%E6%8B%89%E9%93%BE%E8%A1%A8%E4%B8%8E%E6%9E%81%E9%99%90%E5%AD%98%E5%82%A8/">拉链表与极限存储</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;维度建模基础&quot;&gt;&lt;a href=&quot;#维度建模基础&quot; class=&quot;headerlink&quot; title=&quot;维度建模基础&quot;&gt;&lt;/a&gt;维度建模基础&lt;/h3&gt;&lt;h4 id=&quot;维度-amp-维度属性&quot;&gt;&lt;a href=&quot;#维度-amp-维度属性&quot; class=&quot;headerlink&quot; title=&quot;维度 &amp;amp; 维度属性&quot;&gt;&lt;/a&gt;维度 &amp;amp; 维度属性&lt;/h4&gt;&lt;p&gt;维度是维度建模的基础和灵魂，在维度建模中，将度量称为“事实”，将环境描述为“维度”，维度是用来分析事实所需要的多样环境。&lt;/p&gt;
&lt;p&gt;如：将用户交易数据看做一个“事实”，可通过买家、卖家、商品和时间等多个维度来描述整个交易过程所发生的环境。&lt;/p&gt;
&lt;p&gt;维度所包含的表示维度的列，称为维度属性。维度属性是查询约束条件（where）、分组（group by）和报表标签生成的基本来源。&lt;/p&gt;
    
    </summary>
    
      <category term="数据仓库" scheme="http://guoyanlei.top/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
      <category term="数据仓库" scheme="http://guoyanlei.top/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
      <category term="维度建模" scheme="http://guoyanlei.top/tags/%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>hive调优之控制mapper和reducer数</title>
    <link href="http://guoyanlei.top/2018/06/19/2018061901-hive%E8%B0%83%E4%BC%98%E4%B9%8B%E6%8E%A7%E5%88%B6mapper%E5%92%8Creducer%E6%95%B0/"/>
    <id>http://guoyanlei.top/2018/06/19/2018061901-hive调优之控制mapper和reducer数/</id>
    <published>2018-06-19T09:30:04.000Z</published>
    <updated>2018-11-22T08:57:01.494Z</updated>
    
    <content type="html"><![CDATA[<h3 id="hive调优之控制mapper和reducer数"><a href="#hive调优之控制mapper和reducer数" class="headerlink" title="hive调优之控制mapper和reducer数"></a>hive调优之控制mapper和reducer数</h3><p>由于mapreduce中没有办法直接控制map数量，所以只能曲线救国，通过设置每个map中处理的数据量进行设置；reduce是可以直接设置的。</p><a id="more"></a><h3 id="控制mapper数"><a href="#控制mapper数" class="headerlink" title="控制mapper数"></a>控制mapper数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set mapred.max.split.size&#x3D;256000000;        -- 决定每个map处理的最大的文件大小，单位为B</span><br><span class="line">set mapred.min.split.size.per.node&#x3D;1;         -- 节点中可以处理的最小的文件大小</span><br><span class="line">set mapred.min.split.size.per.rack&#x3D;1;         -- 机架中可以处理的最小的文件大小</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="mapper划分文件处理逻辑"><a href="#mapper划分文件处理逻辑" class="headerlink" title="mapper划分文件处理逻辑"></a>mapper划分文件处理逻辑</h4><ul><li><p>执行HQL时，首先会读取表分区下到文件，然后分发到各个节点，此时是根据mapred.max.split.size确认要启动多少个map数。</p></li><li><p>假设有两个文件大小分别为(256M,280M)被分配到节点A，那么会启动两个map，剩余的文件大小为10MB和35MB因为每个大小都不足241MB会先做保留</p></li><li><p>根据参数set mapred.min.split.size.per.node看剩余的大小情况并进行合并。</p></li><li><p>如果值为1，表示a中每个剩余文件都会自己起一个map，这里会起两个，如果设置为大于45 * 1024 * 1024则会合并成一个块，并产生一个map。如果mapred.min.split.size.per.node为10 * 1024 * 1024，那么在这个节点上一共会有4个map，处理的大小为(245MB，245MB，10MB，10MB，10MB，10MB)，余下9MB。如果mapred.min.split.size.per.node为45 * 1024 * 1024，那么会有三个map，处理的大小为(245MB，245MB，45MB) </p></li><li><p>实际中mapred.min.split.size.per.node无法准确地设置成45 * 1024 * 1024，会有剩余并保留带下一步进行判断处理 </p></li><li><p>对b中余出来的文件与其它节点余出来的文件根据mapred.min.split.size.per.rack大小进行判断是否合并，对再次余出来的文件独自产生一个map处理</p></li></ul><h4 id="场景1：控制文件生成个数"><a href="#场景1：控制文件生成个数" class="headerlink" title="场景1：控制文件生成个数"></a>场景1：控制文件生成个数</h4><p>通常在执行HQL时，分配的mapper和reducer个数都是根据文件的大小和任务的情况自动计算的，但有时，我们想去控制最终生成文件的个数。</p><p>前段时间在做hive表的跨集群迁移，需要将老集群上的ODS层表迁移到新集群，由于ODS表中的分区时按天和小时的，分区下小文件较多，在进行hive export和discp操作时，由于要频繁的创建和到导出小文件，因此非常慢。</p><p>能想到的解决方法：合并分区数据，减少小文件，然后再迁移</p><p>由于合并分区通常只有map阶段，没有reduce，因此有多少个mapper就会生成多少个文件，这里就可以通过设置mapper数间接控制文件个数，即加大map分配到文件大小，减少mapper个数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set mapred.max.split.size&#x3D;1024000000;</span><br><span class="line">set mapred.min.split.size.per.node&#x3D;1024000000;</span><br><span class="line">set mapred.min.split.size.per.rack&#x3D;1024000000;</span><br><span class="line">insert into table xxx</span><br><span class="line">select *</span><br><span class="line">  from xxx</span><br><span class="line"> where xxx</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="控制reducer数"><a href="#控制reducer数" class="headerlink" title="控制reducer数"></a>控制reducer数</h3><p>修改reduce的个数就简单很多，直接根据可能的情况作个简单的判断确认需要的reduce数量，如果无法判断，根据当前map数减小10倍，保持在1~100个reduce即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">方法1</span><br><span class="line">set mapred.reduce.tasks&#x3D;10;  -- 设置reduce的数量</span><br><span class="line">方法2</span><br><span class="line">set hive.exec.reducers.bytes.per.reducer&#x3D;1073741824 -- 每个reduce处理的数据量,默认1GB</span><br></pre></td></tr></table></figure><p>可通过以下两个参数配合使用：</p><ul><li>set hive.merge.mapredfiles=true</li><li>distribute by rand()</li></ul><p>小文件太多的话可以打开reduce端的小文件合并，即第一个set，后面的distribute by rand()保证了记录随机分配到50个文件，不管里数据量有多小，最后这50个文件的大小应该是一致的.</p><p>有个推送的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">推送逻辑写在UDF中，如下SQL的实际推送是在Map阶段完成</span><br><span class="line">推送并行度，是依据表ads_hive_rooster_ipush_users_1d分区中文件的个数</span><br><span class="line">和上面生成Map任务的逻辑一样，根据文件的大小和块大小进行分割</span><br><span class="line"></span><br><span class="line">add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;dmp-function-lib-test2.jar;</span><br><span class="line">create temporary function push as &#39;suishen.temp.udf.PushTestUDF&#39;;</span><br><span class="line">select result</span><br><span class="line">      ,count(*)</span><br><span class="line">  from</span><br><span class="line">      (</span><br><span class="line">        select pust(uid) as result</span><br><span class="line">          from</span><br><span class="line">              (</span><br><span class="line">                select app_key</span><br><span class="line">                      ,uid</span><br><span class="line">                      ,item_id</span><br><span class="line">                      ,post_id</span><br><span class="line">                      ,title</span><br><span class="line">                      ,ds</span><br><span class="line">                      ,hh</span><br><span class="line">                  from prod.ads_hive_rooster_ipush_users_1d</span><br><span class="line">                 where ds &#x3D; &#39;20181120&#39;</span><br><span class="line">              ) a</span><br><span class="line">      ) a</span><br><span class="line"> group by result</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">通过设置文件大小来这是Map个数是一种思路，但是不好把控。</span><br><span class="line">另一种方式就是把推送的执行放在Reduce端，通过控制reduce的个数来跟好的控制并行度</span><br><span class="line">可通过distribute by rand()进行重分布，然后mapred.reduce.tasks设置reduce执行个数</span><br><span class="line"></span><br><span class="line">add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;dmp-function-lib-test2.jar;</span><br><span class="line">create temporary function push as &#39;suishen.temp.udf.PushTestUDF&#39;;</span><br><span class="line">set mapred.reduce.tasks&#x3D;100;</span><br><span class="line">select result</span><br><span class="line">      ,count(*)</span><br><span class="line">  from</span><br><span class="line">      (</span><br><span class="line">        select pust(uid) as result</span><br><span class="line">          from</span><br><span class="line">              (</span><br><span class="line">                select app_key</span><br><span class="line">                      ,uid</span><br><span class="line">                      ,item_id</span><br><span class="line">                      ,post_id</span><br><span class="line">                      ,title</span><br><span class="line">                      ,ds</span><br><span class="line">                      ,hh</span><br><span class="line">                  from prod.ads_hive_rooster_ipush_users_1d</span><br><span class="line">                 where ds &#x3D; &#39;20181120&#39;</span><br><span class="line">                distribute by rand()</span><br><span class="line">              ) a</span><br><span class="line">      ) a</span><br><span class="line"> group by result</span><br><span class="line">;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;hive调优之控制mapper和reducer数&quot;&gt;&lt;a href=&quot;#hive调优之控制mapper和reducer数&quot; class=&quot;headerlink&quot; title=&quot;hive调优之控制mapper和reducer数&quot;&gt;&lt;/a&gt;hive调优之控制mapper和reducer数&lt;/h3&gt;&lt;p&gt;由于mapreduce中没有办法直接控制map数量，所以只能曲线救国，通过设置每个map中处理的数据量进行设置；reduce是可以直接设置的。&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="http://guoyanlei.top/categories/hive/"/>
    
    
      <category term="hive" scheme="http://guoyanlei.top/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7.2-时间同步工具ntp&amp;chrony</title>
    <link href="http://guoyanlei.top/2018/05/30/2018053001-CentOS7.2-%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7ntp&amp;chrony/"/>
    <id>http://guoyanlei.top/2018/05/30/2018053001-CentOS7.2-时间同步工具ntp&amp;chrony/</id>
    <published>2018-05-30T06:30:04.000Z</published>
    <updated>2018-05-30T09:10:01.085Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>集群加了几台新节点，安装了Cloudera之后，总是提示：</p><ul><li>不良 : 无法找到主机的 NTP 服务，或该服务未响应时钟偏差请求。</li></ul><p>但是，ntpd服务明明开启了，而且连接正常，还是提示时钟偏差。</p><p>最终调研了chronyd，发现CentOS7.2已经推荐使用chronyd，因此试着改为chronyd，重启Cloudera agent后问题解决了。下面总结下chronyd时钟同步。</p><a id="more"></a><h3 id="chronyd"><a href="#chronyd" class="headerlink" title="chronyd"></a>chronyd</h3><p>Chrony是网络时间协议的 (NTP) 的另一种实现。一直以来众多发行版里标配的都是ntpd对时服务，自rhel7/centos7 起，Chrony做为了发行版里的标配服务，不过老的ntpd服务依旧在rhel7/centos7里可以找到。</p><p>Chrony可以同时做为ntp服务的客户端和服务端。默认安装完后有两个程序chronyd和chronyc。</p><ul><li>chronyd是一个在系统后台运行的守护进程</li><li>chronyc是用来监控chronyd性能和配置其参数程序</li></ul><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># yum install -y chrony              &#x2F;&#x2F;安装服务</span><br><span class="line"># systemctl start chronyd.service    &#x2F;&#x2F;启动服务</span><br><span class="line"># systemctl enable chronyd.service   &#x2F;&#x2F;设置开机自启动，默认是enable的</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="chrony-conf配置"><a href="#chrony-conf配置" class="headerlink" title="chrony.conf配置"></a>chrony.conf配置</h4><p>Chrony的配置在/etc/chrony.conf，其个是ntpd服务基本相同，主要包含了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 时钟服务器，通过prefer提高优先级</span><br><span class="line">server 10.9.141.12 iburst minpoll 3 maxpoll 4 prefer</span><br><span class="line">server 10.9.255.1 iburst minpoll 3 maxpoll 4 </span><br><span class="line">server 10.9.255.2 iburst minpoll 3 maxpoll 4 </span><br><span class="line"></span><br><span class="line"># 设置当chronyd从可用源中选择同步源时，每个层应该添加多少距离到同步距离。默认情况下，CentOS中设置为0，让chronyd在选择源时忽略源的层级</span><br><span class="line">stratumweight 0</span><br><span class="line"></span><br><span class="line">#chronyd程序的主要行为之一，就是根据实际时间计算出计算机增减时间的比率，将它记录到一个文件中是最合理的，它会在重启后为系统时钟作出补偿，甚至可能的话，会从时钟服务器获得较好的估值；</span><br><span class="line">driftfile &#x2F;var&#x2F;lib&#x2F;chrony&#x2F;drift</span><br><span class="line"></span><br><span class="line">#rtcsync指令将启用一个内核模式，在该模式中，系统时间每11分钟会拷贝到实时时钟（RTC）</span><br><span class="line">rtcsync</span><br><span class="line"></span><br><span class="line">#chronyd将根据需求通过减慢或加速时钟，使得系统逐步纠正所有时间偏差。</span><br><span class="line">makestep 10 3</span><br><span class="line"></span><br><span class="line">#允许你限制chronyd监听哪个网络接口的命令包</span><br><span class="line">bindcmdaddress 127.0.0.1</span><br><span class="line">bindcmdaddress ::1</span><br><span class="line"></span><br><span class="line">keyfile &#x2F;etc&#x2F;chrony.keys</span><br><span class="line">commandkey 1</span><br><span class="line">generatecommandkey</span><br><span class="line">noclientlog</span><br><span class="line">logchange 0.5</span><br><span class="line">logdir &#x2F;var&#x2F;log&#x2F;chrony</span><br></pre></td></tr></table></figure><h4 id="ntpd改到chronyd"><a href="#ntpd改到chronyd" class="headerlink" title="ntpd改到chronyd"></a>ntpd改到chronyd</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1. 停掉所有ntpd服务，停止自启动ntpd服务</span><br><span class="line">systemctl stop ntpd.service</span><br><span class="line">systemctl disable ntpd.service</span><br><span class="line">vim &#x2F;etc&#x2F;chrony.conf</span><br><span class="line"></span><br><span class="line">2. 注释掉已有server，添加roc-master的server</span><br><span class="line">server 192.168.1.2 iburst  # 这里的ip地址是NTP服务器地址</span><br><span class="line"></span><br><span class="line">3.重启服务</span><br><span class="line">systemctl enable chronyd.service -l</span><br><span class="line">systemctl restart chronyd.service -l</span><br><span class="line">systemctl status chronyd.service -l</span><br><span class="line">chronyc sourcestats</span><br><span class="line">chronyc sources -v</span><br></pre></td></tr></table></figure><h4 id="chronyd改到ntpd"><a href="#chronyd改到ntpd" class="headerlink" title="chronyd改到ntpd"></a>chronyd改到ntpd</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">systemctl disable chronyd.service -l</span><br><span class="line">systemctl stop chronyd.service -l</span><br><span class="line">systemctl status chronyd.service -l</span><br><span class="line">systemctl restart ntpd.service</span><br><span class="line">systemctl status ntpd.service</span><br><span class="line">ntpq -p</span><br><span class="line">ntpstat</span><br></pre></td></tr></table></figure><h4 id="状态检测"><a href="#状态检测" class="headerlink" title="状态检测"></a>状态检测</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># chronyd</span><br><span class="line">systemctl status chronyd.service -l</span><br><span class="line">chronyc sourcestats</span><br><span class="line">chronyc sources -v</span><br><span class="line"></span><br><span class="line"># ntpd</span><br><span class="line">systemctl status ntpd.service -l</span><br><span class="line">ntpq -p</span><br><span class="line">ntpstat</span><br><span class="line"></span><br><span class="line"># 是否在系统级别得到时间同步：</span><br><span class="line">timedatectl</span><br></pre></td></tr></table></figure><h4 id="chrony的优势"><a href="#chrony的优势" class="headerlink" title="chrony的优势"></a>chrony的优势</h4><ul><li>更快的同步只需要数分钟而非数小时时间，从而最大程度减少了时间和频率误差，这对于并非全天 24 小时运行的台式计算机或系统而言非常有用。</li><li>能够更好地响应时钟频率的快速变化，这对于具备不稳定时钟的虚拟机或导致时钟频率发生变化的节能技术而言非常有用。</li><li>在初始同步后，它不会停止时钟，以防对需要系统时间保持单调的应用程序造成影响。</li><li>在应对临时非对称延迟时（例如，在大规模下载造成链接饱和时）提供了更好的稳定性。</li><li>无需对服务器进行定期轮询，因此具备间歇性网络连接的系统仍然可以快速同步时钟。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;集群加了几台新节点，安装了Cloudera之后，总是提示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不良 : 无法找到主机的 NTP 服务，或该服务未响应时钟偏差请求。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是，ntpd服务明明开启了，而且连接正常，还是提示时钟偏差。&lt;/p&gt;
&lt;p&gt;最终调研了chronyd，发现CentOS7.2已经推荐使用chronyd，因此试着改为chronyd，重启Cloudera agent后问题解决了。下面总结下chronyd时钟同步。&lt;/p&gt;
    
    </summary>
    
      <category term="cloudera-manager" scheme="http://guoyanlei.top/categories/cloudera-manager/"/>
    
    
      <category term="kudu" scheme="http://guoyanlei.top/tags/kudu/"/>
    
      <category term="cloudera-manager" scheme="http://guoyanlei.top/tags/cloudera-manager/"/>
    
  </entry>
  
  <entry>
    <title>Impala优化invalidate metadata &amp; refresh</title>
    <link href="http://guoyanlei.top/2018/05/23/2018052301-Impala%E4%BC%98%E5%8C%96invalidate%20metadata%20&amp;%20refresh/"/>
    <id>http://guoyanlei.top/2018/05/23/2018052301-Impala优化invalidate metadata &amp; refresh/</id>
    <published>2018-05-23T04:30:04.000Z</published>
    <updated>2018-05-25T09:45:28.107Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>CDH集群运行几天后主节点会报磁盘根目录空间不足警告，上机器看一下发现df和du结果差异巨大，猜测有些文件被<br>删除但有进程并未释放文件句柄。</p><a id="more"></a><h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>执行 lsof | grep “(deleted)”<br>不出所料，发现10000多个以下文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">catalogd  18019             impala  252r      REG              253,1  28011300     819296 &#x2F;tmp&#x2F;9a02bf7f-bf3b-461c-a4e9-b49638cbc27b.jar (deleted)</span><br><span class="line">catalogd  18019             impala  253r      REG              253,1  28010056     819223 &#x2F;tmp&#x2F;852f0830-a9d5-4921-b0e4-3089e520b739.jar (deleted)</span><br><span class="line">catalogd  18019             impala  255r      REG              253,1  28010056     819238 &#x2F;tmp&#x2F;bdfd341d-db28-4df8-96ab-de5a9308ddb8.jar (deleted)</span><br><span class="line">catalogd  18019             impala  256r      REG              253,1  28010056     819218 &#x2F;tmp&#x2F;ef50fe5b-aa08-4593-bd99-abcad886216c.jar (deleted)</span><br><span class="line">catalogd  18019             impala  258r      REG              253,1  28010056     819219 &#x2F;tmp&#x2F;fb5d5375-9514-4af1-91a8-dcdc5849165a.jar (deleted)</span><br><span class="line">catalogd  18019             impala  259r      REG              253,1  28010056     819234 &#x2F;tmp&#x2F;2194ce13-5805-48d8-bca2-b4d3e849dca8.jar (deleted)</span><br><span class="line">catalogd  18019             impala  260r      REG              253,1  28010056     819221 &#x2F;tmp&#x2F;a6e621c6-076b-4bed-a45a-559be5f3214e.jar (deleted)</span><br><span class="line">catalogd  18019             impala  261r      REG              253,1  28010056     819222 &#x2F;tmp&#x2F;3e1a5627-70e7-4070-85ef-4c003405d5b9.jar (deleted)</span><br><span class="line">catalogd  18019             impala  262r      REG              253,1  28011255     819293 &#x2F;tmp&#x2F;5c270d7c-75c2-4ce2-aa6b-af9f39d42b23.jar (deleted)</span><br><span class="line">catalogd  18019             impala  263r      REG              253,1  28010056     819224 &#x2F;tmp&#x2F;64e4e5af-8e0e-4d12-b187-adccb24af4e9.jar (deleted)</span><br><span class="line">catalogd  18019             impala  264r      REG              253,1  28010056     819225 &#x2F;tmp&#x2F;7d4fa926-19dd-44c5-92b6-a34ed3530265.jar (deleted)</span><br></pre></td></tr></table></figure><p>catalogd为impala负责更新mate的进程，所以对impala进行排查，经验证发现统计中有执行invalidate metadata的命令<br>，该命令执行后会立即出现大量该问题，未关闭的文件句柄经验证为自定义的udf jar包。出现这种情况只能通过重启impala来解决。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>经粗略研究，应避免使用invalidate metadata 全局更新。可使用invalidate metadata tableName 或者 refresh tableName代替即可。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>INVALIDATE METADATA: 是用于刷新全库或者某个表的元数据，包括表的元数据和表内的文件数据，它会首先清除表的缓存，然后从metastore中重新加载全部数据并缓存，该操作代价比较重，主要用于在hive中修改了表的元数据，需要同步到impalad</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">例如</span><br><span class="line">create table</span><br><span class="line">drop table</span><br><span class="line">alter table add columns</span><br><span class="line">等。</span><br><span class="line"></span><br><span class="line">INVALIDATE METADATA;               &#x2F;&#x2F;重新加载所有库中的所有表</span><br><span class="line">INVALIDATE METADATA [table]        &#x2F;&#x2F;重新加载指定的某个表</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>REFRESH是用于刷新某个表或者某个分区的数据信息，它会重用之前的表元数据，仅仅执行文件刷新操作，它能够检测到表中分区的增加和减少，主要用于表中元数据未修改，数据的修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">例如</span><br><span class="line">INSERT INTO</span><br><span class="line">LOAD DATA</span><br><span class="line">ALTER TABLE ADD PARTITION</span><br><span class="line">LLTER TABLE DROP PARTITION</span><br><span class="line">等，如果直接修改表的HDFS文件（增加、删除或者重命名）也需要指定REFRESH刷新数据信息。</span><br><span class="line"></span><br><span class="line">REFRESH [table]                             &#x2F;&#x2F;刷新某个表</span><br><span class="line">REFRESH [table] PARTITION [partition]       &#x2F;&#x2F;刷新某个表的某个分区</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="INVALIDATE-METADATA原理"><a href="#INVALIDATE-METADATA原理" class="headerlink" title="INVALIDATE METADATA原理"></a>INVALIDATE METADATA原理</h4><p>对于INVALIDATE METADATA操作，由客户端将查询提交到某个impalad节点上，执行如下的操作：</p><ul><li>获取需要执行INVALIDATE METADATA的表，如果没指定表则不设置表示全部表（不考虑这种情况）。</li><li>请求catalogd执行resetMetadata操作，并将isFresh参数设置为false。</li><li>catalogd接收到该请求之后执行invalidateTable操作，将该表的缓存清除，然后重新生成该表的缓存对象，新生成的对象只包含表名+库名的信息，为新生成的表对象生成一个新的catalog版本号（假设新的version=1），将这部分信息返回给调用方（impalad），然后异步执行元数据和数据的加载。</li><li>impalad收到catalogd的返回值，返回值是更新之后的表缓存对象+版本号，但是这是一个不完整的表元数据，impalad将这个元数据应用到本地元数据缓存。</li><li>INVALIDATE METADATA执行完成。</li></ul><p>INVALIDATE METADATA操作带来的副作用是：生成一个新的未完成的元数据对象，对于操作请求的impalad（称它为impalad-A），能够立马获取到该对象，对于其它的impalad需要通过statestored同步，因此执行完该操作，处理该操作的impalad对于该表的缓存是一个新的但是不完整的对象，其余的impalad保存的是旧的元数据。</p><p>对于后续的该表查询操作，分为如下四种情况：</p><ul><li>如果catalogd已经完成该表所有元数据加载，会对该表生成一个新的版本号（假设version=2），然后更新到statestored，由statestored广播到各个impalad节点上，此时所有的查询都查询到最新的元数据和数据。</li><li>如果catalogd尚未完成表的元数据加载或者statestored未广播完成，并且接下来请求到impalad-A（之前执行INVALIDATE METADATA的节点），此时impalad在执行语义分析的时候能够检测到表的元数据不完整（因为当前只有表名和库名，没有任何其余的元数据），impalad会直接请求catalogd获取该表最新的元数据，如果catalogd尚未完成元数据加载，则该请求会等到直到catalogd加载完成并返回impalad最新的元数据。</li><li>如果catalogd尚未完成表的元数据加载或statestored未广播完成，接下来请求到了其他的impalad节点，如果接受请求的impalad尚未通过statestored同步新的不完整的表元数据（version=1），则该impalad中缓存的关于该表的元数据是执行INVALIDATE METADATA之前的，因此根据旧的元数据处理该查询（可能因为文件被删除导致错误）。</li><li>如果catalogd尚未完成表的元数据加载，接下来请求到了其他的impalad节点，如果接受请求的impalad已经通过statestored同步新的不完整的表元数据（version=1），那么接下来会像第二种情况一样处理。<br>从INVALIDATE METADATA的实现来看，该操作不仅仅会全量加载表的元数据和分区、文件元数据，还会影响后面关于该表的查询。</li></ul><h4 id="REFRESH原理"><a href="#REFRESH原理" class="headerlink" title="REFRESH原理"></a>REFRESH原理</h4><p>对于REFRESH操作，由客户端将查询提交到某个impalad节点上，执行如下的操作：</p><ul><li>获取需要执行REFRESH的表和分区信息。</li><li>请求catalogd执行resetMetadata操作，并将isFresh参数设置为true。</li><li>catalogd接收到该请求之后判断是否指定分区，如果指定了分区则执行reload partition操作，如果未指定则执行reload table操作，对于reloadPartition则从metastore中读取partition最新的元数据，然后刷新该partition拥有的所有文件的元数据（大小，权限，数据分布等）；对于reloadTable则从metadata中读取全部的partition信息，然后和缓存中的partition进行比对判断是否有分区需要增加和删除，对于其余的分区则执行元数据的更新。</li><li>impalad收到catalogd的返回值，返回值是更新之后该表的缓存数据，impalad会将该数据更新到自己的缓存中。因此接受请求的impalad能够将当前元数据缓存。</li><li>REFRESH执行完成。</li></ul><p>对于后续的查询，分为如下两种情况：</p><ul><li>如果查询提交到到执行REFRESH的impalad节点，那么查询能够使用最新的元数据。</li><li>如果查询提交到其他impalad节点，需要依赖于该表0更新后的缓存是否已经同步到impalad中，如果已经完成了同步则可以使用最新的元数据，如果未完成则使用旧的元数据。</li></ul><p>可以看出REFRESH操作较之于INVALIDATE METADATA是轻量级的操作，如果更改只涉及到一个分区设置可以只刷新一个分区的元数据，并且它是同步的，对于之后查询的影响较小。</p><h4 id="使用原则"><a href="#使用原则" class="headerlink" title="使用原则"></a>使用原则</h4><p>如果在使用过程中涉及到了元数据或者数据的更新，则需要使用这两者中的一个操作完成，具体如何选择需要根据如下原则：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">● invalidate metadata操作比refresh要重量级</span><br><span class="line">● 如果涉及到表的schema改变，使用invalidate metadata [table]</span><br><span class="line">● 如果只是涉及到表的数据改变，使用refresh [table]</span><br><span class="line">● 如果只是涉及到表的某一个分区数据改变，使用refresh [table] partition [partition]</span><br><span class="line">● 禁止使用invalidate metadata什么都不加，宁愿重启catalogd。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>REFRESH和INVALIDATE METADATA对于impala而言是比较重要的两个操作，分别处理数据和元数据的修改，其中REFRESH操作是同步的，INVALIDATE METADATA是异步的。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;CDH集群运行几天后主节点会报磁盘根目录空间不足警告，上机器看一下发现df和du结果差异巨大，猜测有些文件被&lt;br&gt;删除但有进程并未释放文件句柄。&lt;/p&gt;
    
    </summary>
    
      <category term="impala" scheme="http://guoyanlei.top/categories/impala/"/>
    
    
      <category term="impala" scheme="http://guoyanlei.top/tags/impala/"/>
    
  </entry>
  
  <entry>
    <title>Hive跨集群访问hbase和phoenix表</title>
    <link href="http://guoyanlei.top/2018/05/22/2018052201-Hive%E8%B7%A8%E9%9B%86%E7%BE%A4%E8%AE%BF%E9%97%AEhbase%E5%92%8Cphoenix%E8%A1%A8/"/>
    <id>http://guoyanlei.top/2018/05/22/2018052201-Hive跨集群访问hbase和phoenix表/</id>
    <published>2018-05-22T07:30:04.000Z</published>
    <updated>2018-07-27T11:12:38.766Z</updated>
    
    <content type="html"><![CDATA[<p>在这里记录下公司前段时间对集群做的调整：</p><p>1）背景：由于之前集群是基于虚拟化的方式搭建的集群，一个物理节点上虚拟出了十多个节点，由于每个物理节点的磁盘是固定的，随着我们数据量的增长，以及数据计算频度的增加，这套集群的磁盘io已经达到瓶颈，急需搭建新的物理节点集群。</p><p>2）问题：这套老集群上部署着数据平台所用到的hbase和phoenix等，由于不能影响业务，所以短时间内不好进行迁移，但是集群的计算资源又不够用，继续把计算迁移出去。</p><a id="more"></a><p>3）方案：一方面对新老集群实时日志收集进行双写，另一方面把hive表中的统计数据跨集群迁移，然后在新集群上进行统计操作，把统计后的数据跨集群写入老集群的hbase和phoenix中。</p><p>本文记录下跨集群写入Hbase和Phoenix的方法。</p><h3 id="跨集群写入-hbase-表"><a href="#跨集群写入-hbase-表" class="headerlink" title="跨集群写入 hbase 表"></a>跨集群写入 hbase 表</h3><p>在新建Hbas外部表时，指定老集群hbase所在的zk</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">-- hbase外部表（hive执行）</span><br><span class="line">set hbase.zookeeper.quorum&#x3D;10.10.95.131:2181,10.10.34.92:2181,10.10.48.188:2181;</span><br><span class="line">create EXTERNAL table hbase_ads_dmp_uid_module_pv_1d (</span><br><span class="line">    key         String</span><br><span class="line">   ,stat_date   string</span><br><span class="line">   ,pv          bigint</span><br><span class="line">   ,clk         bigint</span><br><span class="line">   ,page_view   bigint</span><br><span class="line">)</span><br><span class="line">stored by &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;</span><br><span class="line">with serdeproperties (</span><br><span class="line">&quot;hbase.table.name&quot;&#x3D;&quot;hbase_ads_dmp_uid_module_pv_1d&quot;,</span><br><span class="line">&quot;hbase.columns.mapping&quot; &#x3D; &quot;:key,T:stat_date,T:pv#b,T:clk#b,T:page_view#b&quot;</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在执行hive写入Hbase SQL时需要加上老hbase的zk</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set hbase.zookeeper.quorum&#x3D;10.10.95.131:2181,10.10.34.92:2181,10.10.48.188:2181;</span><br><span class="line">insert overwrite table prod.ads_hbase_dmp_ad_mapper_pv_1d</span><br><span class="line">select pv</span><br><span class="line">      ,uv</span><br><span class="line">  from prod.ads_hive_dmp_ad_mapper_pv_1d</span><br><span class="line">  distribute by rand()</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="phoenix-表"><a href="#phoenix-表" class="headerlink" title="phoenix 表"></a>phoenix 表</h3><p>可在建外部表时直接指定phoenix所在的zk</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-- phoenix外部表（hive执行）</span><br><span class="line">add jar hdfs:&#x2F;&#x2F;nameservice:8020&#x2F;user&#x2F;udf&#x2F;phoenix-hive-4.2.2-jar-with-dependencies.jar;</span><br><span class="line">CREATE EXTERNAL TABLE PHOENIX_ADS_DMP_USER_START_1D(</span><br><span class="line">     stat_date      string</span><br><span class="line">    ,app_key        string</span><br><span class="line">    ,start_type     string</span><br><span class="line">    ,rn             int</span><br><span class="line">    ,start_num_sum  bigint</span><br><span class="line">    ,uv             bigint</span><br><span class="line">)</span><br><span class="line">STORED BY  &quot;org.apache.phoenix.hive.PhoenixStorageHandler&quot;</span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">&#39;phoenix.zookeeper.quorum&#39;&#x3D;&#39;10.10.95.131,10.10.34.92,10.10.48.188:2181&#39;,</span><br><span class="line">&#39;phoenix.hbase.table.name&#39;&#x3D;&#39;PHOENIX_ADS_DMP_USER_START_1D&#39;,</span><br><span class="line">&#39;phoenix.zookeeper.znode.parent&#39;&#x3D;&#39;hbase&#39;,</span><br><span class="line">&#39;phoenix.rowkeys&#39;&#x3D;&#39;stat_date,app_key,start_type,rn&#39;,</span><br><span class="line">&#39;phoenix.column.mapping&#39;&#x3D;&#39;start_num_sum:A.start_num_sum,uv:A.uv&#39;</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>还需要再hivesever2所在的机器添加hbase配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;etc&#x2F;hbase&#x2F;conf&#x2F;hbase-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.quorum&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;10.10.95.131:2181,10.10.34.92:2181,10.10.48.188:2181&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.client.scanner.caching&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;10000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rpc.timeout&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;150000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在这里记录下公司前段时间对集群做的调整：&lt;/p&gt;
&lt;p&gt;1）背景：由于之前集群是基于虚拟化的方式搭建的集群，一个物理节点上虚拟出了十多个节点，由于每个物理节点的磁盘是固定的，随着我们数据量的增长，以及数据计算频度的增加，这套集群的磁盘io已经达到瓶颈，急需搭建新的物理节点集群。&lt;/p&gt;
&lt;p&gt;2）问题：这套老集群上部署着数据平台所用到的hbase和phoenix等，由于不能影响业务，所以短时间内不好进行迁移，但是集群的计算资源又不够用，继续把计算迁移出去。&lt;/p&gt;
    
    </summary>
    
      <category term="hbase" scheme="http://guoyanlei.top/categories/hbase/"/>
    
    
      <category term="hive" scheme="http://guoyanlei.top/tags/hive/"/>
    
      <category term="跨集群" scheme="http://guoyanlei.top/tags/%E8%B7%A8%E9%9B%86%E7%BE%A4/"/>
    
      <category term="hbase" scheme="http://guoyanlei.top/tags/hbase/"/>
    
      <category term="phoenix" scheme="http://guoyanlei.top/tags/phoenix/"/>
    
  </entry>
  
</feed>
