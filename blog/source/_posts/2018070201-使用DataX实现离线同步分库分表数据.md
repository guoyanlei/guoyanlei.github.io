title: 使用DataX实现离线同步分库分表数据
date: 2018/07/02 14:30:00
tags:
- DataX
categories:
- 数据同步

---


### DataX简介


DataX 是阿里巴巴内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。

DataX更像是一个数据枢纽，它可以读取多种数据源中数据，经过内部的转换又可以输出到多种数据源中。

其架构设计主要包含三部分：

- Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework
- Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。
- Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。

更详细的介绍，请移步 https://yq.aliyun.com/articles/59373

这里只总结DataX在同步MySQL分库分表的使用。

<!--more-->


### 同步分库中表的数据

我们业务系统的用户基本信息表，被分库存储在不同的库中，下面介绍使用DataX如何将这些分库的数据同步到HDFA中。

#### 下载 & 使用

从[Datax Github](https://github.com/alibaba/DataX) 中下载源码编译，或直接下载已编译好的工具包。

下载后解压至本地某个目录，进入bin目录，即可运行同步作业：

```
$ cd  {DATAX_HOME}/bin
$ python datax.py {YOUR_JOB.json}

```

配置和使用很简单，只需配置YOUR_JOB.json，在这个文件中配置输入源.

其中 datax.py可以配置多个参数，如下：

```
-j <jvm parameters>  	# 设置jvm参数，如-Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError
--jobid=<job unique id> # 设置job唯一id
-m <job runtime mode> 	# 设置运行模式: standalone(默认), local, distribute.
-p <parameter used in job config> 	# 动态设置一些参数，当在配置文件设置了一些变量时，如${tableName}，那么可以通过-p"-DtableName=your-table-name"来动态设置。
-r <parameter used in view job config[reader] template>  # 使用一些reader模版，eg: mysqlreader,streamreader
-w <parameter used in view job config[writer] template>, # 使用一些reader模版，eg: mysqlwriter,streamwriter
```

#### 同步分库的配置

下面给出同步分库中表数据的配置，content中包含两部分内容，reader和writer，其中reader中配置了mysql的分库地址。

```
# user_info.json
{
  "job": {
    "setting": {
      "speed": {
        "channel":60
      }
    },
    "content": [
      {
        "reader": {
          "name": "mysqlreader",
          "parameter": {
            "username": "xxx",
            "password": "xxx",
            "column": [
                 "uid"
                ,"nick_name"
                ,"accounts"
                ,"email"
                , ... ...
            ],
            "connection": [
              {"table": ["user_info"],"jdbcUrl": ["jdbc:mysql://ip_1:3306/metastore_1000"]},
              {"table": ["user_info"],"jdbcUrl": ["jdbc:mysql://ip_2:3306/metastore_1001"]},
              {"table": ["user_info"],"jdbcUrl": ["jdbc:mysql://ip_3:3306/metastore_1002"]},
              ... ...
            ]
          }
        },
        "writer": {
          "name": "hdfswriter",
          "parameter": {
            "defaultFS": "hdfs://nameservice:8020",
            "hadoopConfig":{
              "dfs.nameservices": "nameservice",
              "dfs.ha.namenodes.nameservice": "namenode44,namenode46",
              "dfs.namenode.rpc-address.nameservice.namenode44": "nn1.hadoop.bigdata.dmp.com:8020",
              "dfs.namenode.rpc-address.nameservice.namenode46": "nn2.hadoop.bigdata.dmp.com:8020",
              "dfs.client.failover.proxy.provider.nameservice": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
            },
            "fileType": "orc",
            "path": "/user/hive/warehouse/test.db/ads_dim_user_info",
            "fileName": "user_info",
            "column": [
              {
                "name": "uid",
                "type": "BIGINT"
              },
              {
                "name": "nick_name",
                "type": "STRING"
              },
              {
                "name": "accounts",
                "type": "STRING"
              },
              {
                "name": "email",
                "type": "STRING"
              },
              ... ...
            ],
            "writeMode": "append",
            "fieldDelimiter": "\t",
            "compress":"NONE"
          }
        }
      }
    ]
  }
}

```

### 同步分表中的数据

和同步分库的结构类似，同步分表的conf如下：

```
# user_relation.json
{
  "job": {
    "setting": {
      "speed": {
        "channel":60
      }
    },
    "content": [
      {
        "reader": {
          "name": "mysqlreader",
          "parameter": {
            "username": "xxx",
            "password": "xxx",
            "column": [
                 "uid"
                ,"attention_uid"
                ,"status"
                ,"create_time"
                ,"update_time"
            ],
            "connection": [
              {"table": ["user_relation_0"],"jdbcUrl":   ["jdbc:mysql://ip_1:3306/user_attention"]},
              {"table": ["user_relation_1"],"jdbcUrl":   ["jdbc:mysql://ip_1:3306/user_attention"]},
              {"table": ["user_relation_10"],"jdbcUrl":  ["jdbc:mysql://ip_1:3306/user_attention"]},
              {"table": ["user_relation_100"],"jdbcUrl": ["jdbc:mysql://ip_1:3306/user_attention"]},
              {"table": ["user_relation_101"],"jdbcUrl": ["jdbc:mysql://ip_1:3306/user_attention"]},
              ... ...
            ]
          }
        },
        "writer": {
          "name": "hdfswriter",
          "parameter": {
            "defaultFS": "hdfs://nameservice:8020",
            "hadoopConfig":{
              "dfs.nameservices": "nameservice",
              "dfs.ha.namenodes.nameservice": "namenode44,namenode46",
              "dfs.namenode.rpc-address.nameservice.namenode44": "nn1.hadoop.bigdata.dmp.com:8020",
              "dfs.namenode.rpc-address.nameservice.namenode46": "nn2.hadoop.bigdata.dmp.com:8020",
              "dfs.client.failover.proxy.provider.nameservice": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
            },
            "fileType": "orc",
            "path": "/user/hive/warehouse/test.db/ads_dim_user_relation",
            "fileName": "user_info",
            "column": [
              {
                "name": "uid",
                "type": "BIGINT"
              },
              {
                "name": "attention_uid",
                "type": "BIGINT"
              },
              {
                "name": "status",
                "type": "int"
              },
              {
                "name": "create_time",
                "type": "BIGINT"
              },
              {
                "name": "update_time",
                "type": "BIGINT"
              }
            ],
            "writeMode": "append",
            "fieldDelimiter": "\t",
            "compress":"NONE"
          }
        }
      }
    ]
  }
}
```
具体的hdfs写入，参考[文档说明](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)

在执行时，若hdfs中的目录为空，需要先创建，若不为空，需根据writeMode来判断，若模式是：append，写入前不做任何处理，DataX hdfswriter直接使用filename写入，并保证文件名不冲突；若模式是：nonConflict，如果目录下有fileName前缀的文件，直接报错。

可以建立一个执行脚本，如下：

```
#!/bin/bash
hdfs dfs -rm -r /user/hive/warehouse/test.db/ads_dim_user_relation
hdfs dfs -rm -r /user/impala/.Trash/Current/user/hive/warehouse/test.db/ads_dim_user_relation
hdfs dfs -mkdir /user/hive/warehouse/test.db/ads_dim_user_relation

python datax.py user_relation.json -j '-Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError'

```

### 创建Hive表

按指定格式导入到hdfs后，就可以建立hive表，并指定hdfs目录，就可以读取其中的数据了。

```
create table business.ods_dim_weili_user_relation(
     uid              bigint
    ,attention_uid    bigint
    ,status           int
    ,create_time      bigint
    ,update_time      bigint
)
row format delimited fields terminated by '\t'
stored as orc;

```





